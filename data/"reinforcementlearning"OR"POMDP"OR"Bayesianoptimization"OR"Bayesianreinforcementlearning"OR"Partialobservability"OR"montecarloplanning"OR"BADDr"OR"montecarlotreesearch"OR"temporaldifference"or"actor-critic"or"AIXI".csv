position,title,result_id,link,snippet,publication_info_summary,publication_info_authors_0_name,publication_info_authors_0_link,publication_info_authors_0_serpapi_scholar_link,publication_info_authors_0_author_id,publication_info_authors_1_name,publication_info_authors_1_link,publication_info_authors_1_serpapi_scholar_link,publication_info_authors_1_author_id,publication_info_authors_2_name,publication_info_authors_2_link,publication_info_authors_2_serpapi_scholar_link,publication_info_authors_2_author_id,resources_0_title,resources_0_file_format,resources_0_link,inline_links_serpapi_cite_link,inline_links_cited_by_total,inline_links_cited_by_link,inline_links_cited_by_cites_id,inline_links_cited_by_serpapi_scholar_link,inline_links_related_pages_link,inline_links_serpapi_related_pages_link,inline_links_versions_total,inline_links_versions_link,inline_links_versions_cluster_id,inline_links_versions_serpapi_scholar_link,type,inline_links_cached_page_link,publication_info_authors_3_name,publication_info_authors_3_link,publication_info_authors_3_serpapi_scholar_link,publication_info_authors_3_author_id,resources_1_title,resources_1_link
1,Bayesian reinforcement learning,18PTp0x1v2cJ,https://link.springer.com/chapter/10.1007/978-3-642-27645-3_11,"… In this section, we study a Bayesian actor-critic (BAC) algorithm that incorporates GPTD in its critic (Ghavamzadeh and Engel, 2007). We show how the posterior moments returned by …","N Vlassis, M Ghavamzadeh, S Mannor… - Reinforcement Learning …, 2012 - Springer",N Vlassis,https://scholar.google.com/citations?user=JJWWPjsAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=JJWWPjsAAAAJ&engine=google_scholar_author&hl=en,JJWWPjsAAAAJ,M Ghavamzadeh,https://scholar.google.com/citations?user=Bo-wyrkAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=Bo-wyrkAAAAJ&engine=google_scholar_author&hl=en,Bo-wyrkAAAAJ,S Mannor,https://scholar.google.com/citations?user=q1HlbIUAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=q1HlbIUAAAAJ&engine=google_scholar_author&hl=en,q1HlbIUAAAAJ,uni.lu,PDF,https://orbilu.uni.lu/bitstream/10993/3390/1/BRLchapter.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=18PTp0x1v2cJ,103.0,"https://scholar.google.com/scholar?cites=7475822878551950295&as_sdt=20000005&sciodt=0,21&hl=en&num=20",7475822878551950295,https://serpapi.com/search.json?as_sdt=20000005&cites=7475822878551950295&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:18PTp0x1v2cJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,21",https://serpapi.com/search.json?as_sdt=0%2C21&engine=google_scholar&hl=en&num=20&q=related%3A18PTp0x1v2cJ%3Ascholar.google.com%2F&start=0,13.0,"https://scholar.google.com/scholar?cluster=7475822878551950295&hl=en&num=20&as_sdt=0,21",7475822878551950295,https://serpapi.com/search.json?as_sdt=0%2C21&cluster=7475822878551950295&engine=google_scholar&hl=en&num=20,,,,,,,,
2,Reinforcement learning algorithms for solving classification problems,UlqYEg2IP8UJ,https://ieeexplore.ieee.org/abstract/document/5967372/,"… Actor-Critic Learning Automaton (ALCA) [3], although we could also have used QV-learning [8] or other actor-critic … Furthermore, other reinforcement learning algorithms such as the AIXI …","MA Wiering, H Van Hasselt… - … and Reinforcement …, 2011 - ieeexplore.ieee.org",MA Wiering,https://scholar.google.com/citations?user=880vFIgAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=880vFIgAAAAJ&engine=google_scholar_author&hl=en,880vFIgAAAAJ,H Van Hasselt,https://scholar.google.com/citations?user=W80oBMkAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=W80oBMkAAAAJ&engine=google_scholar_author&hl=en,W80oBMkAAAAJ,,,,,researchgate.net,PDF,https://www.researchgate.net/profile/Lambert-Schomaker/publication/224250582_Reinforcement_learning_algorithms_for_solving_classification_problems/links/0fcfd508e4b86cc5a5000000/Reinforcement-learning-algorithms-for-solving-classification-problems.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=UlqYEg2IP8UJ,88.0,"https://scholar.google.com/scholar?cites=14213228538732501586&as_sdt=20000005&sciodt=0,21&hl=en&num=20",14213228538732501586,https://serpapi.com/search.json?as_sdt=20000005&cites=14213228538732501586&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:UlqYEg2IP8UJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,21",https://serpapi.com/search.json?as_sdt=0%2C21&engine=google_scholar&hl=en&num=20&q=related%3AUlqYEg2IP8UJ%3Ascholar.google.com%2F&start=0,9.0,"https://scholar.google.com/scholar?cluster=14213228538732501586&hl=en&num=20&as_sdt=0,21",14213228538732501586,https://serpapi.com/search.json?as_sdt=0%2C21&cluster=14213228538732501586&engine=google_scholar&hl=en&num=20,,,,,,,,
3,Multiagent reinforcement learning-based cooperative multitype task offloading strategy for internet of vehicles in B5G/6G network,lKB11Y3k6AcJ,https://ieeexplore.ieee.org/abstract/document/10045759/,"… the delay of task execution, we regard cooperative offloading as a Markov decision process (MDP), and improve the convergence speed and stability of traditional soft actor-critic (SAC) …","Y Cui, H Li, D Zhang, A Zhu, Y Li… - IEEE Internet of Things …, 2023 - ieeexplore.ieee.org",,,,,,,,,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=lKB11Y3k6AcJ,18.0,"https://scholar.google.com/scholar?cites=569956650685145236&as_sdt=20000005&sciodt=0,21&hl=en&num=20",569956650685145236,https://serpapi.com/search.json?as_sdt=20000005&cites=569956650685145236&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:lKB11Y3k6AcJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,21",https://serpapi.com/search.json?as_sdt=0%2C21&engine=google_scholar&hl=en&num=20&q=related%3AlKB11Y3k6AcJ%3Ascholar.google.com%2F&start=0,,,,,,,,,,,,
4,Optimal Consensus Control for Continuous-time Multi-agent Systems via Actor-Critic Neural Networks,0OVp3GfHdUUJ,https://ieeexplore.ieee.org/abstract/document/9738588/,"… by utilizing the framework of reinforcement learning. A leader-follower … Furthermore, an actor-critic neural network is applied for the PI … learning, policy iteration, actor-critic neural network …","X Jia, K Wolter - 2022 8th International Conference on …, 2022 - ieeexplore.ieee.org",,,,,,,,,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=0OVp3GfHdUUJ,,,,,"https://scholar.google.com/scholar?q=related:0OVp3GfHdUUJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,21",https://serpapi.com/search.json?as_sdt=0%2C21&engine=google_scholar&hl=en&num=20&q=related%3A0OVp3GfHdUUJ%3Ascholar.google.com%2F&start=0,,,,,,,,,,,,
5,Distributed Actor-Critic Approach for Frequency Synchronization of Isolated AC Microgrids,19O5jx29iB8J,https://ieeexplore.ieee.org/abstract/document/10142098/,"… Reinforcement learning techniques are used to study this consensus problem, and the actor-critic … The algorithm for the actorcritic method on each DG is presented, and its effectiveness …","SW Lin, CF Tung, CC Chu - 2023 IEEE/IAS 59th Industrial and …, 2023 - ieeexplore.ieee.org",SW Lin,https://scholar.google.com/citations?user=qwFjTH0AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=qwFjTH0AAAAJ&engine=google_scholar_author&hl=en,qwFjTH0AAAAJ,CC Chu,https://scholar.google.com/citations?user=RHOx02cAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=RHOx02cAAAAJ&engine=google_scholar_author&hl=en,RHOx02cAAAAJ,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=19O5jx29iB8J,1.0,"https://scholar.google.com/scholar?cites=2272273946671436759&as_sdt=20000005&sciodt=0,21&hl=en&num=20",2272273946671436759,https://serpapi.com/search.json?as_sdt=20000005&cites=2272273946671436759&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:19O5jx29iB8J:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,21",https://serpapi.com/search.json?as_sdt=0%2C21&engine=google_scholar&hl=en&num=20&q=related%3A19O5jx29iB8J%3Ascholar.google.com%2F&start=0,3.0,"https://scholar.google.com/scholar?cluster=2272273946671436759&hl=en&num=20&as_sdt=0,21",2272273946671436759,https://serpapi.com/search.json?as_sdt=0%2C21&cluster=2272273946671436759&engine=google_scholar&hl=en&num=20,,,,,,,,
6,Observer-based consensus control for MASs with prescribed constraints via reinforcement learning algorithm,6VgQlrs4n6YJ,https://ieeexplore.ieee.org/abstract/document/10225438/,"… In addition, the updating laws of actor-critic NNs are established by using a simplified reinforcement learning (RL) algorithm based on the uniqueness of optimal solution, and the …","A Luo, Q Zhou, H Ma, H Li - IEEE Transactions on Neural …, 2023 - ieeexplore.ieee.org",,,,,,,,,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=6VgQlrs4n6YJ,27.0,"https://scholar.google.com/scholar?cites=12006377509920725225&as_sdt=20000005&sciodt=0,21&hl=en&num=20",12006377509920725225,https://serpapi.com/search.json?as_sdt=20000005&cites=12006377509920725225&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:6VgQlrs4n6YJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,21",https://serpapi.com/search.json?as_sdt=0%2C21&engine=google_scholar&hl=en&num=20&q=related%3A6VgQlrs4n6YJ%3Ascholar.google.com%2F&start=0,3.0,"https://scholar.google.com/scholar?cluster=12006377509920725225&hl=en&num=20&as_sdt=0,21",12006377509920725225,https://serpapi.com/search.json?as_sdt=0%2C21&cluster=12006377509920725225&engine=google_scholar&hl=en&num=20,,,,,,,,
7,Safe Reinforcement Learning for Connected and Automated Vehicle Platooning,QYeepJLX9XIJ,https://ieeexplore.ieee.org/abstract/document/10639981/,"… reinforcement learning (… actor-critic RL framework to synthesize efficient control policies for CAV platooning. Specifically, a Lyapunov-based critic network is proposed in the actor-critic …","K Jiang, Y Lu, R Su - 2024 IEEE 7th International Conference …, 2024 - ieeexplore.ieee.org",K Jiang,https://scholar.google.com/citations?user=tCHMPX0AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=tCHMPX0AAAAJ&engine=google_scholar_author&hl=en,tCHMPX0AAAAJ,Y Lu,https://scholar.google.com/citations?user=_uFez_0AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=_uFez_0AAAAJ&engine=google_scholar_author&hl=en,_uFez_0AAAAJ,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=QYeepJLX9XIJ,,,,,"https://scholar.google.com/scholar?q=related:QYeepJLX9XIJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,21",https://serpapi.com/search.json?as_sdt=0%2C21&engine=google_scholar&hl=en&num=20&q=related%3AQYeepJLX9XIJ%3Ascholar.google.com%2F&start=0,,,,,,,,,,,,
8,Towards machine learning of motor skills,s09kl0WTGdYJ,https://link.springer.com/chapter/10.1007/978-3-540-74764-2_22,"… primitive Aixi … reinforcement learning methods, ie, the Natural Actor-Critic and the Reward-Weighted Regression algorithm. We demonstrate the efficiency of these reinforcement …","J Peters, S Schaal, B Schölkopf - Autonome Mobile Systeme 2007: 20 …, 2007 - Springer",J Peters,https://scholar.google.com/citations?user=-kIVAcAAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=-kIVAcAAAAAJ&engine=google_scholar_author&hl=en,-kIVAcAAAAAJ,S Schaal,https://scholar.google.com/citations?user=YGQs1AYAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=YGQs1AYAAAAJ&engine=google_scholar_author&hl=en,YGQs1AYAAAAJ,B Schölkopf,https://scholar.google.com/citations?user=DZ-fHPgAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=DZ-fHPgAAAAJ&engine=google_scholar_author&hl=en,DZ-fHPgAAAAJ,tu-darmstadt.de,PDF,https://www.ias.informatik.tu-darmstadt.de/uploads/Publications/Publications/Peters_POAMS_2007.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=s09kl0WTGdYJ,4.0,"https://scholar.google.com/scholar?cites=15427523925636304819&as_sdt=20000005&sciodt=0,21&hl=en&num=20",15427523925636304819,https://serpapi.com/search.json?as_sdt=20000005&cites=15427523925636304819&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:s09kl0WTGdYJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,21",https://serpapi.com/search.json?as_sdt=0%2C21&engine=google_scholar&hl=en&num=20&q=related%3As09kl0WTGdYJ%3Ascholar.google.com%2F&start=0,11.0,"https://scholar.google.com/scholar?cluster=15427523925636304819&hl=en&num=20&as_sdt=0,21",15427523925636304819,https://serpapi.com/search.json?as_sdt=0%2C21&cluster=15427523925636304819&engine=google_scholar&hl=en&num=20,,,,,,,,
9,Policy learning for motor skills,04cNeu01-mcJ,https://link.springer.com/chapter/10.1007/978-3-540-69162-4_25,… of supervised and reinforcement learning in order to aquire … forced to execute each motor primitive Aixi = bi in order to fulfill … that the Natural Actor-Critic outperforms both finite-difference …,"J Peters, S Schaal - International Conference on Neural Information …, 2007 - Springer",J Peters,https://scholar.google.com/citations?user=-kIVAcAAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=-kIVAcAAAAAJ&engine=google_scholar_author&hl=en,-kIVAcAAAAAJ,S Schaal,https://scholar.google.com/citations?user=YGQs1AYAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=YGQs1AYAAAAJ&engine=google_scholar_author&hl=en,YGQs1AYAAAAJ,,,,,researchgate.net,PDF,https://www.researchgate.net/profile/Jan-Peters-2/publication/41781907_Policy_Learning_for_Motor_Skills/links/00b7d51462f91f1984000000/Policy-Learning-for-Motor-Skills.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=04cNeu01-mcJ,22.0,"https://scholar.google.com/scholar?cites=7492360224155469779&as_sdt=20000005&sciodt=0,21&hl=en&num=20",7492360224155469779,https://serpapi.com/search.json?as_sdt=20000005&cites=7492360224155469779&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:04cNeu01-mcJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,21",https://serpapi.com/search.json?as_sdt=0%2C21&engine=google_scholar&hl=en&num=20&q=related%3A04cNeu01-mcJ%3Ascholar.google.com%2F&start=0,11.0,"https://scholar.google.com/scholar?cluster=7492360224155469779&hl=en&num=20&as_sdt=0,21",7492360224155469779,https://serpapi.com/search.json?as_sdt=0%2C21&cluster=7492360224155469779&engine=google_scholar&hl=en&num=20,,,,,,,,
10,Distributed optimal synchronization control of linear networked systems under unknown dynamics,PKh_bw36itYJ,https://ieeexplore.ieee.org/abstract/document/7963029/,"… Actor-critic structures [13] are used efficiently in RL, specifically … integral reinforcement learning (IRL) and actor-critic structures … Moreover, by using actor-critic-identifier approximators for …","F Tatari, MB Naghibi-Sistani… - 2017 American …, 2017 - ieeexplore.ieee.org",F Tatari,https://scholar.google.com/citations?user=kocqXnAAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=kocqXnAAAAAJ&engine=google_scholar_author&hl=en,kocqXnAAAAAJ,MB Naghibi-Sistani,https://scholar.google.com/citations?user=3OiE5loAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=3OiE5loAAAAJ&engine=google_scholar_author&hl=en,3OiE5loAAAAJ,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=PKh_bw36itYJ,13.0,"https://scholar.google.com/scholar?cites=15459443606514804796&as_sdt=20000005&sciodt=0,21&hl=en&num=20",15459443606514804796,https://serpapi.com/search.json?as_sdt=20000005&cites=15459443606514804796&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:PKh_bw36itYJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,21",https://serpapi.com/search.json?as_sdt=0%2C21&engine=google_scholar&hl=en&num=20&q=related%3APKh_bw36itYJ%3Ascholar.google.com%2F&start=0,,,,,,,,,,,,
11,Autonomous platoon control with integrated deep reinforcement learning and dynamic programming,OBOFKEZUGeoJ,https://ieeexplore.ieee.org/abstract/document/9951132/,"… In FH-DDPG, there is a pair of actor/critic networks associated with each time step, and the actors/critics are trained backward in time. Therefore, when we train the actor/critic of the first …","T Liu, L Lei, K Zheng, K Zhang - IEEE Internet of Things Journal, 2022 - ieeexplore.ieee.org",L Lei,https://scholar.google.com/citations?user=Ut0-14IAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=Ut0-14IAAAAJ&engine=google_scholar_author&hl=en,Ut0-14IAAAAJ,K Zheng,https://scholar.google.com/citations?user=IX_id5UAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=IX_id5UAAAAJ&engine=google_scholar_author&hl=en,IX_id5UAAAAJ,K Zhang,https://scholar.google.com/citations?user=hLT3_twAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=hLT3_twAAAAJ&engine=google_scholar_author&hl=en,hLT3_twAAAAJ,arxiv.org,PDF,https://arxiv.org/pdf/2206.07536,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=OBOFKEZUGeoJ,29.0,"https://scholar.google.com/scholar?cites=16868606539597157176&as_sdt=20000005&sciodt=0,21&hl=en&num=20",16868606539597157176,https://serpapi.com/search.json?as_sdt=20000005&cites=16868606539597157176&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:OBOFKEZUGeoJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,21",https://serpapi.com/search.json?as_sdt=0%2C21&engine=google_scholar&hl=en&num=20&q=related%3AOBOFKEZUGeoJ%3Ascholar.google.com%2F&start=0,3.0,"https://scholar.google.com/scholar?cluster=16868606539597157176&hl=en&num=20&as_sdt=0,21",16868606539597157176,https://serpapi.com/search.json?as_sdt=0%2C21&cluster=16868606539597157176&engine=google_scholar&hl=en&num=20,,,,,,,,
12,Data-driven containment control of discrete-time multi-agent systems via value iteration,6C5pqZEje6UJ,http://scis.scichina.com/en/2020/189205.pdf,"… multi-agent system via reinforcement learning. Containment control (CC… systems under the framework of reinforcement learning. … , we use an actor-critic neural network to approximate Jl …","Z Peng, J Hu, BK Ghosh - Science China. Information Sciences, 2020 - scis.scichina.com",Z Peng,https://scholar.google.com/citations?user=9AUL9JEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=9AUL9JEAAAAJ&engine=google_scholar_author&hl=en,9AUL9JEAAAAJ,J Hu,https://scholar.google.com/citations?user=KMoR83sAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=KMoR83sAAAAJ&engine=google_scholar_author&hl=en,KMoR83sAAAAJ,,,,,scichina.com,PDF,http://scis.scichina.com/en/2020/189205.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=6C5pqZEje6UJ,26.0,"https://scholar.google.com/scholar?cites=11924163546912993000&as_sdt=20000005&sciodt=0,21&hl=en&num=20",11924163546912993000,https://serpapi.com/search.json?as_sdt=20000005&cites=11924163546912993000&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:6C5pqZEje6UJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,21",https://serpapi.com/search.json?as_sdt=0%2C21&engine=google_scholar&hl=en&num=20&q=related%3A6C5pqZEje6UJ%3Ascholar.google.com%2F&start=0,5.0,"https://scholar.google.com/scholar?cluster=11924163546912993000&hl=en&num=20&as_sdt=0,21",11924163546912993000,https://serpapi.com/search.json?as_sdt=0%2C21&cluster=11924163546912993000&engine=google_scholar&hl=en&num=20,Pdf,"http://scholar.googleusercontent.com/scholar?q=cache:6C5pqZEje6UJ:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,21",,,,,,
13,Distributed Vibration Control of Large Flexible Satellite Solar Panel Via Reinforcement Learning,eLxUjkv4qiAJ,https://ieeexplore.ieee.org/abstract/document/10327318/,"… Different from the traditional actor-critic neural network model in policy iteration algorithm, in this paper, we just use one neural network to act the role of critic network, which represents …","J Huang, X Liu, H Liu - 2023 35th Chinese Control and …, 2023 - ieeexplore.ieee.org",,,,,,,,,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=eLxUjkv4qiAJ,,,,,"https://scholar.google.com/scholar?q=related:eLxUjkv4qiAJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,21",https://serpapi.com/search.json?as_sdt=0%2C21&engine=google_scholar&hl=en&num=20&q=related%3AeLxUjkv4qiAJ%3Ascholar.google.com%2F&start=0,,,,,,,,,,,,
14,Deep reinforcement learning aided platoon control relying on V2X information,VWX58wP_MSsJ,https://ieeexplore.ieee.org/abstract/document/9743615/,"… 3) Accuracy of Q-value estimations: As learning accurate Q-values is very important for the success of actor-critic algorithms, we examined the Q-values estimated by the critic after …","L Lei, T Liu, K Zheng, L Hanzo - IEEE Transactions on …, 2022 - ieeexplore.ieee.org",L Lei,https://scholar.google.com/citations?user=Ut0-14IAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=Ut0-14IAAAAJ&engine=google_scholar_author&hl=en,Ut0-14IAAAAJ,K Zheng,https://scholar.google.com/citations?user=IX_id5UAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=IX_id5UAAAAJ&engine=google_scholar_author&hl=en,IX_id5UAAAAJ,L Hanzo,https://scholar.google.com/citations?user=p0jnEW0AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=p0jnEW0AAAAJ&engine=google_scholar_author&hl=en,p0jnEW0AAAAJ,arxiv.org,PDF,https://arxiv.org/pdf/2203.15781,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=VWX58wP_MSsJ,43.0,"https://scholar.google.com/scholar?cites=3112549209932916053&as_sdt=20000005&sciodt=0,21&hl=en&num=20",3112549209932916053,https://serpapi.com/search.json?as_sdt=20000005&cites=3112549209932916053&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:VWX58wP_MSsJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,21",https://serpapi.com/search.json?as_sdt=0%2C21&engine=google_scholar&hl=en&num=20&q=related%3AVWX58wP_MSsJ%3Ascholar.google.com%2F&start=0,5.0,"https://scholar.google.com/scholar?cluster=3112549209932916053&hl=en&num=20&as_sdt=0,21",3112549209932916053,https://serpapi.com/search.json?as_sdt=0%2C21&cluster=3112549209932916053&engine=google_scholar&hl=en&num=20,,,,,,,,
15,General sum stochastic games with networked information flows,IFm3jXVW9HwJ,https://arxiv.org/abs/2205.02760,… actor-critic framework and describe how the different information structures from Section 3.4 modifies the actor critic … multiagent reinforcement learning and discussed its implications for …,"SHQ Li, LJ Ratliff, P Kumar - arXiv preprint arXiv:2205.02760, 2022 - arxiv.org",SHQ Li,https://scholar.google.com/citations?user=yZhro2IAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=yZhro2IAAAAJ&engine=google_scholar_author&hl=en,yZhro2IAAAAJ,LJ Ratliff,https://scholar.google.com/citations?user=LVPkbeYAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=LVPkbeYAAAAJ&engine=google_scholar_author&hl=en,LVPkbeYAAAAJ,P Kumar,https://scholar.google.com/citations?user=mh6kznYAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=mh6kznYAAAAJ&engine=google_scholar_author&hl=en,mh6kznYAAAAJ,arxiv.org,PDF,https://arxiv.org/pdf/2205.02760,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=IFm3jXVW9HwJ,,,,,"https://scholar.google.com/scholar?q=related:IFm3jXVW9HwJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,21",https://serpapi.com/search.json?as_sdt=0%2C21&engine=google_scholar&hl=en&num=20&q=related%3AIFm3jXVW9HwJ%3Ascholar.google.com%2F&start=0,3.0,"https://scholar.google.com/scholar?cluster=9003916617909229856&hl=en&num=20&as_sdt=0,21",9003916617909229856,https://serpapi.com/search.json?as_sdt=0%2C21&cluster=9003916617909229856&engine=google_scholar&hl=en&num=20,,"https://scholar.googleusercontent.com/scholar?q=cache:IFm3jXVW9HwJ:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,21",,,,,,
16,Robust optimal formation control of heterogeneous multi-agent system via reinforcement learning,fZINRmzvmDUJ,https://ieeexplore.ieee.org/abstract/document/9276397/,"… In [19], the RL algorithm was used in the identifier-actor-critic architecture to solve … actor-critic-identifier architecture to achieve the desired formation. In [21], the reinforcement learning (…","W Lin, W Zhao, H Liu - IEEE Access, 2020 - ieeexplore.ieee.org",W Zhao,https://scholar.google.com/citations?user=tzJ9BsoAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=tzJ9BsoAAAAJ&engine=google_scholar_author&hl=en,tzJ9BsoAAAAJ,,,,,,,,,ieee.org,PDF,https://ieeexplore.ieee.org/iel7/6287639/6514899/09276397.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=fZINRmzvmDUJ,15.0,"https://scholar.google.com/scholar?cites=3862099928781001341&as_sdt=20000005&sciodt=0,21&hl=en&num=20",3862099928781001341,https://serpapi.com/search.json?as_sdt=20000005&cites=3862099928781001341&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:fZINRmzvmDUJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,21",https://serpapi.com/search.json?as_sdt=0%2C21&engine=google_scholar&hl=en&num=20&q=related%3AfZINRmzvmDUJ%3Ascholar.google.com%2F&start=0,2.0,"https://scholar.google.com/scholar?cluster=3862099928781001341&hl=en&num=20&as_sdt=0,21",3862099928781001341,https://serpapi.com/search.json?as_sdt=0%2C21&cluster=3862099928781001341&engine=google_scholar&hl=en&num=20,,,,,,,,
17,Integral-Reinforcement-Learning-Based Hierarchical Optimal Evolutionary Strategy for Continuous Action Social Dilemma Games,48bJo8cKQjAJ,https://ieeexplore.ieee.org/abstract/document/10599362/,"… -follower structure, we introduce an integral reinforcement learning (RL) algorithm known as … Finally, actor/critic neural networks (NNs) are employed to implement the RL algorithm [53]…","L Fan, D Yu, Z Wang - IEEE Transactions on Computational …, 2024 - ieeexplore.ieee.org",D Yu,https://scholar.google.com/citations?user=sxN3HdcAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=sxN3HdcAAAAJ&engine=google_scholar_author&hl=en,sxN3HdcAAAAJ,,,,,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=48bJo8cKQjAJ,,,,,"https://scholar.google.com/scholar?q=related:48bJo8cKQjAJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,21",https://serpapi.com/search.json?as_sdt=0%2C21&engine=google_scholar&hl=en&num=20&q=related%3A48bJo8cKQjAJ%3Ascholar.google.com%2F&start=0,,,,,,,,,,,,
18,Reinforcement Learning for Synchronization of Heterogeneous Multiagent Systems by Improved -Functions,mdBa9Lr-ylUJ,https://ieeexplore.ieee.org/abstract/document/10690164/,"This article dedicates to investigating a methodology for enhancing adaptability to environmental changes of reinforcement learning (RL) techniques with data efficiency, by which a joint …","J Li, L Yuan, W Cheng, T Chai… - IEEE Transactions on …, 2024 - ieeexplore.ieee.org",,,,,,,,,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=mdBa9Lr-ylUJ,,,,,"https://scholar.google.com/scholar?q=related:mdBa9Lr-ylUJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,21",https://serpapi.com/search.json?as_sdt=0%2C21&engine=google_scholar&hl=en&num=20&q=related%3AmdBa9Lr-ylUJ%3Ascholar.google.com%2F&start=0,2.0,"https://scholar.google.com/scholar?cluster=6182033517436391577&hl=en&num=20&as_sdt=0,21",6182033517436391577,https://serpapi.com/search.json?as_sdt=0%2C21&cluster=6182033517436391577&engine=google_scholar&hl=en&num=20,,,,,,,,
19,Event-triggered distributed control of nonlinear interconnected systems using online reinforcement learning with exploration,dIzKaYwe6hoJ,https://ieeexplore.ieee.org/abstract/document/8027115/,"In this paper, a distributed control scheme for an interconnected system composed of uncertain input affine nonlinear subsystems with event triggered state feedback is presented by …","V Narayanan, S Jagannathan - IEEE transactions on …, 2017 - ieeexplore.ieee.org",V Narayanan,https://scholar.google.com/citations?user=rirGB10AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=rirGB10AAAAJ&engine=google_scholar_author&hl=en,rirGB10AAAAJ,S Jagannathan,https://scholar.google.com/citations?user=RTewL_wAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=RTewL_wAAAAJ&engine=google_scholar_author&hl=en,RTewL_wAAAAJ,,,,,ieee.org,PDF,https://ieeexplore.ieee.org/ielaam/6221036/8438339/8027115-aam.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=dIzKaYwe6hoJ,93.0,"https://scholar.google.com/scholar?cites=1939396177955556468&as_sdt=20000005&sciodt=0,21&hl=en&num=20",1939396177955556468,https://serpapi.com/search.json?as_sdt=20000005&cites=1939396177955556468&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:dIzKaYwe6hoJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,21",https://serpapi.com/search.json?as_sdt=0%2C21&engine=google_scholar&hl=en&num=20&q=related%3AdIzKaYwe6hoJ%3Ascholar.google.com%2F&start=0,5.0,"https://scholar.google.com/scholar?cluster=1939396177955556468&hl=en&num=20&as_sdt=0,21",1939396177955556468,https://serpapi.com/search.json?as_sdt=0%2C21&cluster=1939396177955556468&engine=google_scholar&hl=en&num=20,,,,,,,,
20,Containment control of heterogeneous systems with non-autonomous leaders: A distributed optimal model reference approach,jzPWvG7QwzIJ,https://ieeexplore.ieee.org/abstract/document/8501968/,"… off-policy reinforcement learning approach implemented on an actor-critic structure is utilized … In this section, the off-policy RL algorithm is combined with actor-critic structure [48] to the …","Y Yang, S Cheng, Y Yin, DC Wunsch - IEEE Access, 2018 - ieeexplore.ieee.org",Y Yang,https://scholar.google.com/citations?user=smwQtUwAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=smwQtUwAAAAJ&engine=google_scholar_author&hl=en,smwQtUwAAAAJ,DC Wunsch,https://scholar.google.com/citations?user=fQC7bIoAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=fQC7bIoAAAAJ&engine=google_scholar_author&hl=en,fQC7bIoAAAAJ,,,,,ieee.org,PDF,https://ieeexplore.ieee.org/iel7/6287639/8274985/08501968.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=jzPWvG7QwzIJ,7.0,"https://scholar.google.com/scholar?cites=3657996496388109199&as_sdt=20000005&sciodt=0,21&hl=en&num=20",3657996496388109199,https://serpapi.com/search.json?as_sdt=20000005&cites=3657996496388109199&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:jzPWvG7QwzIJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,21",https://serpapi.com/search.json?as_sdt=0%2C21&engine=google_scholar&hl=en&num=20&q=related%3AjzPWvG7QwzIJ%3Ascholar.google.com%2F&start=0,2.0,"https://scholar.google.com/scholar?cluster=3657996496388109199&hl=en&num=20&as_sdt=0,21",3657996496388109199,https://serpapi.com/search.json?as_sdt=0%2C21&cluster=3657996496388109199&engine=google_scholar&hl=en&num=20,,,,,,,,
1,Digital Twin-Driven Formation Control of ROVs: An Integral Reinforcement Learning-Based Solution,VrShdxUnXCkJ,https://ieeexplore.ieee.org/abstract/document/10680189/,"… [18] developed a soft actor–critic (SAC) based grasp strategy … Based on this, an integral reinforcement learning (IRL)-based … Fu, “Social learning with actor–critic for dynamic grasping of …","T Zhang, J Yan, X Yang, C Chen… - IEEE Transactions on …, 2024 - ieeexplore.ieee.org",J Yan,https://scholar.google.com/citations?user=HtjDQ1AAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=HtjDQ1AAAAAJ&engine=google_scholar_author&hl=en,HtjDQ1AAAAAJ,X Yang,https://scholar.google.com/citations?user=ISADH70AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=ISADH70AAAAJ&engine=google_scholar_author&hl=en,ISADH70AAAAJ,C Chen,https://scholar.google.com/citations?user=NGQrfUwAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=NGQrfUwAAAAJ&engine=google_scholar_author&hl=en,NGQrfUwAAAAJ,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=VrShdxUnXCkJ,,,,,"https://scholar.google.com/scholar?q=related:VrShdxUnXCkJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,44",https://serpapi.com/search.json?as_sdt=0%2C44&engine=google_scholar&hl=en&num=20&q=related%3AVrShdxUnXCkJ%3Ascholar.google.com%2F&start=20,,,,,,,,,,,,
2,Deep reinforcement learning for wireless scheduling in distributed networked control,OwFFUczrHpkJ,https://arxiv.org/abs/2109.12562,"… [26] developed an actor-critic DRL for learning stochastic policies with continuous action space for scheduling, power allocation, and modulation scheme adaptation. However, it has …","G Pang, K Huang, DE Quevedo, B Vucetic, Y Li… - arXiv preprint arXiv …, 2021 - arxiv.org",G Pang,https://scholar.google.com/citations?user=v1X_GU4AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=v1X_GU4AAAAJ&engine=google_scholar_author&hl=en,v1X_GU4AAAAJ,DE Quevedo,https://scholar.google.com/citations?user=s0MIz4sAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=s0MIz4sAAAAJ&engine=google_scholar_author&hl=en,s0MIz4sAAAAJ,B Vucetic,https://scholar.google.com/citations?user=uS5QyboAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=uS5QyboAAAAJ&engine=google_scholar_author&hl=en,uS5QyboAAAAJ,arxiv.org,PDF,https://arxiv.org/pdf/2109.12562,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=OwFFUczrHpkJ,18.0,"https://scholar.google.com/scholar?cites=11033515399873626427&as_sdt=5,44&sciodt=0,44&hl=en&num=20",11033515399873626427,https://serpapi.com/search.json?as_sdt=5%2C44&cites=11033515399873626427&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:OwFFUczrHpkJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,44",https://serpapi.com/search.json?as_sdt=0%2C44&engine=google_scholar&hl=en&num=20&q=related%3AOwFFUczrHpkJ%3Ascholar.google.com%2F&start=20,2.0,"https://scholar.google.com/scholar?cluster=11033515399873626427&hl=en&num=20&as_sdt=0,44",11033515399873626427,https://serpapi.com/search.json?as_sdt=0%2C44&cluster=11033515399873626427&engine=google_scholar&hl=en&num=20,,"https://scholar.googleusercontent.com/scholar?q=cache:OwFFUczrHpkJ:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,44",Y Li,https://scholar.google.com/citations?user=xLj8A2EAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=xLj8A2EAAAAJ&engine=google_scholar_author&hl=en,xLj8A2EAAAAJ,,
3,Containment Control of Heterogeneous Systems with Non-Autonomous Leaders: A Distributed Optimal Model Reference Approach,iX0THxC6lMEJ,https://ieeexplore.ieee.org/ielaam/6287639/8274985/8501968-aam.pdf,"… off-policy reinforcement learning approach implemented on an actor-critic structure is utilized … In this section, the off-policy RL algorithm is combined with actor-critic structure [48] to the …","Y YONGLIANG, S CHENG, Y YIN, C DONALD… - ieeexplore.ieee.org",C DONALD,https://scholar.google.com/citations?user=fQC7bIoAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=fQC7bIoAAAAJ&engine=google_scholar_author&hl=en,fQC7bIoAAAAJ,,,,,,,,,ieee.org,PDF,https://ieeexplore.ieee.org/ielaam/6287639/8274985/8501968-aam.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=iX0THxC6lMEJ,,,,,"https://scholar.google.com/scholar?q=related:iX0THxC6lMEJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,44",https://serpapi.com/search.json?as_sdt=0%2C44&engine=google_scholar&hl=en&num=20&q=related%3AiX0THxC6lMEJ%3Ascholar.google.com%2F&start=20,,,,,Pdf,"https://scholar.googleusercontent.com/scholar?q=cache:iX0THxC6lMEJ:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,44",,,,,,
4,Robust Reinforcement Learning Control of a Furuta Pendulum,_O1ZqN4NDr4J,https://lup.lub.lu.se/student-papers/record/9069000/file/9069001.pdf,"… For Reinforcement Learning, and specifically an actor-critic method, the objective is to perturb the state that the actor is given in such a way that it takes a suboptimal action. The function …",P Olhager - 2021 - lup.lub.lu.se,,,,,,,,,,,,,lu.se,PDF,https://lup.lub.lu.se/student-papers/record/9069000/file/9069001.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=_O1ZqN4NDr4J,,,,,"https://scholar.google.com/scholar?q=related:_O1ZqN4NDr4J:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,44",https://serpapi.com/search.json?as_sdt=0%2C44&engine=google_scholar&hl=en&num=20&q=related%3A_O1ZqN4NDr4J%3Ascholar.google.com%2F&start=20,2.0,"https://scholar.google.com/scholar?cluster=13694898766838623740&hl=en&num=20&as_sdt=0,44",13694898766838623740,https://serpapi.com/search.json?as_sdt=0%2C44&cluster=13694898766838623740&engine=google_scholar&hl=en&num=20,Pdf,"https://scholar.googleusercontent.com/scholar?q=cache:_O1ZqN4NDr4J:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,44",,,,,,
5,Self-predictive universal AI,obLq9N9NL2YJ,https://proceedings.neurips.cc/paper_files/paper/2023/hash/56a225639da77e8f7c0409f6d5ba996b-Abstract-Conference.html,"… We prove that Self-AIXI converges to AIXI, and inherits a series of properties like maximal … Self-AIXI generates action-data in a similar fashion as other Temporal Difference TD(0) [1] …","E Catt, J Grau-Moya, M Hutter… - Advances in …, 2023 - proceedings.neurips.cc",E Catt,https://scholar.google.com/citations?user=d1JYeMIAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=d1JYeMIAAAAJ&engine=google_scholar_author&hl=en,d1JYeMIAAAAJ,J Grau-Moya,https://scholar.google.com/citations?user=u8ccN8sAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=u8ccN8sAAAAJ&engine=google_scholar_author&hl=en,u8ccN8sAAAAJ,M Hutter,https://scholar.google.com/citations?user=7hmCntEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=7hmCntEAAAAJ&engine=google_scholar_author&hl=en,7hmCntEAAAAJ,neurips.cc,PDF,https://proceedings.neurips.cc/paper_files/paper/2023/file/56a225639da77e8f7c0409f6d5ba996b-Paper-Conference.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=obLq9N9NL2YJ,3.0,"https://scholar.google.com/scholar?cites=7363189540056117921&as_sdt=5,44&sciodt=0,44&hl=en&num=20",7363189540056117921,https://serpapi.com/search.json?as_sdt=5%2C44&cites=7363189540056117921&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:obLq9N9NL2YJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,44",https://serpapi.com/search.json?as_sdt=0%2C44&engine=google_scholar&hl=en&num=20&q=related%3AobLq9N9NL2YJ%3Ascholar.google.com%2F&start=20,3.0,"https://scholar.google.com/scholar?cluster=7363189540056117921&hl=en&num=20&as_sdt=0,44",7363189540056117921,https://serpapi.com/search.json?as_sdt=0%2C44&cluster=7363189540056117921&engine=google_scholar&hl=en&num=20,,"https://scholar.googleusercontent.com/scholar?q=cache:obLq9N9NL2YJ:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,44",,,,,,
6,Data-Driven Distributed Current Sharing Consensus Optimal Control of DC Microgrids via Reinforcement Learning,gJhh2-TLKZAJ,https://ieeexplore.ieee.org/abstract/document/10444694/,"… Wu, “Model-free distributed consensus control based on actor–critic framework for discrete-time nonlinear multiagent systems,” IEEE Trans. Syst., Man, Cybern., Syst., vol. 50, no. …","X Dong, H Zhang, X Xie, Z Ming - IEEE Transactions on …, 2024 - ieeexplore.ieee.org",H Zhang,https://scholar.google.com/citations?user=nbQu9moAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=nbQu9moAAAAJ&engine=google_scholar_author&hl=en,nbQu9moAAAAJ,,,,,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=gJhh2-TLKZAJ,4.0,"https://scholar.google.com/scholar?cites=10388058199300348032&as_sdt=5,44&sciodt=0,44&hl=en&num=20",10388058199300348032,https://serpapi.com/search.json?as_sdt=5%2C44&cites=10388058199300348032&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:gJhh2-TLKZAJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,44",https://serpapi.com/search.json?as_sdt=0%2C44&engine=google_scholar&hl=en&num=20&q=related%3AgJhh2-TLKZAJ%3Ascholar.google.com%2F&start=20,,,,,,,,,,,,
7,Differential graphical game‐based multi‐agent tracking control using integral reinforcement learning,5Eo0uV-uBnMJ,https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/cth2.12667,"… In this section, a data-driven off-policy IRL algorithm using actor–critic NNs is designed for approximating the Paretooptimal strategy of the multi-agent cooperative differential graphical …","Y Guo, Q Sun, Y Wang, Q Pan - IET Control Theory & …, 2024 - Wiley Online Library",,,,,,,,,,,,,wiley.com,PDF,https://ietresearch.onlinelibrary.wiley.com/doi/pdf/10.1049/cth2.12667,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=5Eo0uV-uBnMJ,1.0,"https://scholar.google.com/scholar?cites=8288503890374314724&as_sdt=5,44&sciodt=0,44&hl=en&num=20",8288503890374314724,https://serpapi.com/search.json?as_sdt=5%2C44&cites=8288503890374314724&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:5Eo0uV-uBnMJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,44",https://serpapi.com/search.json?as_sdt=0%2C44&engine=google_scholar&hl=en&num=20&q=related%3A5Eo0uV-uBnMJ%3Ascholar.google.com%2F&start=20,,,,,,,,,,,Full View,"https://scholar.google.com/scholar?output=instlink&q=info:5Eo0uV-uBnMJ:scholar.google.com/&hl=en&num=20&as_sdt=0,44&scillfp=6554821645230350288&oi=lle"
8,Review and Evaluation of Multi-Agent Control Applications for Energy Management in Buildings.,nLRb84Xr6wsJ,https://www.researchgate.net/profile/Panagiotis-Michailidis-3/publication/384382023_Review_and_Evaluation_of_Multi-Agent_Control_Applications_for_Energy_Management_in_Buildings/links/66f6dfeb906bca2ac3d205bc/Review-and-Evaluation-of-Multi-Agent-Control-Applications-for-Energy-Management-in-Buildings.pdf,… Model Predictive Control (MPC) and reinforcement learning (RL) along with their synergistic … [65] proposed a Multi-Agent Actor–critic (MAAC) approach to minimize energy costs while …,"P Michailidis, I Michailidis… - Energies (19961073 …, 2024 - researchgate.net",P Michailidis,https://scholar.google.com/citations?user=E5RFom0AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=E5RFom0AAAAJ&engine=google_scholar_author&hl=en,E5RFom0AAAAJ,I Michailidis,https://scholar.google.com/citations?user=hUC2CC8AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=hUC2CC8AAAAJ&engine=google_scholar_author&hl=en,hUC2CC8AAAAJ,,,,,researchgate.net,PDF,https://www.researchgate.net/profile/Panagiotis-Michailidis-3/publication/384382023_Review_and_Evaluation_of_Multi-Agent_Control_Applications_for_Energy_Management_in_Buildings/links/66f6dfeb906bca2ac3d205bc/Review-and-Evaluation-of-Multi-Agent-Control-Applications-for-Energy-Management-in-Buildings.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=nLRb84Xr6wsJ,,,,,"https://scholar.google.com/scholar?q=related:nLRb84Xr6wsJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,44",https://serpapi.com/search.json?as_sdt=0%2C44&engine=google_scholar&hl=en&num=20&q=related%3AnLRb84Xr6wsJ%3Ascholar.google.com%2F&start=20,5.0,"https://scholar.google.com/scholar?cluster=859039114490262684&hl=en&num=20&as_sdt=0,44",859039114490262684,https://serpapi.com/search.json?as_sdt=0%2C44&cluster=859039114490262684&engine=google_scholar&hl=en&num=20,Pdf,"https://scholar.googleusercontent.com/scholar?q=cache:nLRb84Xr6wsJ:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,44",,,,,,
9,Reinforcement learning with limited prior knowledge in long-term environments,jeIACDbijtkJ,https://drive.google.com/file/d/1wIPPk9huo_QsQb_pYpVHBA3Tko0R1i8M/view,"… , both of which are reinforcement learners: the AIXI [81] which maximises future reward with a … and actor-critic methods have been especially popular in deep reinforcement learning …",D Bossens - 2020 - drive.google.com,D Bossens,https://scholar.google.com/citations?user=w2feGIoAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=w2feGIoAAAAJ&engine=google_scholar_author&hl=en,w2feGIoAAAAJ,,,,,,,,,google.com,PDF,https://drive.google.com/file/d/1wIPPk9huo_QsQb_pYpVHBA3Tko0R1i8M/view,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=jeIACDbijtkJ,,,,,"https://scholar.google.com/scholar?q=related:jeIACDbijtkJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,44",https://serpapi.com/search.json?as_sdt=0%2C44&engine=google_scholar&hl=en&num=20&q=related%3AjeIACDbijtkJ%3Ascholar.google.com%2F&start=20,3.0,"https://scholar.google.com/scholar?cluster=15676716074613662349&hl=en&num=20&as_sdt=0,44",15676716074613662349,https://serpapi.com/search.json?as_sdt=0%2C44&cluster=15676716074613662349&engine=google_scholar&hl=en&num=20,Pdf,,,,,,,
10,AVDDPG: Federated reinforcement learning applied to autonomous platoon control,5o8jjMX9NxYJ,https://arxiv.org/abs/2207.03484,… Applications of FL led to the development and study of federated reinforcement learning (FRL). Few works exist on the topic of FRL applied to autonomous vehicle (AV) platoons. In …,"C Boin, L Lei, SX Yang - arXiv preprint arXiv:2207.03484, 2022 - arxiv.org",L Lei,https://scholar.google.com/citations?user=Ut0-14IAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=Ut0-14IAAAAJ&engine=google_scholar_author&hl=en,Ut0-14IAAAAJ,SX Yang,https://scholar.google.com/citations?user=cyRDiPMAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=cyRDiPMAAAAJ&engine=google_scholar_author&hl=en,cyRDiPMAAAAJ,,,,,arxiv.org,PDF,https://arxiv.org/pdf/2207.03484,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=5o8jjMX9NxYJ,3.0,"https://scholar.google.com/scholar?cites=1601027217455026150&as_sdt=5,44&sciodt=0,44&hl=en&num=20",1601027217455026150,https://serpapi.com/search.json?as_sdt=5%2C44&cites=1601027217455026150&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:5o8jjMX9NxYJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,44",https://serpapi.com/search.json?as_sdt=0%2C44&engine=google_scholar&hl=en&num=20&q=related%3A5o8jjMX9NxYJ%3Ascholar.google.com%2F&start=20,5.0,"https://scholar.google.com/scholar?cluster=1601027217455026150&hl=en&num=20&as_sdt=0,44",1601027217455026150,https://serpapi.com/search.json?as_sdt=0%2C44&cluster=1601027217455026150&engine=google_scholar&hl=en&num=20,,"https://scholar.googleusercontent.com/scholar?q=cache:5o8jjMX9NxYJ:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,44",,,,,,
11,The benefits of model-based generalization in reinforcement learning,949fPq4cAbAJ,https://arxiv.org/abs/2211.02222,"… In particular, we use DQN instead of actor-critic and, since our … An AIXI agent maximizes expected return over all computable world … While AIXI is itself not computable, computable …","K Young, A Ramesh, L Kirsch… - arXiv preprint arXiv …, 2022 - arxiv.org",K Young,https://scholar.google.com/citations?user=zI2uHi8AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=zI2uHi8AAAAJ&engine=google_scholar_author&hl=en,zI2uHi8AAAAJ,A Ramesh,https://scholar.google.com/citations?user=60K82BkAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=60K82BkAAAAJ&engine=google_scholar_author&hl=en,60K82BkAAAAJ,L Kirsch,https://scholar.google.com/citations?user=w8AkOEAAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=w8AkOEAAAAAJ&engine=google_scholar_author&hl=en,w8AkOEAAAAAJ,arxiv.org,PDF,https://arxiv.org/pdf/2211.02222,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=949fPq4cAbAJ,17.0,"https://scholar.google.com/scholar?cites=12682449560348364791&as_sdt=5,44&sciodt=0,44&hl=en&num=20",12682449560348364791,https://serpapi.com/search.json?as_sdt=5%2C44&cites=12682449560348364791&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:949fPq4cAbAJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,44",https://serpapi.com/search.json?as_sdt=0%2C44&engine=google_scholar&hl=en&num=20&q=related%3A949fPq4cAbAJ%3Ascholar.google.com%2F&start=20,7.0,"https://scholar.google.com/scholar?cluster=12682449560348364791&hl=en&num=20&as_sdt=0,44",12682449560348364791,https://serpapi.com/search.json?as_sdt=0%2C44&cluster=12682449560348364791&engine=google_scholar&hl=en&num=20,,"https://scholar.googleusercontent.com/scholar?q=cache:949fPq4cAbAJ:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,44",,,,,,
12,Goal-directed online learning of predictive models,GFgkjb32sMwJ,https://link.springer.com/chapter/10.1007/978-3-642-29946-9_6,"… Our approach is (loosely) based on the actor-critic framework. Model parameters are estimated in an online fashion, interleaved with steps of data gathering and policy optimization. The …","SCW Ong, Y Grinberg, J Pineau - … Advances in Reinforcement Learning …, 2012 - Springer",Y Grinberg,https://scholar.google.com/citations?user=z-sG3n4AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=z-sG3n4AAAAJ&engine=google_scholar_author&hl=en,z-sG3n4AAAAJ,J Pineau,https://scholar.google.com/citations?user=CEt6_mMAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=CEt6_mMAAAAJ&engine=google_scholar_author&hl=en,CEt6_mMAAAAJ,,,,,psu.edu,PDF,https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=91477e628c016f4583a113a287a62cd41fe6509d,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=GFgkjb32sMwJ,6.0,"https://scholar.google.com/scholar?cites=14749560073615595544&as_sdt=5,44&sciodt=0,44&hl=en&num=20",14749560073615595544,https://serpapi.com/search.json?as_sdt=5%2C44&cites=14749560073615595544&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:GFgkjb32sMwJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,44",https://serpapi.com/search.json?as_sdt=0%2C44&engine=google_scholar&hl=en&num=20&q=related%3AGFgkjb32sMwJ%3Ascholar.google.com%2F&start=20,10.0,"https://scholar.google.com/scholar?cluster=14749560073615595544&hl=en&num=20&as_sdt=0,44",14749560073615595544,https://serpapi.com/search.json?as_sdt=0%2C44&cluster=14749560073615595544&engine=google_scholar&hl=en&num=20,,,,,,,,
13,Real-Time Autonomic Decision Making Under Uncertain Environments for UAV-Based Search-And-Rescue Missions,Rmz-M6du3gUJ,https://search.proquest.com/openview/8e4289d95fd7ef073ac285969bb521fc/1?pq-origsite=gscholar&cbl=18750&diss=y,… actor-critic based Multi-Agent Deep Reinforcement Learning (… • Imperfect communication and partial observability—the … to solve the partial observability/limited communication problem. …,V Sadhu - 2020 - search.proquest.com,V Sadhu,https://scholar.google.com/citations?user=K5GcTPYAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=K5GcTPYAAAAJ&engine=google_scholar_author&hl=en,K5GcTPYAAAAJ,,,,,,,,,rutgers.edu,PDF,https://rucore.libraries.rutgers.edu/rutgers-lib/64809/PDF/1/play/,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=Rmz-M6du3gUJ,,,,,"https://scholar.google.com/scholar?q=related:Rmz-M6du3gUJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,44",https://serpapi.com/search.json?as_sdt=0%2C44&engine=google_scholar&hl=en&num=20&q=related%3ARmz-M6du3gUJ%3Ascholar.google.com%2F&start=20,3.0,"https://scholar.google.com/scholar?cluster=422897079430310982&hl=en&num=20&as_sdt=0,44",422897079430310982,https://serpapi.com/search.json?as_sdt=0%2C44&cluster=422897079430310982&engine=google_scholar&hl=en&num=20,,,,,,,,
14,Model-free output consensus control for partially observable heterogeneous multivehicle systems,_MQDn2IEaikJ,https://ieeexplore.ieee.org/abstract/document/9043476/,"… With a distributed adaptive observer, a model-free reinforcement learning (RL) algorithm … In this section, the Q-function-based ADP approach is implemented via actor-critic structure …","Y Sun, X Chen, W Wang, H Fu… - IEEE Internet of Things …, 2020 - ieeexplore.ieee.org",Y Sun,https://scholar.google.com/citations?user=jhV1sqwAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=jhV1sqwAAAAJ&engine=google_scholar_author&hl=en,jhV1sqwAAAAJ,X Chen,https://scholar.google.com/citations?user=Jcrh5xAAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=Jcrh5xAAAAAJ&engine=google_scholar_author&hl=en,Jcrh5xAAAAAJ,W Wang,https://scholar.google.com/citations?user=UedS9LQAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=UedS9LQAAAAJ&engine=google_scholar_author&hl=en,UedS9LQAAAAJ,researchgate.net,PDF,https://www.researchgate.net/profile/Yipu-Sun/publication/340074300_Model-Free_Output_Consensus_Control_for_Partially_Observable_Heterogeneous_Multi-vehicle_Systems/links/5f18f227a6fdcc9626aa290e/Model-Free-Output-Consensus-Control-for-Partially-Observable-Heterogeneous-Multi-vehicle-Systems.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=_MQDn2IEaikJ,11.0,"https://scholar.google.com/scholar?cites=2984202524707505404&as_sdt=5,44&sciodt=0,44&hl=en&num=20",2984202524707505404,https://serpapi.com/search.json?as_sdt=5%2C44&cites=2984202524707505404&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:_MQDn2IEaikJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,44",https://serpapi.com/search.json?as_sdt=0%2C44&engine=google_scholar&hl=en&num=20&q=related%3A_MQDn2IEaikJ%3Ascholar.google.com%2F&start=20,2.0,"https://scholar.google.com/scholar?cluster=2984202524707505404&hl=en&num=20&as_sdt=0,44",2984202524707505404,https://serpapi.com/search.json?as_sdt=0%2C44&cluster=2984202524707505404&engine=google_scholar&hl=en&num=20,,,,,,,,
15,An online event-triggered near-optimal controller for Nash solution in interconnected system,YTdosD2CyigJ,https://ieeexplore.ieee.org/abstract/document/9016373/,… The near-optimal control policies are generated online only at events using actor-critic neural network architecture whose weights are updated too at the same instants. The approach …,"NK Dhar, NK Verma, L Behera - IEEE Transactions on Neural …, 2020 - ieeexplore.ieee.org",NK Dhar,https://scholar.google.com/citations?user=xH9YlKQAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=xH9YlKQAAAAJ&engine=google_scholar_author&hl=en,xH9YlKQAAAAJ,NK Verma,https://scholar.google.com/citations?user=Iir1EBsAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=Iir1EBsAAAAJ&engine=google_scholar_author&hl=en,Iir1EBsAAAAJ,L Behera,https://scholar.google.com/citations?user=QWTcyP8AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=QWTcyP8AAAAJ&engine=google_scholar_author&hl=en,QWTcyP8AAAAJ,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=YTdosD2CyigJ,12.0,"https://scholar.google.com/scholar?cites=2939304908276905825&as_sdt=5,44&sciodt=0,44&hl=en&num=20",2939304908276905825,https://serpapi.com/search.json?as_sdt=5%2C44&cites=2939304908276905825&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:YTdosD2CyigJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,44",https://serpapi.com/search.json?as_sdt=0%2C44&engine=google_scholar&hl=en&num=20&q=related%3AYTdosD2CyigJ%3Ascholar.google.com%2F&start=20,3.0,"https://scholar.google.com/scholar?cluster=2939304908276905825&hl=en&num=20&as_sdt=0,44",2939304908276905825,https://serpapi.com/search.json?as_sdt=0%2C44&cluster=2939304908276905825&engine=google_scholar&hl=en&num=20,,,,,,,,
16,Machine learning of motor skills for robotics,Wheao5LtNTcJ,https://search.proquest.com/openview/7a41ca55ed6965156dc397c853b40b14/1?pq-origsite=gscholar&cbl=18750,"… classes of novel reinforcement learning methods, ie, the Natural Actor-Critic and the Reward… with a mapping xi = fi(q,q,t) can be forced to execute each motor primitive Aixi = bi in order to …",JR Peters - 2007 - search.proquest.com,JR Peters,https://scholar.google.com/citations?user=-kIVAcAAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=-kIVAcAAAAAJ&engine=google_scholar_author&hl=en,-kIVAcAAAAAJ,,,,,,,,,mpg.de,PDF,https://pure.mpg.de/rest/items/item_1789551/component/file_3033260/content,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=Wheao5LtNTcJ,104.0,"https://scholar.google.com/scholar?cites=3978347059917494106&as_sdt=5,44&sciodt=0,44&hl=en&num=20",3978347059917494106,https://serpapi.com/search.json?as_sdt=5%2C44&cites=3978347059917494106&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:Wheao5LtNTcJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,44",https://serpapi.com/search.json?as_sdt=0%2C44&engine=google_scholar&hl=en&num=20&q=related%3AWheao5LtNTcJ%3Ascholar.google.com%2F&start=20,12.0,"https://scholar.google.com/scholar?cluster=3978347059917494106&hl=en&num=20&as_sdt=0,44",3978347059917494106,https://serpapi.com/search.json?as_sdt=0%2C44&cluster=3978347059917494106&engine=google_scholar&hl=en&num=20,Book,,,,,,,
17,Learning causal state representations of partially observable environments,V49aVkmME14J,https://arxiv.org/abs/1906.10437,"… In sum, we propose a fourth approach for learning latent representations in partial observability settings with causal states, and provide theoretical guarantees from both computational …","A Zhang, ZC Lipton, L Pineda… - arXiv preprint arXiv …, 2019 - arxiv.org",A Zhang,https://scholar.google.com/citations?user=mXtH1UYAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=mXtH1UYAAAAJ&engine=google_scholar_author&hl=en,mXtH1UYAAAAJ,ZC Lipton,https://scholar.google.com/citations?user=MN9Kfg8AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=MN9Kfg8AAAAJ&engine=google_scholar_author&hl=en,MN9Kfg8AAAAJ,L Pineda,https://scholar.google.com/citations?user=rebEn8oAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=rebEn8oAAAAJ&engine=google_scholar_author&hl=en,rebEn8oAAAAJ,arxiv.org,PDF,https://arxiv.org/pdf/1906.10437,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=V49aVkmME14J,58.0,"https://scholar.google.com/scholar?cites=6778916110732005207&as_sdt=5,44&sciodt=0,44&hl=en&num=20",6778916110732005207,https://serpapi.com/search.json?as_sdt=5%2C44&cites=6778916110732005207&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:V49aVkmME14J:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,44",https://serpapi.com/search.json?as_sdt=0%2C44&engine=google_scholar&hl=en&num=20&q=related%3AV49aVkmME14J%3Ascholar.google.com%2F&start=20,5.0,"https://scholar.google.com/scholar?cluster=6778916110732005207&hl=en&num=20&as_sdt=0,44",6778916110732005207,https://serpapi.com/search.json?as_sdt=0%2C44&cluster=6778916110732005207&engine=google_scholar&hl=en&num=20,,"https://scholar.googleusercontent.com/scholar?q=cache:V49aVkmME14J:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,44",,,,,,
18,Deep Reinforcement Learning-Based DoS Attack and its Countermeasures in Cyber-Physical Systems,FiE55scXcBIJ,https://search.proquest.com/openview/6925d4a4147754a4f0c6186d69212679/1?pq-origsite=gscholar&cbl=2026366&diss=y,"… Second, we propose a DDPG-based algorithm which integrates the advantages of DQN and actor-critic to obtain an effective learning-based attack power allocation in continuous space…",M Huang - 2022 - search.proquest.com,,,,,,,,,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=FiE55scXcBIJ,,,,,"https://scholar.google.com/scholar?q=related:FiE55scXcBIJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,44",https://serpapi.com/search.json?as_sdt=0%2C44&engine=google_scholar&hl=en&num=20&q=related%3AFiE55scXcBIJ%3Ascholar.google.com%2F&start=20,,,,,,,,,,,,
19,State entropy maximization in POMDPs,yIHmNSisytwJ,https://www.politesi.polimi.it/handle/10589/219731,"… State Entropy Maximization to the POMDP framework. We will do … Actor-critic methods have been designed to combine the … Within the actor-critic methods, the Soft Actor-Critic (SAC) [14] …",D CIRINO - 2022 - politesi.polimi.it,,,,,,,,,,,,,polimi.it,PDF,https://www.politesi.polimi.it/bitstream/10589/219731/3/2024_04_Cirino_Tesi_01.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=yIHmNSisytwJ,,,,,"https://scholar.google.com/scholar?q=related:yIHmNSisytwJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,44",https://serpapi.com/search.json?as_sdt=0%2C44&engine=google_scholar&hl=en&num=20&q=related%3AyIHmNSisytwJ%3Ascholar.google.com%2F&start=20,,,,,,"https://scholar.googleusercontent.com/scholar?q=cache:yIHmNSisytwJ:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,44",,,,,,
20,Adaptive-critic design for decentralized event-triggered control of constrained nonlinear interconnected systems within an identifier-critic framework,pKgOWAs5I0MJ,https://ieeexplore.ieee.org/abstract/document/9314069/,"… This success is mainly attributed to its special actor–critic dual networks framework. Specifically, an actor releases an action to the systems/surroundings, and a critic makes an …","X Huo, HR Karimi, X Zhao, B Wang… - IEEE Transactions on …, 2021 - ieeexplore.ieee.org",HR Karimi,https://scholar.google.com/citations?user=YcTS0ZMAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=YcTS0ZMAAAAJ&engine=google_scholar_author&hl=en,YcTS0ZMAAAAJ,X Zhao,https://scholar.google.com/citations?user=uYkpwCAAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=uYkpwCAAAAAJ&engine=google_scholar_author&hl=en,uYkpwCAAAAAJ,B Wang,https://scholar.google.com/citations?user=9sau-6oAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=9sau-6oAAAAJ&engine=google_scholar_author&hl=en,9sau-6oAAAAJ,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=pKgOWAs5I0MJ,157.0,"https://scholar.google.com/scholar?cites=4837773145610823844&as_sdt=5,44&sciodt=0,44&hl=en&num=20",4837773145610823844,https://serpapi.com/search.json?as_sdt=5%2C44&cites=4837773145610823844&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:pKgOWAs5I0MJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,44",https://serpapi.com/search.json?as_sdt=0%2C44&engine=google_scholar&hl=en&num=20&q=related%3ApKgOWAs5I0MJ%3Ascholar.google.com%2F&start=20,5.0,"https://scholar.google.com/scholar?cluster=4837773145610823844&hl=en&num=20&as_sdt=0,44",4837773145610823844,https://serpapi.com/search.json?as_sdt=0%2C44&cluster=4837773145610823844&engine=google_scholar&hl=en&num=20,,,,,,,,
1,Learning optimal stochastic sensor scheduling for remote estimation with channel capacity constraint,BGd6QQYBMSgJ,https://ieeexplore.ieee.org/abstract/document/9784925/,"… policy gradient, a recent deep reinforcement learning algorithm, is utilized to obtain an … DDPG adopts the actor-critic architecture, that is, an actor network π(s|θπ), which is used to …","L Yang, Y Xu, Z Huang, H Rao… - IEEE Transactions on …, 2022 - ieeexplore.ieee.org",L Yang,https://scholar.google.com/citations?user=G-u29p4AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=G-u29p4AAAAJ&engine=google_scholar_author&hl=en,G-u29p4AAAAJ,,,,,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=BGd6QQYBMSgJ,6.0,"https://scholar.google.com/scholar?cites=2896097161755911940&as_sdt=5,33&sciodt=0,33&hl=en&num=20",2896097161755911940,https://serpapi.com/search.json?as_sdt=5%2C33&cites=2896097161755911940&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:BGd6QQYBMSgJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,33",https://serpapi.com/search.json?as_sdt=0%2C33&engine=google_scholar&hl=en&num=20&q=related%3ABGd6QQYBMSgJ%3Ascholar.google.com%2F&start=40,2.0,"https://scholar.google.com/scholar?cluster=2896097161755911940&hl=en&num=20&as_sdt=0,33",2896097161755911940,https://serpapi.com/search.json?as_sdt=0%2C33&cluster=2896097161755911940&engine=google_scholar&hl=en&num=20,,,,,,,,
2,A Federated Reinforcement Learning Approach for Autonomous Vehicle Platooning,wY2pKV0kz3QJ,https://atrium.lib.uoguelph.ca/handle/10214/27063,"… The limitations of DQN in continuous environments led to the development of DDPG: a combination of DQN, actor-critic, and deterministic policy gradients, providing a DRL algorithm …",C Boin - 2022 - atrium.lib.uoguelph.ca,,,,,,,,,,,,,uoguelph.ca,PDF,https://atrium.lib.uoguelph.ca/bitstream/handle/10214/27063/Boin_Christian_202207_MASc.pdf?sequence=5,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=wY2pKV0kz3QJ,,,,,"https://scholar.google.com/scholar?q=related:wY2pKV0kz3QJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,33",https://serpapi.com/search.json?as_sdt=0%2C33&engine=google_scholar&hl=en&num=20&q=related%3AwY2pKV0kz3QJ%3Ascholar.google.com%2F&start=40,2.0,"https://scholar.google.com/scholar?cluster=8416986211128282561&hl=en&num=20&as_sdt=0,33",8416986211128282561,https://serpapi.com/search.json?as_sdt=0%2C33&cluster=8416986211128282561&engine=google_scholar&hl=en&num=20,,"https://scholar.googleusercontent.com/scholar?q=cache:wY2pKV0kz3QJ:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,33",,,,,,
3,On Meeting a Maximum Delay Constraint Using Reinforcement Learning in Wireless Networks,UigMuUmjciQJ,https://search.proquest.com/openview/85c3d6a98ca79e585d50314473966e1c/1?pq-origsite=gscholar&cbl=18750&diss=y,"… with the full knowledge of the statistics and in this regard, we benefit from reinforcement learning (RL). Second, as the first work in this area, we use inverse RL (IRL) in case no prior …",H Shafieirad - 2023 - search.proquest.com,H Shafieirad,https://scholar.google.com/citations?user=8Q85_zQAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=8Q85_zQAAAAJ&engine=google_scholar_author&hl=en,8Q85_zQAAAAJ,,,,,,,,,utoronto.ca,PDF,https://tspace.library.utoronto.ca/bitstream/1807/130454/3/Shafieirad_Hossein_202311_PhD_thesis.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=UigMuUmjciQJ,1.0,"https://scholar.google.com/scholar?cites=2626341069742942290&as_sdt=5,33&sciodt=0,33&hl=en&num=20",2626341069742942290,https://serpapi.com/search.json?as_sdt=5%2C33&cites=2626341069742942290&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:UigMuUmjciQJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,33",https://serpapi.com/search.json?as_sdt=0%2C33&engine=google_scholar&hl=en&num=20&q=related%3AUigMuUmjciQJ%3Ascholar.google.com%2F&start=40,2.0,"https://scholar.google.com/scholar?cluster=2626341069742942290&hl=en&num=20&as_sdt=0,33",2626341069742942290,https://serpapi.com/search.json?as_sdt=0%2C33&cluster=2626341069742942290&engine=google_scholar&hl=en&num=20,,,,,,,,
4,Joint routing and computation offloading based deep reinforcement learning for Flying Ad hoc Networks,HxAlDJ4U6McJ,https://www.sciencedirect.com/science/article/pii/S1389128624003463,"… The network model references the actor–critic structure as shown in the right-side light blue section of Fig. 3. First, the state values are fed into the actor-network, which consists of the …","N Lin, J Huang, A Hawbani, L Zhao, H Tang, Y Guan… - Computer Networks, 2024 - Elsevier",A Hawbani,https://scholar.google.com/citations?user=HZRx6AkAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=HZRx6AkAAAAJ&engine=google_scholar_author&hl=en,HZRx6AkAAAAJ,L Zhao,https://scholar.google.com/citations?user=7KlP2dIAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=7KlP2dIAAAAJ&engine=google_scholar_author&hl=en,7KlP2dIAAAAJ,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=HxAlDJ4U6McJ,,,,,"https://scholar.google.com/scholar?q=related:HxAlDJ4U6McJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,33",https://serpapi.com/search.json?as_sdt=0%2C33&engine=google_scholar&hl=en&num=20&q=related%3AHxAlDJ4U6McJ%3Ascholar.google.com%2F&start=40,,,,,,,,,,,,
5,Assessing Policy Optimization agents using Algorithmic IQ test,5owe7CLS1HgJ,https://www.itspy.cz/wp-content/uploads/2023/10/it_spy_2023_diplomova_prace_28.pdf,… As AIQ is closely related to the Reinforcement Learning … arguments for the power of AIXI being extremely relative to … “The term “Actor-Critic” is best thought of as a framework or a …,P Zeman - 2023 - itspy.cz,,,,,,,,,,,,,itspy.cz,PDF,https://www.itspy.cz/wp-content/uploads/2023/10/it_spy_2023_diplomova_prace_28.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=5owe7CLS1HgJ,1.0,"https://scholar.google.com/scholar?cites=8706815027046157542&as_sdt=5,33&sciodt=0,33&hl=en&num=20",8706815027046157542,https://serpapi.com/search.json?as_sdt=5%2C33&cites=8706815027046157542&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:5owe7CLS1HgJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,33",https://serpapi.com/search.json?as_sdt=0%2C33&engine=google_scholar&hl=en&num=20&q=related%3A5owe7CLS1HgJ%3Ascholar.google.com%2F&start=40,2.0,"https://scholar.google.com/scholar?cluster=8706815027046157542&hl=en&num=20&as_sdt=0,33",8706815027046157542,https://serpapi.com/search.json?as_sdt=0%2C33&cluster=8706815027046157542&engine=google_scholar&hl=en&num=20,Pdf,"https://scholar.googleusercontent.com/scholar?q=cache:5owe7CLS1HgJ:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,33",,,,,,
6,Cooperative behavior acquisition by learning and evolution in a multi-agent environment for mobile robots,BHoKYH-bspAJ,http://www.er.ams.eng.osaka-u.ac.jp/Paper/1999/Uchibe99z.pdf,… the local interaction while reinforcement learning copes with the … learn the desired behaviors using reinforcement learning. We also … We implement the actor-critic architecture [66] as an …,E Uchibe - 1999 - er.ams.eng.osaka-u.ac.jp,E Uchibe,https://scholar.google.com/citations?user=nXqBRo8AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=nXqBRo8AAAAJ&engine=google_scholar_author&hl=en,nXqBRo8AAAAJ,,,,,,,,,osaka-u.ac.jp,PDF,http://www.er.ams.eng.osaka-u.ac.jp/Paper/1999/Uchibe99z.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=BHoKYH-bspAJ,16.0,"https://scholar.google.com/scholar?cites=10426567058690570756&as_sdt=5,33&sciodt=0,33&hl=en&num=20",10426567058690570756,https://serpapi.com/search.json?as_sdt=5%2C33&cites=10426567058690570756&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:BHoKYH-bspAJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,33",https://serpapi.com/search.json?as_sdt=0%2C33&engine=google_scholar&hl=en&num=20&q=related%3ABHoKYH-bspAJ%3Ascholar.google.com%2F&start=40,,,,,Pdf,"http://scholar.googleusercontent.com/scholar?q=cache:BHoKYH-bspAJ:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,33",,,,,,
7,Solving partially observable environments with universal search using dataflow graph-based programming model,psGhfg0Z8ycJ,https://www.tandfonline.com/doi/abs/10.1080/03772063.2021.2004461,"… [Citation27] used an actor-critic based deep RL framework to … Three distinct approaches for solving POMDP environments are … For MC-AIXI, initially the agent is given a specific training …","SK Paul, P Bhaumik - IETE Journal of Research, 2023 - Taylor & Francis",SK Paul,https://scholar.google.com/citations?user=YHRO7eoAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=YHRO7eoAAAAJ&engine=google_scholar_author&hl=en,YHRO7eoAAAAJ,P Bhaumik,https://scholar.google.com/citations?user=dzrHTMcAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=dzrHTMcAAAAJ&engine=google_scholar_author&hl=en,dzrHTMcAAAAJ,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=psGhfg0Z8ycJ,4.0,"https://scholar.google.com/scholar?cites=2878672132569678246&as_sdt=5,33&sciodt=0,33&hl=en&num=20",2878672132569678246,https://serpapi.com/search.json?as_sdt=5%2C33&cites=2878672132569678246&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:psGhfg0Z8ycJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,33",https://serpapi.com/search.json?as_sdt=0%2C33&engine=google_scholar&hl=en&num=20&q=related%3ApsGhfg0Z8ycJ%3Ascholar.google.com%2F&start=40,2.0,"https://scholar.google.com/scholar?cluster=2878672132569678246&hl=en&num=20&as_sdt=0,33",2878672132569678246,https://serpapi.com/search.json?as_sdt=0%2C33&cluster=2878672132569678246&engine=google_scholar&hl=en&num=20,,,,,,,,
8,Efficient Reinforcement Learning through Improved Cognitive Capabilities.,ppj30sAkneYJ,https://ir.soken.ac.jp/?action=repository_uri&item_id=6415&file_id=19&file_no=2,"… in reinforcement learning: how to overcome the lack of curiosity in … reinforcement learning to real-world domains, we present our contributions towards endowing reinforcement learning …",N Bougie - 2021 - ir.soken.ac.jp,N Bougie,https://scholar.google.com/citations?user=j_Kf-msAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=j_Kf-msAAAAJ&engine=google_scholar_author&hl=en,j_Kf-msAAAAJ,,,,,,,,,soken.ac.jp,PDF,https://ir.soken.ac.jp/?action=repository_uri&item_id=6415&file_id=19&file_no=2,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=ppj30sAkneYJ,,,,,"https://scholar.google.com/scholar?q=related:ppj30sAkneYJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,33",https://serpapi.com/search.json?as_sdt=0%2C33&engine=google_scholar&hl=en&num=20&q=related%3Appj30sAkneYJ%3Ascholar.google.com%2F&start=40,2.0,"https://scholar.google.com/scholar?cluster=16617478610658760870&hl=en&num=20&as_sdt=0,33",16617478610658760870,https://serpapi.com/search.json?as_sdt=0%2C33&cluster=16617478610658760870&engine=google_scholar&hl=en&num=20,Pdf,"https://scholar.googleusercontent.com/scholar?q=cache:ppj30sAkneYJ:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,33",,,,,,
9,Multi-Contact Simulator and Reinforcement Learning for Screw Tightening Tasks,sJLOET5_VggJ,https://s-space.snu.ac.kr/handle/10371/166403,… This method is called actor-critic method. And one of the most successful algorithm of this is Proximal Policy Optimization(PPO)[15]. Limitation of it is that it can be easily stuck in local …,손동원 - 2020 - s-space.snu.ac.kr,,,,,,,,,,,,,snu.ac.kr,PDF,https://s-space.snu.ac.kr/bitstream/10371/166403/1/000000160762.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=sJLOET5_VggJ,,,,,"https://scholar.google.com/scholar?q=related:sJLOET5_VggJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,33",https://serpapi.com/search.json?as_sdt=0%2C33&engine=google_scholar&hl=en&num=20&q=related%3AsJLOET5_VggJ%3Ascholar.google.com%2F&start=40,,,,,,"https://scholar.googleusercontent.com/scholar?q=cache:sJLOET5_VggJ:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,33",,,,,,
10,Online optimal operation of parallel voltage-source inverters using partial information,guvVOhk6awcJ,https://ieeexplore.ieee.org/abstract/document/7748452/,"… Since in general it is not easy to solve (9) to find the costs Vi, ∀i ∈ N we shall use an actor/critic network architecture for every inverter to provide approximations of its cost and controller…","KG Vamvoudakis, JP Hespanha - IEEE Transactions on …, 2016 - ieeexplore.ieee.org",KG Vamvoudakis,https://scholar.google.com/citations?user=4hWyM_gAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=4hWyM_gAAAAJ&engine=google_scholar_author&hl=en,4hWyM_gAAAAJ,JP Hespanha,https://scholar.google.com/citations?user=AsT__tUAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=AsT__tUAAAAJ&engine=google_scholar_author&hl=en,AsT__tUAAAAJ,,,,,ieee.org,PDF,https://ieeexplore.ieee.org/ielaam/41/7895231/7748452-aam.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=guvVOhk6awcJ,22.0,"https://scholar.google.com/scholar?cites=534584860809227138&as_sdt=5,33&sciodt=0,33&hl=en&num=20",534584860809227138,https://serpapi.com/search.json?as_sdt=5%2C33&cites=534584860809227138&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:guvVOhk6awcJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,33",https://serpapi.com/search.json?as_sdt=0%2C33&engine=google_scholar&hl=en&num=20&q=related%3AguvVOhk6awcJ%3Ascholar.google.com%2F&start=40,2.0,"https://scholar.google.com/scholar?cluster=534584860809227138&hl=en&num=20&as_sdt=0,33",534584860809227138,https://serpapi.com/search.json?as_sdt=0%2C33&cluster=534584860809227138&engine=google_scholar&hl=en&num=20,,,,,,,,
11,Leveraging Transfer Learning with Federated DRL for Autonomous Vehicles Platooning,QSNbrW4GnY8J,https://ieeexplore.ieee.org/abstract/document/10592485/,… integrating Federated Deep Reinforcement Learning (FDRL) … involves training Deep Reinforcement Learning (DRL) model … The DDPG algorithm operates through an actor-critic frame…,"MEA Ameur, H Drias, B Brik… - … and Mobile Computing …, 2024 - ieeexplore.ieee.org",H Drias,https://scholar.google.com/citations?user=KN10OEIAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=KN10OEIAAAAJ&engine=google_scholar_author&hl=en,KN10OEIAAAAJ,B Brik,https://scholar.google.com/citations?user=c3EPy9sAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=c3EPy9sAAAAJ&engine=google_scholar_author&hl=en,c3EPy9sAAAAJ,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=QSNbrW4GnY8J,,,,,"https://scholar.google.com/scholar?q=related:QSNbrW4GnY8J:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,33",https://serpapi.com/search.json?as_sdt=0%2C33&engine=google_scholar&hl=en&num=20&q=related%3AQSNbrW4GnY8J%3Ascholar.google.com%2F&start=40,,,,,,,,,,,,
12,Lifetime policy reuse and the importance of task capacity,UoWCjByV9cEJ,https://content.iospress.com/articles/ai-communications/aic230040,… POMDP in which observations and states are always equal such that O = S and To = T . While traditional RL solves a single MDP or POMDP… as an actor-critic reinforcement learner. PPO …,"DM Bossens, AJ Sobey - AI Communications, 2024 - content.iospress.com",DM Bossens,https://scholar.google.com/citations?user=w2feGIoAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=w2feGIoAAAAJ&engine=google_scholar_author&hl=en,w2feGIoAAAAJ,AJ Sobey,https://scholar.google.com/citations?user=inoZq8AAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=inoZq8AAAAAJ&engine=google_scholar_author&hl=en,inoZq8AAAAAJ,,,,,arxiv.org,PDF,https://arxiv.org/pdf/2106.01741,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=UoWCjByV9cEJ,4.0,"https://scholar.google.com/scholar?cites=13976240968463189330&as_sdt=5,33&sciodt=0,33&hl=en&num=20",13976240968463189330,https://serpapi.com/search.json?as_sdt=5%2C33&cites=13976240968463189330&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:UoWCjByV9cEJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,33",https://serpapi.com/search.json?as_sdt=0%2C33&engine=google_scholar&hl=en&num=20&q=related%3AUoWCjByV9cEJ%3Ascholar.google.com%2F&start=40,6.0,"https://scholar.google.com/scholar?cluster=13976240968463189330&hl=en&num=20&as_sdt=0,33",13976240968463189330,https://serpapi.com/search.json?as_sdt=0%2C33&cluster=13976240968463189330&engine=google_scholar&hl=en&num=20,,,,,,,,
13,Adaptive event-triggered transmission scheduling in rate-limited multiloop remote control,AdsjxvNUl-MJ,https://ieeexplore.ieee.org/abstract/document/9714179/,"… as a constrained multiagent POMDP problem, we develop a … formulated as a constrained multiagent POMDP problem. 2) The … Mordatch, “Multiagent actor-critic for mixed cooperative-…","L Chen, B Hu, ZH Guan - IEEE Transactions on Industrial …, 2022 - ieeexplore.ieee.org",L Chen,https://scholar.google.com/citations?user=-0n0cu8AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=-0n0cu8AAAAJ&engine=google_scholar_author&hl=en,-0n0cu8AAAAJ,,,,,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=AdsjxvNUl-MJ,4.0,"https://scholar.google.com/scholar?cites=16399669974070975233&as_sdt=5,33&sciodt=0,33&hl=en&num=20",16399669974070975233,https://serpapi.com/search.json?as_sdt=5%2C33&cites=16399669974070975233&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:AdsjxvNUl-MJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,33",https://serpapi.com/search.json?as_sdt=0%2C33&engine=google_scholar&hl=en&num=20&q=related%3AAdsjxvNUl-MJ%3Ascholar.google.com%2F&start=40,,,,,,,,,,,,
14,Three Dogmas of Reinforcement Learning,Mp03SlYgR3IJ,https://arxiv.org/abs/2407.10583,… Modern reinforcement learning has been conditioned by at … of as the science of reinforcement learning. While each of the … In order to realize the potential of reinforcement learning as a …,"D Abel, MK Ho, A Harutyunyan - arXiv preprint arXiv:2407.10583, 2024 - arxiv.org",D Abel,https://scholar.google.com/citations?user=lvBJlmwAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=lvBJlmwAAAAJ&engine=google_scholar_author&hl=en,lvBJlmwAAAAJ,MK Ho,https://scholar.google.com/citations?user=yK7yTiwAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=yK7yTiwAAAAJ&engine=google_scholar_author&hl=en,yK7yTiwAAAAJ,A Harutyunyan,https://scholar.google.com/citations?user=Pon8OksAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=Pon8OksAAAAJ&engine=google_scholar_author&hl=en,Pon8OksAAAAJ,arxiv.org,PDF,https://arxiv.org/pdf/2407.10583,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=Mp03SlYgR3IJ,1.0,"https://scholar.google.com/scholar?cites=8234585998654676274&as_sdt=5,33&sciodt=0,33&hl=en&num=20",8234585998654676274,https://serpapi.com/search.json?as_sdt=5%2C33&cites=8234585998654676274&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:Mp03SlYgR3IJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,33",https://serpapi.com/search.json?as_sdt=0%2C33&engine=google_scholar&hl=en&num=20&q=related%3AMp03SlYgR3IJ%3Ascholar.google.com%2F&start=40,4.0,"https://scholar.google.com/scholar?cluster=8234585998654676274&hl=en&num=20&as_sdt=0,33",8234585998654676274,https://serpapi.com/search.json?as_sdt=0%2C33&cluster=8234585998654676274&engine=google_scholar&hl=en&num=20,,"https://scholar.googleusercontent.com/scholar?q=cache:Mp03SlYgR3IJ:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,33",,,,,,
15,Compress and control,VLYbIHXqcl8J,https://ojs.aaai.org/index.php/AAAI/article/view/9600,"… Finally, we remark that information-theoretic perspectives on reinforcement learning have … and reinforcement learning, leading to the AIXI optimality notion for reinforcement learning …","J Veness, M Bellemare, M Hutter, A Chua… - Proceedings of the …, 2015 - ojs.aaai.org",J Veness,https://scholar.google.com/citations?user=_iYrAxEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=_iYrAxEAAAAJ&engine=google_scholar_author&hl=en,_iYrAxEAAAAJ,M Bellemare,https://scholar.google.com/citations?user=uyYPun0AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=uyYPun0AAAAJ&engine=google_scholar_author&hl=en,uyYPun0AAAAJ,M Hutter,https://scholar.google.com/citations?user=7hmCntEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=7hmCntEAAAAJ&engine=google_scholar_author&hl=en,7hmCntEAAAAJ,aaai.org,PDF,https://ojs.aaai.org/index.php/AAAI/article/view/9600/9459,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=VLYbIHXqcl8J,28.0,"https://scholar.google.com/scholar?cites=6877817369718928980&as_sdt=5,33&sciodt=0,33&hl=en&num=20",6877817369718928980,https://serpapi.com/search.json?as_sdt=5%2C33&cites=6877817369718928980&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:VLYbIHXqcl8J:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,33",https://serpapi.com/search.json?as_sdt=0%2C33&engine=google_scholar&hl=en&num=20&q=related%3AVLYbIHXqcl8J%3Ascholar.google.com%2F&start=40,12.0,"https://scholar.google.com/scholar?cluster=6877817369718928980&hl=en&num=20&as_sdt=0,33",6877817369718928980,https://serpapi.com/search.json?as_sdt=0%2C33&cluster=6877817369718928980&engine=google_scholar&hl=en&num=20,,"https://scholar.googleusercontent.com/scholar?q=cache:VLYbIHXqcl8J:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,33",,,,,,
16,End-to-end network slicing design policy in 5G networks,F5HjLVgF7MoJ,https://kclpure.kcl.ac.uk/portal/files/196853253/2023_Wang_Ranyin_1851523_ethesis.pdf,"… And the network model is trained by leveraging the Advantage Actor-Critic algorithm. It is … with Reinforcement Learning elements. And a DRL-based algorithm, the Advantage Actor-Critic …",R Wang - 2023 - kclpure.kcl.ac.uk,R Wang,https://scholar.google.com/citations?user=4FAm6I4AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=4FAm6I4AAAAJ&engine=google_scholar_author&hl=en,4FAm6I4AAAAJ,,,,,,,,,kcl.ac.uk,PDF,https://kclpure.kcl.ac.uk/portal/files/196853253/2023_Wang_Ranyin_1851523_ethesis.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=F5HjLVgF7MoJ,,,,,"https://scholar.google.com/scholar?q=related:F5HjLVgF7MoJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,33",https://serpapi.com/search.json?as_sdt=0%2C33&engine=google_scholar&hl=en&num=20&q=related%3AF5HjLVgF7MoJ%3Ascholar.google.com%2F&start=40,4.0,"https://scholar.google.com/scholar?cluster=14622067966450307351&hl=en&num=20&as_sdt=0,33",14622067966450307351,https://serpapi.com/search.json?as_sdt=0%2C33&cluster=14622067966450307351&engine=google_scholar&hl=en&num=20,Pdf,"https://scholar.googleusercontent.com/scholar?q=cache:F5HjLVgF7MoJ:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,33",,,,,,
17,AGI via Combining Logic with Deep Learning,Y3wypT291uIJ,https://link.springer.com/chapter/10.1007/978-3-030-93758-4_34,… be incorporated under a reinforcement-learning framework to … AIXI is an abstract AGI model introduced by Marcus Hutter in … algorithm and plan to use Actor-Critic (which also allows …,"KY Yan - … Intelligence: 14th International Conference, AGI 2021 …, 2022 - Springer",,,,,,,,,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=Y3wypT291uIJ,2.0,"https://scholar.google.com/scholar?cites=16345459970049997923&as_sdt=5,33&sciodt=0,33&hl=en&num=20",16345459970049997923,https://serpapi.com/search.json?as_sdt=5%2C33&cites=16345459970049997923&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:Y3wypT291uIJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,33",https://serpapi.com/search.json?as_sdt=0%2C33&engine=google_scholar&hl=en&num=20&q=related%3AY3wypT291uIJ%3Ascholar.google.com%2F&start=40,2.0,"https://scholar.google.com/scholar?cluster=16345459970049997923&hl=en&num=20&as_sdt=0,33",16345459970049997923,https://serpapi.com/search.json?as_sdt=0%2C33&cluster=16345459970049997923&engine=google_scholar&hl=en&num=20,,,,,,,,
18,Containment control of heterogeneous systems with active leaders of bounded unknown control using reinforcement learning,eVpyrNtVWwkJ,https://ieeexplore.ieee.org/abstract/document/8285254/,This paper solves the containment problem of multi-agent systems on undirected graph with multiple active leaders using off-policy reinforcement learning (RL). The leaders are active …,"Y Yang, R Song, Y Yin, DC Wunsch… - 2017 IEEE Symposium …, 2017 - ieeexplore.ieee.org",Y Yang,https://scholar.google.com/citations?user=smwQtUwAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=smwQtUwAAAAJ&engine=google_scholar_author&hl=en,smwQtUwAAAAJ,R Song,https://scholar.google.com/citations?user=wGWQpLIAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=wGWQpLIAAAAJ&engine=google_scholar_author&hl=en,wGWQpLIAAAAJ,DC Wunsch,https://scholar.google.com/citations?user=fQC7bIoAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=fQC7bIoAAAAJ&engine=google_scholar_author&hl=en,fQC7bIoAAAAJ,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=eVpyrNtVWwkJ,7.0,"https://scholar.google.com/scholar?cites=674226971201395321&as_sdt=5,33&sciodt=0,33&hl=en&num=20",674226971201395321,https://serpapi.com/search.json?as_sdt=5%2C33&cites=674226971201395321&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:eVpyrNtVWwkJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,33",https://serpapi.com/search.json?as_sdt=0%2C33&engine=google_scholar&hl=en&num=20&q=related%3AeVpyrNtVWwkJ%3Ascholar.google.com%2F&start=40,2.0,"https://scholar.google.com/scholar?cluster=674226971201395321&hl=en&num=20&as_sdt=0,33",674226971201395321,https://serpapi.com/search.json?as_sdt=0%2C33&cluster=674226971201395321&engine=google_scholar&hl=en&num=20,,,,,,,,
19,Adaptive Optimal Bipartite Consensus Control for Heterogeneous Multi-Agent Systems,GvX3VFYBTbkJ,https://ieeexplore.ieee.org/abstract/document/10517424/,… approximated through the actor-critic neural networks for the … multiagent systems utilize the actor-critic neural networks to find … and off-policy online integral reinforcement learning (IRL) …,"B Liang, Y Wei, W Yu - IEEE Transactions on Control of …, 2024 - ieeexplore.ieee.org",W Yu,https://scholar.google.com/citations?user=I7XxngUAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=I7XxngUAAAAJ&engine=google_scholar_author&hl=en,I7XxngUAAAAJ,,,,,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=GvX3VFYBTbkJ,,,,,"https://scholar.google.com/scholar?q=related:GvX3VFYBTbkJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,33",https://serpapi.com/search.json?as_sdt=0%2C33&engine=google_scholar&hl=en&num=20&q=related%3AGvX3VFYBTbkJ%3Ascholar.google.com%2F&start=40,,,,,,,,,,,,
20,Effects of Conservatism on Offline Learning,37IIgTpl2bsJ,https://openreview.net/forum?id=nWFFfnnz-mF,… Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor… 2019) in the case of actor-critic algorithms. An alternate way to regularize values is using …,"K Suri, F Shkurti - openreview.net",,,,,,,,,,,,,openreview.net,PDF,https://openreview.net/pdf?id=nWFFfnnz-mF,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=37IIgTpl2bsJ,,,,,"https://scholar.google.com/scholar?q=related:37IIgTpl2bsJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,33",https://serpapi.com/search.json?as_sdt=0%2C33&engine=google_scholar&hl=en&num=20&q=related%3A37IIgTpl2bsJ%3Ascholar.google.com%2F&start=40,,,,,,"https://scholar.googleusercontent.com/scholar?q=cache:37IIgTpl2bsJ:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,33",,,,,,
1,Data-driven control: Overview and perspectives,_S_Z7JnimuIJ,https://ieeexplore.ieee.org/abstract/document/9867266/,"… Hence, the authors proposed to approximate u∗ and V ∗ as neural networks (ie, with an actor-critic architecture) and iteratively train them to fit the above equations with trajectory data. …","W Tang, P Daoutidis - 2022 American Control Conference …, 2022 - ieeexplore.ieee.org",W Tang,https://scholar.google.com/citations?user=K67JZlsAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=K67JZlsAAAAJ&engine=google_scholar_author&hl=en,K67JZlsAAAAJ,P Daoutidis,https://scholar.google.com/citations?user=qIm39l8AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=qIm39l8AAAAJ&engine=google_scholar_author&hl=en,qIm39l8AAAAJ,,,,,nsf.gov,PDF,https://par.nsf.gov/servlets/purl/10465399,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=_S_Z7JnimuIJ,38.0,"https://scholar.google.com/scholar?cites=16328612549716684797&as_sdt=10000005&sciodt=0,20&hl=en&num=20",16328612549716684797,https://serpapi.com/search.json?as_sdt=10000005&cites=16328612549716684797&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:_S_Z7JnimuIJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,20",https://serpapi.com/search.json?as_sdt=0%2C20&engine=google_scholar&hl=en&num=20&q=related%3A_S_Z7JnimuIJ%3Ascholar.google.com%2F&start=60,3.0,"https://scholar.google.com/scholar?cluster=16328612549716684797&hl=en&num=20&as_sdt=0,20",16328612549716684797,https://serpapi.com/search.json?as_sdt=0%2C20&cluster=16328612549716684797&engine=google_scholar&hl=en&num=20,,,,,,,,
2,Asynchronous distributed reinforcement learning for lqr control via zeroth-order block coordinate descent,cyJvZfAe9tkJ,https://ieeexplore.ieee.org/abstract/document/10494371/,"Recently introduced distributed zeroth-order optimization (ZOO) algorithms have shown their utility in distributed reinforcement learning (RL). Unfortunately, in the gradient estimation …","G Jing, H Bai, J George, A Chakrabortty… - IEEE Transactions on …, 2024 - ieeexplore.ieee.org",G Jing,https://scholar.google.com/citations?user=Czk87NYAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=Czk87NYAAAAJ&engine=google_scholar_author&hl=en,Czk87NYAAAAJ,H Bai,https://scholar.google.com/citations?user=z7N_-U4AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=z7N_-U4AAAAJ&engine=google_scholar_author&hl=en,z7N_-U4AAAAJ,J George,https://scholar.google.com/citations?user=caxjdt4AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=caxjdt4AAAAJ&engine=google_scholar_author&hl=en,caxjdt4AAAAJ,arxiv.org,PDF,https://arxiv.org/pdf/2107.12416,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=cyJvZfAe9tkJ,7.0,"https://scholar.google.com/scholar?cites=15705774768343949939&as_sdt=10000005&sciodt=0,20&hl=en&num=20",15705774768343949939,https://serpapi.com/search.json?as_sdt=10000005&cites=15705774768343949939&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:cyJvZfAe9tkJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,20",https://serpapi.com/search.json?as_sdt=0%2C20&engine=google_scholar&hl=en&num=20&q=related%3AcyJvZfAe9tkJ%3Ascholar.google.com%2F&start=60,5.0,"https://scholar.google.com/scholar?cluster=15705774768343949939&hl=en&num=20&as_sdt=0,20",15705774768343949939,https://serpapi.com/search.json?as_sdt=0%2C20&cluster=15705774768343949939&engine=google_scholar&hl=en&num=20,,,A Chakrabortty,https://scholar.google.com/citations?user=cyRBMkkAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=cyRBMkkAAAAJ&engine=google_scholar_author&hl=en,cyRBMkkAAAAJ,,
3,Robust optimal control law learning for heterogeneous rotorcraft formation involving unknown parameters,co0YeAXuRoYJ,https://ieeexplore.ieee.org/abstract/document/9213939/,… to implement the RL algorithm with a identifier-actor-critic architecture. A model-based RL algorithm … Reinforcement Learning Algorithm Design The reinforcement learning theory will be …,"H Liu, Q Meng, L Yang, H Tian… - … on Unmanned Aircraft …, 2020 - ieeexplore.ieee.org",,,,,,,,,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=co0YeAXuRoYJ,,,,,"https://scholar.google.com/scholar?q=related:co0YeAXuRoYJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,20",https://serpapi.com/search.json?as_sdt=0%2C20&engine=google_scholar&hl=en&num=20&q=related%3Aco0YeAXuRoYJ%3Ascholar.google.com%2F&start=60,,,,,,,,,,,,
4,Adaptive dynamic programming approach for Stackelberg game-based fault-tolerant control,iEphoDKc9LIJ,https://ieeexplore.ieee.org/abstract/document/10164408/,"… • Compared with the commonly used actor-critic framework, this paper proposes two critic neural networks for solving the Hamilton-Jacobi (HJ) equations, which shows to be effective …","Y Xu, B Jiang, Y Meng - 2023 6th International Symposium on …, 2023 - ieeexplore.ieee.org",B Jiang,https://scholar.google.com/citations?user=FRXg54EAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=FRXg54EAAAAJ&engine=google_scholar_author&hl=en,FRXg54EAAAAJ,,,,,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=iEphoDKc9LIJ,1.0,"https://scholar.google.com/scholar?cites=12895103374321601160&as_sdt=10000005&sciodt=0,20&hl=en&num=20",12895103374321601160,https://serpapi.com/search.json?as_sdt=10000005&cites=12895103374321601160&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:iEphoDKc9LIJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,20",https://serpapi.com/search.json?as_sdt=0%2C20&engine=google_scholar&hl=en&num=20&q=related%3AiEphoDKc9LIJ%3Ascholar.google.com%2F&start=60,,,,,,,,,,,,
5,Explainability of power grid attack strategies learned by Deep Reinforcement Learning Agents,uzPtYeeQYCgJ,https://www.researchgate.net/profile/Torben-Logemann/publication/372782170_Explainability_of_power_grid_attack_strategies_learned_by_Deep_Reinforcement_Learning_Agents/links/64c823b9db38b20d6dacff9a/Explainability-of-power-grid-attack-strategies-learned-by-Deep-Reinforcement-Learning-Agents.pdf,… The DDPG algorithm has the advantage over the Advantage Actor-Critic (A2C) algorithm in that it can be optimized from end to end since the entire system is differentiable and can train …,T Logemann - 2023 - researchgate.net,T Logemann,https://scholar.google.com/citations?user=9TgFlZ0AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=9TgFlZ0AAAAJ&engine=google_scholar_author&hl=en,9TgFlZ0AAAAJ,,,,,,,,,researchgate.net,PDF,https://www.researchgate.net/profile/Torben-Logemann/publication/372782170_Explainability_of_power_grid_attack_strategies_learned_by_Deep_Reinforcement_Learning_Agents/links/64c823b9db38b20d6dacff9a/Explainability-of-power-grid-attack-strategies-learned-by-Deep-Reinforcement-Learning-Agents.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=uzPtYeeQYCgJ,,,,,"https://scholar.google.com/scholar?q=related:uzPtYeeQYCgJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,20",https://serpapi.com/search.json?as_sdt=0%2C20&engine=google_scholar&hl=en&num=20&q=related%3AuzPtYeeQYCgJ%3Ascholar.google.com%2F&start=60,2.0,"https://scholar.google.com/scholar?cluster=2909484682736120763&hl=en&num=20&as_sdt=0,20",2909484682736120763,https://serpapi.com/search.json?as_sdt=0%2C20&cluster=2909484682736120763&engine=google_scholar&hl=en&num=20,Pdf,"https://scholar.googleusercontent.com/scholar?q=cache:uzPtYeeQYCgJ:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,20",,,,,,
6,Learning structured representations for perception and control,Ct7gJ0X8WsYJ,https://dspace.mit.edu/bitstream/handle/1721.1/107557/974640245-MIT.pdf?sequence=1,… One such idealized model is called AIXI; it maximizes the expected cumulative rewards … in the deep reinforcement learning literature including asynchronous actor critic methods [93]. …,TD Kulkarni - 2016 - dspace.mit.edu,TD Kulkarni,https://scholar.google.com/citations?user=rrPyvsgAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=rrPyvsgAAAAJ&engine=google_scholar_author&hl=en,rrPyvsgAAAAJ,,,,,,,,,mit.edu,PDF,https://dspace.mit.edu/bitstream/handle/1721.1/107557/974640245-MIT.pdf?sequence=1,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=Ct7gJ0X8WsYJ,,,,,"https://scholar.google.com/scholar?q=related:Ct7gJ0X8WsYJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,20",https://serpapi.com/search.json?as_sdt=0%2C20&engine=google_scholar&hl=en&num=20&q=related%3ACt7gJ0X8WsYJ%3Ascholar.google.com%2F&start=60,,,,,Pdf,"https://scholar.googleusercontent.com/scholar?q=cache:Ct7gJ0X8WsYJ:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,20",,,,,,
7,Data-Driven Solutions to Mixed Control: A Hamilton-Inequality-Driven Reinforcement Learning Approach,Ka2Kj_qG_hkJ,https://ieeexplore.ieee.org/abstract/document/9206320/,"… This paper presents a data-based reinforcement learning algorithm for control of nonlinear … [16] KG Vamvoudakis and FL Lewis, “Online actor-critic algorithm to solve the continuous-…","Y Yang, M Mazouchi, H Modares - 2020 IEEE Conference on …, 2020 - ieeexplore.ieee.org",Y Yang,https://scholar.google.com/citations?user=smwQtUwAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=smwQtUwAAAAJ&engine=google_scholar_author&hl=en,smwQtUwAAAAJ,M Mazouchi,https://scholar.google.com/citations?user=vpqGNf8AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=vpqGNf8AAAAJ&engine=google_scholar_author&hl=en,vpqGNf8AAAAJ,H Modares,https://scholar.google.com/citations?user=xhucCdUAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=xhucCdUAAAAJ&engine=google_scholar_author&hl=en,xhucCdUAAAAJ,google.com,PDF,https://drive.google.com/file/d/1uRiP4Ps01KWlAEC6-QTJ3BiBc36X-FYp/view,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=Ka2Kj_qG_hkJ,,,,,"https://scholar.google.com/scholar?q=related:Ka2Kj_qG_hkJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,20",https://serpapi.com/search.json?as_sdt=0%2C20&engine=google_scholar&hl=en&num=20&q=related%3AKa2Kj_qG_hkJ%3Ascholar.google.com%2F&start=60,3.0,"https://scholar.google.com/scholar?cluster=1873082905740881193&hl=en&num=20&as_sdt=0,20",1873082905740881193,https://serpapi.com/search.json?as_sdt=0%2C20&cluster=1873082905740881193&engine=google_scholar&hl=en&num=20,,,,,,,,
8,A spiking neural network of state transition probabilities in model-based reinforcement learning,H2FafzgzsqcJ,https://uwspace.uwaterloo.ca/handle/10012/12574,"… the field of reinforcement learning was based on psychological studies of the instrumental conditioning of humans and other animals. Recently, reinforcement learning algorithms have …",M Shein - 2017 - uwspace.uwaterloo.ca,,,,,,,,,,,,,uwaterloo.ca,PDF,https://uwspace.uwaterloo.ca/bitstream/handle/10012/12574/Shein_Mariah.pdf?sequence=3&isAllowed=y,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=H2FafzgzsqcJ,1.0,"https://scholar.google.com/scholar?cites=12083777067936276767&as_sdt=10000005&sciodt=0,20&hl=en&num=20",12083777067936276767,https://serpapi.com/search.json?as_sdt=10000005&cites=12083777067936276767&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:H2FafzgzsqcJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,20",https://serpapi.com/search.json?as_sdt=0%2C20&engine=google_scholar&hl=en&num=20&q=related%3AH2FafzgzsqcJ%3Ascholar.google.com%2F&start=60,2.0,"https://scholar.google.com/scholar?cluster=12083777067936276767&hl=en&num=20&as_sdt=0,20",12083777067936276767,https://serpapi.com/search.json?as_sdt=0%2C20&cluster=12083777067936276767&engine=google_scholar&hl=en&num=20,,"https://scholar.googleusercontent.com/scholar?q=cache:H2FafzgzsqcJ:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,20",,,,,,
9,"Intelligent Transportation Systems, Hybrid Electric Vehicles, Powertrain Control, Cooperative Adaptive Cruise Control, Model Predictive Control",6ELBhUEioXUJ,https://search.proquest.com/openview/941cd5a1f682214cb6c191b64b8ac5db/1?pq-origsite=gscholar&cbl=18750&diss=y,"… Finally, we integrate the recent development in reinforcement learning to design a novel multi… We proposed to use two reinforcement learning agents in two levels of abstraction. The first …",H Kazemi - 2019 - search.proquest.com,H Kazemi,https://scholar.google.com/citations?user=O2-z9TYAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=O2-z9TYAAAAJ&engine=google_scholar_author&hl=en,O2-z9TYAAAAJ,,,,,,,,,wvu.edu,PDF,https://researchrepository.wvu.edu/cgi/viewcontent.cgi?article=4905&context=etd,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=6ELBhUEioXUJ,1.0,"https://scholar.google.com/scholar?cites=8476093638500238056&as_sdt=10000005&sciodt=0,20&hl=en&num=20",8476093638500238056,https://serpapi.com/search.json?as_sdt=10000005&cites=8476093638500238056&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:6ELBhUEioXUJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,20",https://serpapi.com/search.json?as_sdt=0%2C20&engine=google_scholar&hl=en&num=20&q=related%3A6ELBhUEioXUJ%3Ascholar.google.com%2F&start=60,4.0,"https://scholar.google.com/scholar?cluster=8476093638500238056&hl=en&num=20&as_sdt=0,20",8476093638500238056,https://serpapi.com/search.json?as_sdt=0%2C20&cluster=8476093638500238056&engine=google_scholar&hl=en&num=20,Book,,,,,,,
10,Output resilient containment control of heterogeneous systems with active leaders using reinforcement learning under attack inputs,02NrZZpipREJ,https://ieeexplore.ieee.org/abstract/document/8869870/,… containment control problem of heterogeneous multiple-agent systems (MASs) with unknown active leaders under attack inputs by using data-based offpolicy reinforcement learning (RL…,"Q Li, L Xia, R Song - IEEE Access, 2019 - ieeexplore.ieee.org",L Xia,https://scholar.google.com/citations?user=Euxh1SoAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=Euxh1SoAAAAJ&engine=google_scholar_author&hl=en,Euxh1SoAAAAJ,R Song,https://scholar.google.com/citations?user=wGWQpLIAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=wGWQpLIAAAAJ&engine=google_scholar_author&hl=en,wGWQpLIAAAAJ,,,,,ieee.org,PDF,https://ieeexplore.ieee.org/iel7/6287639/8600701/08869870.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=02NrZZpipREJ,8.0,"https://scholar.google.com/scholar?cites=1271530885068055507&as_sdt=10000005&sciodt=0,20&hl=en&num=20",1271530885068055507,https://serpapi.com/search.json?as_sdt=10000005&cites=1271530885068055507&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:02NrZZpipREJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,20",https://serpapi.com/search.json?as_sdt=0%2C20&engine=google_scholar&hl=en&num=20&q=related%3A02NrZZpipREJ%3Ascholar.google.com%2F&start=60,3.0,"https://scholar.google.com/scholar?cluster=1271530885068055507&hl=en&num=20&as_sdt=0,20",1271530885068055507,https://serpapi.com/search.json?as_sdt=0%2C20&cluster=1271530885068055507&engine=google_scholar&hl=en&num=20,,,,,,,,
11,Information structures for causally explainable decisions,r-_apGm0mIMJ,https://www.mdpi.com/1099-4300/23/5/601,"… The discussion includes relatively recent developments (eg, integration of Thompson sampling into Monte Carlo Tree Search) while noting foundational works in prescriptive decision …","LA Cox Jr - Entropy, 2021 - mdpi.com",LA Cox Jr,https://scholar.google.com/citations?user=i0PiR8UAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=i0PiR8UAAAAJ&engine=google_scholar_author&hl=en,i0PiR8UAAAAJ,,,,,,,,,mdpi.com,PDF,https://www.mdpi.com/1099-4300/23/5/601/pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=r-_apGm0mIMJ,10.0,"https://scholar.google.com/scholar?cites=9482527381258956719&as_sdt=10000005&sciodt=0,20&hl=en&num=20",9482527381258956719,https://serpapi.com/search.json?as_sdt=10000005&cites=9482527381258956719&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:r-_apGm0mIMJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,20",https://serpapi.com/search.json?as_sdt=0%2C20&engine=google_scholar&hl=en&num=20&q=related%3Ar-_apGm0mIMJ%3Ascholar.google.com%2F&start=60,9.0,"https://scholar.google.com/scholar?cluster=9482527381258956719&hl=en&num=20&as_sdt=0,20",9482527381258956719,https://serpapi.com/search.json?as_sdt=0%2C20&cluster=9482527381258956719&engine=google_scholar&hl=en&num=20,,"https://scholar.googleusercontent.com/scholar?q=cache:r-_apGm0mIMJ:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,20",,,,,,
12,Fully data-driven robust output formation tracking control for heterogeneous multiagent system with multiple leaders and actuator faults,M5hIX1CpP3MJ,https://ieeexplore.ieee.org/abstract/document/9994618/,"… Different from those ideas of estimating and fitting specific components, reinforcement learning (… As for continuous-time (CT) systems, the model-based actorcritic RL was studied in [25] …","Y Shi, Y Hua, J Yu, X Dong… - IEEE Transactions on …, 2022 - ieeexplore.ieee.org",Y Shi,https://scholar.google.com/citations?user=DBqSTJ0AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=DBqSTJ0AAAAJ&engine=google_scholar_author&hl=en,DBqSTJ0AAAAJ,J Yu,https://scholar.google.com/citations?user=XaGaDnYAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=XaGaDnYAAAAJ&engine=google_scholar_author&hl=en,XaGaDnYAAAAJ,X Dong,https://scholar.google.com/citations?user=4GeHHnUAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=4GeHHnUAAAAJ&engine=google_scholar_author&hl=en,4GeHHnUAAAAJ,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=M5hIX1CpP3MJ,11.0,"https://scholar.google.com/scholar?cites=8304542400555554867&as_sdt=10000005&sciodt=0,20&hl=en&num=20",8304542400555554867,https://serpapi.com/search.json?as_sdt=10000005&cites=8304542400555554867&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:M5hIX1CpP3MJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,20",https://serpapi.com/search.json?as_sdt=0%2C20&engine=google_scholar&hl=en&num=20&q=related%3AM5hIX1CpP3MJ%3Ascholar.google.com%2F&start=60,3.0,"https://scholar.google.com/scholar?cluster=8304542400555554867&hl=en&num=20&as_sdt=0,20",8304542400555554867,https://serpapi.com/search.json?as_sdt=0%2C20&cluster=8304542400555554867&engine=google_scholar&hl=en&num=20,,,,,,,,
13,Faster Policy Adaptation in Environments with Exogeneity: A State Augmentation Approach.,k2HCgbLdvpgJ,https://ifaamas.org/Proceedings/aamas2018/pdfs/p1035.pdf,… We also compare it with two state-of-the-art approaches for reinforcement learning in dynamic environments based on extensions of Q-learning: Frequency Adjusted Q-learning (FAQL) […,"Z Li, Z Chen, P Poupart, S Das, Y Geng - AAMAS, 2018 - ifaamas.org",P Poupart,https://scholar.google.com/citations?user=KhAJWroAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=KhAJWroAAAAJ&engine=google_scholar_author&hl=en,KhAJWroAAAAJ,S Das,https://scholar.google.com/citations?user=HRSDO6IAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=HRSDO6IAAAAJ&engine=google_scholar_author&hl=en,HRSDO6IAAAAJ,Y Geng,https://scholar.google.com/citations?user=SbA3q80AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=SbA3q80AAAAJ&engine=google_scholar_author&hl=en,SbA3q80AAAAJ,ifaamas.org,PDF,https://ifaamas.org/Proceedings/aamas2018/pdfs/p1035.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=k2HCgbLdvpgJ,1.0,"https://scholar.google.com/scholar?cites=11006478298090987923&as_sdt=10000005&sciodt=0,20&hl=en&num=20",11006478298090987923,https://serpapi.com/search.json?as_sdt=10000005&cites=11006478298090987923&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:k2HCgbLdvpgJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,20",https://serpapi.com/search.json?as_sdt=0%2C20&engine=google_scholar&hl=en&num=20&q=related%3Ak2HCgbLdvpgJ%3Ascholar.google.com%2F&start=60,7.0,"https://scholar.google.com/scholar?cluster=11006478298090987923&hl=en&num=20&as_sdt=0,20",11006478298090987923,https://serpapi.com/search.json?as_sdt=0%2C20&cluster=11006478298090987923&engine=google_scholar&hl=en&num=20,Pdf,"https://scholar.googleusercontent.com/scholar?q=cache:k2HCgbLdvpgJ:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,20",,,,,,
14,Theory Embedded Learning,oTe39RgUUMcJ,https://tspace.library.utoronto.ca/handle/1807/130447,"… We present the results of applying the DDPG algorithm, a type of actor-critic reinforcement learning algorithm, to the same CartPole control task in order to demonstrate the extreme data …",C Chi - 2023 - tspace.library.utoronto.ca,C Chi,https://scholar.google.com/citations?user=-BZtJogAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=-BZtJogAAAAJ&engine=google_scholar_author&hl=en,-BZtJogAAAAJ,,,,,,,,,utoronto.ca,PDF,https://tspace.library.utoronto.ca/bitstream/1807/130447/3/Chi_Cheng__202311_MAS_thesis.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=oTe39RgUUMcJ,1.0,"https://scholar.google.com/scholar?cites=14362001309118707617&as_sdt=10000005&sciodt=0,20&hl=en&num=20",14362001309118707617,https://serpapi.com/search.json?as_sdt=10000005&cites=14362001309118707617&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:oTe39RgUUMcJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,20",https://serpapi.com/search.json?as_sdt=0%2C20&engine=google_scholar&hl=en&num=20&q=related%3AoTe39RgUUMcJ%3Ascholar.google.com%2F&start=60,,,,,,"https://scholar.googleusercontent.com/scholar?q=cache:oTe39RgUUMcJ:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,20",,,,,,
15,Event-triggered local control for nonlinear interconnected systems through particle swarm optimization-based adaptive dynamic programming,NKrhkBwbFLgJ,https://ieeexplore.ieee.org/abstract/document/10214258/,This article investigates local control problems for nonlinear interconnected systems by using adaptive dynamic programming (ADP) with particle swarm optimization (PSO). Through …,"B Zhao, G Shi, D Liu - IEEE Transactions on Systems, Man, and …, 2023 - ieeexplore.ieee.org",B Zhao,https://scholar.google.com/citations?user=977NXzsAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=977NXzsAAAAJ&engine=google_scholar_author&hl=en,977NXzsAAAAJ,,,,,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=NKrhkBwbFLgJ,20.0,"https://scholar.google.com/scholar?cites=13264256612016695860&as_sdt=10000005&sciodt=0,20&hl=en&num=20",13264256612016695860,https://serpapi.com/search.json?as_sdt=10000005&cites=13264256612016695860&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:NKrhkBwbFLgJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,20",https://serpapi.com/search.json?as_sdt=0%2C20&engine=google_scholar&hl=en&num=20&q=related%3ANKrhkBwbFLgJ%3Ascholar.google.com%2F&start=60,,,,,,,,,,,,
16,Causally explainable decision recommendations using causal artificial intelligence,Uo2Eyn-_lmoJ,https://link.springer.com/chapter/10.1007/978-3-031-32013-2_9,"… 9.1 is inadequate to support full optimization, then reinforcement learning or other adaptive control methods, together with optimization heuristics, are used to improve policies over time. …","LA Cox Jr - AI-ML for Decision and Risk Analysis: Challenges and …, 2023 - Springer",LA Cox Jr,https://scholar.google.com/citations?user=i0PiR8UAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=i0PiR8UAAAAJ&engine=google_scholar_author&hl=en,i0PiR8UAAAAJ,,,,,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=Uo2Eyn-_lmoJ,2.0,"https://scholar.google.com/scholar?cites=7680536770106395986&as_sdt=10000005&sciodt=0,20&hl=en&num=20",7680536770106395986,https://serpapi.com/search.json?as_sdt=10000005&cites=7680536770106395986&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:Uo2Eyn-_lmoJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,20",https://serpapi.com/search.json?as_sdt=0%2C20&engine=google_scholar&hl=en&num=20&q=related%3AUo2Eyn-_lmoJ%3Ascholar.google.com%2F&start=60,3.0,"https://scholar.google.com/scholar?cluster=7680536770106395986&hl=en&num=20&as_sdt=0,20",7680536770106395986,https://serpapi.com/search.json?as_sdt=0%2C20&cluster=7680536770106395986&engine=google_scholar&hl=en&num=20,,,,,,,,
17,Model-free optimal control of linear multiagent systems via decomposition and hierarchical approximation,bpcVcs7J7kIJ,https://ieeexplore.ieee.org/abstract/document/9409705/,"… and data-driven techniques [5]–[9] such as reinforcement learning (RL) need to be used. For instance, various RL algorithms such as actor–critic methods [10], Q-learning [11], policy …","G Jing, H Bai, J George… - IEEE Transactions on …, 2021 - ieeexplore.ieee.org",G Jing,https://scholar.google.com/citations?user=Czk87NYAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=Czk87NYAAAAJ&engine=google_scholar_author&hl=en,Czk87NYAAAAJ,H Bai,https://scholar.google.com/citations?user=z7N_-U4AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=z7N_-U4AAAAJ&engine=google_scholar_author&hl=en,z7N_-U4AAAAJ,J George,https://scholar.google.com/citations?user=caxjdt4AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=caxjdt4AAAAJ&engine=google_scholar_author&hl=en,caxjdt4AAAAJ,ieee.org,PDF,https://ieeexplore.ieee.org/ielaam/6509490/9540809/9409705-aam.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=bpcVcs7J7kIJ,24.0,"https://scholar.google.com/scholar?cites=4823014139474843502&as_sdt=10000005&sciodt=0,20&hl=en&num=20",4823014139474843502,https://serpapi.com/search.json?as_sdt=10000005&cites=4823014139474843502&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:bpcVcs7J7kIJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,20",https://serpapi.com/search.json?as_sdt=0%2C20&engine=google_scholar&hl=en&num=20&q=related%3AbpcVcs7J7kIJ%3Ascholar.google.com%2F&start=60,6.0,"https://scholar.google.com/scholar?cluster=4823014139474843502&hl=en&num=20&as_sdt=0,20",4823014139474843502,https://serpapi.com/search.json?as_sdt=0%2C20&cluster=4823014139474843502&engine=google_scholar&hl=en&num=20,,,,,,,,
18,Vehicle Networks: Statistical and Game Theoretic Approaches to Their Evaluation and Design,hhxVaxWBsW8J,https://search.proquest.com/openview/7cf7ff3b556752d5d0e3845fafe64e82/1?pq-origsite=gscholar&cbl=18750&diss=y,"… of reinforcement learning, we decide to make a detailed overview of this area. Reinforcement learning is a … algorithms: Actor-Critic with Experience Replay (ACER) [92], Actor-Critic using …",G Dubosarskii - 2020 - search.proquest.com,G Dubosarskii,https://scholar.google.com/citations?user=7kRxVoIAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=7kRxVoIAAAAJ&engine=google_scholar_author&hl=en,7kRxVoIAAAAJ,,,,,,,,,uwo.ca,PDF,https://ir.lib.uwo.ca/cgi/viewcontent.cgi?article=9466&context=etd,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=hhxVaxWBsW8J,,,,,"https://scholar.google.com/scholar?q=related:hhxVaxWBsW8J:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,20",https://serpapi.com/search.json?as_sdt=0%2C20&engine=google_scholar&hl=en&num=20&q=related%3AhhxVaxWBsW8J%3Ascholar.google.com%2F&start=60,4.0,"https://scholar.google.com/scholar?cluster=8048355938082823302&hl=en&num=20&as_sdt=0,20",8048355938082823302,https://serpapi.com/search.json?as_sdt=0%2C20&cluster=8048355938082823302&engine=google_scholar&hl=en&num=20,,,,,,,,
19,Theory Embedded Learning A Universal Framework for Combining Theory With Learning,qnbH-kDhnZcJ,https://search.proquest.com/openview/3183cdd8fa87162228116cc4c37d2250/1?pq-origsite=gscholar&cbl=18750&diss=y,"… We present the results of applying the DDPG algorithm, a type of actor-critic reinforcement learning algorithm, to the same CartPole control task in order to demonstrate the extreme data …",C Chi - 2023 - search.proquest.com,,,,,,,,,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=qnbH-kDhnZcJ,,,,,"https://scholar.google.com/scholar?q=related:qnbH-kDhnZcJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,20",https://serpapi.com/search.json?as_sdt=0%2C20&engine=google_scholar&hl=en&num=20&q=related%3AqnbH-kDhnZcJ%3Ascholar.google.com%2F&start=60,,,,,,,,,,,,
20,Secure Control for Markov Jump Cyber–Physical Systems Subject to Malicious Attacks: A Resilient Hybrid Learning Scheme,rLNoKQzA1sAJ,https://ieeexplore.ieee.org/abstract/document/10669086/,"… To address this issue, we propose a framework for parallel reinforcement learning. Thereafter, a model-based resilient hybrid learning scheme is first designed to obtain the optimal …","H Shen, Y Wang, J Wu, JH Park… - IEEE Transactions on …, 2024 - ieeexplore.ieee.org",JH Park,https://scholar.google.com/citations?user=t-3_zUQAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=t-3_zUQAAAAJ&engine=google_scholar_author&hl=en,t-3_zUQAAAAJ,,,,,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=rLNoKQzA1sAJ,,,,,"https://scholar.google.com/scholar?q=related:rLNoKQzA1sAJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,20",https://serpapi.com/search.json?as_sdt=0%2C20&engine=google_scholar&hl=en&num=20&q=related%3ArLNoKQzA1sAJ%3Ascholar.google.com%2F&start=60,3.0,"https://scholar.google.com/scholar?cluster=13895504858765112236&hl=en&num=20&as_sdt=0,20",13895504858765112236,https://serpapi.com/search.json?as_sdt=0%2C20&cluster=13895504858765112236&engine=google_scholar&hl=en&num=20,,,,,,,,
1,Resilient Output Formation-Containment Tracking of Heterogeneous Multi-Agent Systems: A Learning-Based Framework using Dynamic Data,pRW7fSjhXQkJ,https://ieeexplore.ieee.org/abstract/document/10480596/,"… years have witnessed a significant progress of reinforcement learning (RL) [28] due to its … [30], value iteration (VI) [31] and actorcritic algorithms [32] for both discrete-time and continuous…","Y Shi, Y Hua, J Yu, X Dong, J Lü… - IEEE Transactions on …, 2024 - ieeexplore.ieee.org",Y Shi,https://scholar.google.com/citations?user=DBqSTJ0AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=DBqSTJ0AAAAJ&engine=google_scholar_author&hl=en,DBqSTJ0AAAAJ,J Yu,https://scholar.google.com/citations?user=XaGaDnYAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=XaGaDnYAAAAJ&engine=google_scholar_author&hl=en,XaGaDnYAAAAJ,J Lü,https://scholar.google.com/citations?user=mCjNN7kAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=mCjNN7kAAAAJ&engine=google_scholar_author&hl=en,mCjNN7kAAAAJ,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=pRW7fSjhXQkJ,,,,,"https://scholar.google.com/scholar?q=related:pRW7fSjhXQkJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,11",https://serpapi.com/search.json?as_sdt=0%2C11&engine=google_scholar&hl=en&num=20&q=related%3ApRW7fSjhXQkJ%3Ascholar.google.com%2F&start=80,,,,,,,,,,,,
2,Counter autonomy defense for aerial autonomous systems,mBa9kh1n7YIJ,https://search.proquest.com/openview/07e161dd7fbb74a850752f324f83d221/1?pq-origsite=gscholar&cbl=18750&diss=y,"… We follow the methods of [24] and will use actor-critic reinforcement learning by policy iteration. Figure 2.4 illustrates this process, which successively iterates toward the solution to …",ME Duntz - 2020 - search.proquest.com,,,,,,,,,,,,,purdue.edu,PDF,https://hammer.purdue.edu/articles/thesis/Counter_Autonomy_Defense_for_Aerial_Autonomous_Systems/12174420/1/files/22390884.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=mBa9kh1n7YIJ,2.0,"https://scholar.google.com/scholar?cites=9434310171124635288&as_sdt=80005&sciodt=0,11&hl=en&num=20",9434310171124635288,https://serpapi.com/search.json?as_sdt=80005&cites=9434310171124635288&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:mBa9kh1n7YIJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,11",https://serpapi.com/search.json?as_sdt=0%2C11&engine=google_scholar&hl=en&num=20&q=related%3AmBa9kh1n7YIJ%3Ascholar.google.com%2F&start=80,4.0,"https://scholar.google.com/scholar?cluster=9434310171124635288&hl=en&num=20&as_sdt=0,11",9434310171124635288,https://serpapi.com/search.json?as_sdt=0%2C11&cluster=9434310171124635288&engine=google_scholar&hl=en&num=20,,,,,,,,
3,Cooperative Fault-Tolerant Formation Tracking Control for Heterogeneous Air-Ground Systems Using a Learning-Based Method,XFqgm28N1UsJ,https://ieeexplore.ieee.org/abstract/document/10330631/,"… [30], reinforcement learning (RL) has been widely applied to control research fields. Based on the pioneering works on typical Qlearning and actor-critic methods for discrete systems, …","Y Shi, Y Hua, J Yu, X Dong, J Lü… - IEEE Transactions on …, 2023 - ieeexplore.ieee.org",Y Shi,https://scholar.google.com/citations?user=DBqSTJ0AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=DBqSTJ0AAAAJ&engine=google_scholar_author&hl=en,DBqSTJ0AAAAJ,J Yu,https://scholar.google.com/citations?user=XaGaDnYAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=XaGaDnYAAAAJ&engine=google_scholar_author&hl=en,XaGaDnYAAAAJ,J Lü,https://scholar.google.com/citations?user=mCjNN7kAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=mCjNN7kAAAAJ&engine=google_scholar_author&hl=en,mCjNN7kAAAAJ,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=XFqgm28N1UsJ,1.0,"https://scholar.google.com/scholar?cites=5464288495887473244&as_sdt=80005&sciodt=0,11&hl=en&num=20",5464288495887473244,https://serpapi.com/search.json?as_sdt=80005&cites=5464288495887473244&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:XFqgm28N1UsJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,11",https://serpapi.com/search.json?as_sdt=0%2C11&engine=google_scholar&hl=en&num=20&q=related%3AXFqgm28N1UsJ%3Ascholar.google.com%2F&start=80,2.0,"https://scholar.google.com/scholar?cluster=5464288495887473244&hl=en&num=20&as_sdt=0,11",5464288495887473244,https://serpapi.com/search.json?as_sdt=0%2C11&cluster=5464288495887473244&engine=google_scholar&hl=en&num=20,,,,,,,,
4,Data-driven cooperative output regulation of multi-agent systems via robust adaptive dynamic programming,2cSv1X9H_B8J,https://ieeexplore.ieee.org/abstract/document/8391745/,"… Yang, “Adaptive actor-critic design-based integral sliding-mode control for partially … adaptive algorithm for optimal control with integral reinforcement learning,” Int. J. Robust Nonlin. …","W Gao, Y Jiang, M Davari - … on Circuits and Systems II: Express …, 2018 - ieeexplore.ieee.org",W Gao,https://scholar.google.com/citations?user=XNYwzswAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=XNYwzswAAAAJ&engine=google_scholar_author&hl=en,XNYwzswAAAAJ,Y Jiang,https://scholar.google.com/citations?user=QYanTRsAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=QYanTRsAAAAJ&engine=google_scholar_author&hl=en,QYanTRsAAAAJ,M Davari,https://scholar.google.com/citations?user=fHOM7SEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=fHOM7SEAAAAJ&engine=google_scholar_author&hl=en,fHOM7SEAAAAJ,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=2cSv1X9H_B8J,73.0,"https://scholar.google.com/scholar?cites=2304795723678336217&as_sdt=80005&sciodt=0,11&hl=en&num=20",2304795723678336217,https://serpapi.com/search.json?as_sdt=80005&cites=2304795723678336217&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:2cSv1X9H_B8J:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,11",https://serpapi.com/search.json?as_sdt=0%2C11&engine=google_scholar&hl=en&num=20&q=related%3A2cSv1X9H_B8J%3Ascholar.google.com%2F&start=80,2.0,"https://scholar.google.com/scholar?cluster=2304795723678336217&hl=en&num=20&as_sdt=0,11",2304795723678336217,https://serpapi.com/search.json?as_sdt=0%2C11&cluster=2304795723678336217&engine=google_scholar&hl=en&num=20,,,,,,,,
5,HDP algorithms for trajectory tracking and formation control of multi-agent systems,HEvAOt6lC7QJ,https://ieeexplore.ieee.org/abstract/document/9726234/,"… These structures are combined with reinforcement learning (RL), which has been used as a … control policy P will be calculated for a controller gain K update (actor/critic architecture). …","EFM Ferreira, JVDF Neto, RR Selmic - IEEE Access, 2022 - ieeexplore.ieee.org",JVDF Neto,https://scholar.google.com/citations?user=gkCdm_8AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=gkCdm_8AAAAJ&engine=google_scholar_author&hl=en,gkCdm_8AAAAJ,RR Selmic,https://scholar.google.com/citations?user=PxMC2pcAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=PxMC2pcAAAAJ&engine=google_scholar_author&hl=en,PxMC2pcAAAAJ,,,,,ieee.org,PDF,https://ieeexplore.ieee.org/iel7/6287639/6514899/09726234.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=HEvAOt6lC7QJ,3.0,"https://scholar.google.com/scholar?cites=12973645525457849116&as_sdt=80005&sciodt=0,11&hl=en&num=20",12973645525457849116,https://serpapi.com/search.json?as_sdt=80005&cites=12973645525457849116&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:HEvAOt6lC7QJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,11",https://serpapi.com/search.json?as_sdt=0%2C11&engine=google_scholar&hl=en&num=20&q=related%3AHEvAOt6lC7QJ%3Ascholar.google.com%2F&start=80,3.0,"https://scholar.google.com/scholar?cluster=12973645525457849116&hl=en&num=20&as_sdt=0,11",12973645525457849116,https://serpapi.com/search.json?as_sdt=0%2C11&cluster=12973645525457849116&engine=google_scholar&hl=en&num=20,,,,,,,,
6,The alignment problem from a deep learning perspective,7Xu0MsiIRAcJ,https://arxiv.org/abs/2209.00626,… We speculate that exploration problems for actor-critic RL algorithms could be further exacerbated by collusion between situationally-aware actors and critics—eg if a single network …,"R Ngo, L Chan, S Mindermann - arXiv preprint arXiv:2209.00626, 2022 - arxiv.org",R Ngo,https://scholar.google.com/citations?user=7CY93A4AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=7CY93A4AAAAJ&engine=google_scholar_author&hl=en,7CY93A4AAAAJ,L Chan,https://scholar.google.com/citations?user=6Id1G8cAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=6Id1G8cAAAAJ&engine=google_scholar_author&hl=en,6Id1G8cAAAAJ,S Mindermann,https://scholar.google.com/citations?user=slBPlrQAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=slBPlrQAAAAJ&engine=google_scholar_author&hl=en,slBPlrQAAAAJ,arxiv.org,PDF,https://arxiv.org/pdf/2209.00626.pdf?utm_source=adwords?utm_source=\\\\%22https://jobalert.ie\\\\%22,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=7Xu0MsiIRAcJ,157.0,"https://scholar.google.com/scholar?cites=523693850107345901&as_sdt=80005&sciodt=0,11&hl=en&num=20",523693850107345901,https://serpapi.com/search.json?as_sdt=80005&cites=523693850107345901&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:7Xu0MsiIRAcJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,11",https://serpapi.com/search.json?as_sdt=0%2C11&engine=google_scholar&hl=en&num=20&q=related%3A7Xu0MsiIRAcJ%3Ascholar.google.com%2F&start=80,4.0,"https://scholar.google.com/scholar?cluster=523693850107345901&hl=en&num=20&as_sdt=0,11",523693850107345901,https://serpapi.com/search.json?as_sdt=0%2C11&cluster=523693850107345901&engine=google_scholar&hl=en&num=20,,"https://scholar.googleusercontent.com/scholar?q=cache:7Xu0MsiIRAcJ:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,11",,,,,,
7,Securing Industrial Production from Sophisticated Cyberattacks.,fS5wVlljtOIJ,https://pdfs.semanticscholar.org/0647/de0f3f35614a615fb15e672f5175c4ab34d0.pdf,"… However, the definition of damage, di, given in (1) suggests a natural structure to formulate a delayed reward function for a reinforcement learning agent seeking to construct a set of …","A Sundstrom, DW Limoge, V Pinskiy, M Putman - ICISSP, 2020 - pdfs.semanticscholar.org",A Sundstrom,https://scholar.google.com/citations?user=w7zKpOsAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=w7zKpOsAAAAJ&engine=google_scholar_author&hl=en,w7zKpOsAAAAJ,DW Limoge,https://scholar.google.com/citations?user=4Slqw1kAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=4Slqw1kAAAAJ&engine=google_scholar_author&hl=en,4Slqw1kAAAAJ,,,,,semanticscholar.org,PDF,https://pdfs.semanticscholar.org/0647/de0f3f35614a615fb15e672f5175c4ab34d0.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=fS5wVlljtOIJ,,,,,"https://scholar.google.com/scholar?q=related:fS5wVlljtOIJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,11",https://serpapi.com/search.json?as_sdt=0%2C11&engine=google_scholar&hl=en&num=20&q=related%3AfS5wVlljtOIJ%3Ascholar.google.com%2F&start=80,3.0,"https://scholar.google.com/scholar?cluster=16335790983733063293&hl=en&num=20&as_sdt=0,11",16335790983733063293,https://serpapi.com/search.json?as_sdt=0%2C11&cluster=16335790983733063293&engine=google_scholar&hl=en&num=20,Pdf,"https://scholar.googleusercontent.com/scholar?q=cache:fS5wVlljtOIJ:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,11",,,,,,
8,Communication-efficient allocation of multiple indivisible resources in a federated multi-agent system,ODuxGrueYZYJ,https://ieeexplore.ieee.org/abstract/document/10384067/,"… A federated multi-agent system for actor-critic reinforcement learning is proposed in [26]. For details on the fair allocation of indivisible resources, interested readers can refer to the …","SE Alam, D Shukla - 2023 62nd IEEE Conference on Decision …, 2023 - ieeexplore.ieee.org",SE Alam,https://scholar.google.com/citations?user=8PZoDZkAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=8PZoDZkAAAAJ&engine=google_scholar_author&hl=en,8PZoDZkAAAAJ,D Shukla,https://scholar.google.com/citations?user=KXHNQNwAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=KXHNQNwAAAAJ&engine=google_scholar_author&hl=en,KXHNQNwAAAAJ,,,,,paperplaza.net,PDF,https://css.paperplaza.net/images/temp/CDC/files/1784.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=ODuxGrueYZYJ,1.0,"https://scholar.google.com/scholar?cites=10836116704874019640&as_sdt=80005&sciodt=0,11&hl=en&num=20",10836116704874019640,https://serpapi.com/search.json?as_sdt=80005&cites=10836116704874019640&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:ODuxGrueYZYJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,11",https://serpapi.com/search.json?as_sdt=0%2C11&engine=google_scholar&hl=en&num=20&q=related%3AODuxGrueYZYJ%3Ascholar.google.com%2F&start=80,2.0,"https://scholar.google.com/scholar?cluster=10836116704874019640&hl=en&num=20&as_sdt=0,11",10836116704874019640,https://serpapi.com/search.json?as_sdt=0%2C11&cluster=10836116704874019640&engine=google_scholar&hl=en&num=20,,,,,,,,
9,Reducing blackwell and average optimality to discounted mdps via the blackwell discount factor,tmuv9o3BpbIJ,https://proceedings.neurips.cc/paper_files/paper/2023/hash/a4e720ce31ccd8ba747d8863e1580fa8-Abstract-Conference.html,"… This notion has become popular in the field of reinforcement learning, mainly due to its … Deep direct reinforcement learning for financial signal representation and trading. IEEE …","J Grand-Clément, M Petrik - Advances in Neural …, 2024 - proceedings.neurips.cc",J Grand-Clément,https://scholar.google.com/citations?user=K_ZLzdoAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=K_ZLzdoAAAAJ&engine=google_scholar_author&hl=en,K_ZLzdoAAAAJ,M Petrik,https://scholar.google.com/citations?user=yT-xSLoAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=yT-xSLoAAAAJ&engine=google_scholar_author&hl=en,yT-xSLoAAAAJ,,,,,neurips.cc,PDF,https://proceedings.neurips.cc/paper_files/paper/2023/file/a4e720ce31ccd8ba747d8863e1580fa8-Paper-Conference.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=tmuv9o3BpbIJ,11.0,"https://scholar.google.com/scholar?cites=12872907925381671862&as_sdt=80005&sciodt=0,11&hl=en&num=20",12872907925381671862,https://serpapi.com/search.json?as_sdt=80005&cites=12872907925381671862&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:tmuv9o3BpbIJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,11",https://serpapi.com/search.json?as_sdt=0%2C11&engine=google_scholar&hl=en&num=20&q=related%3Atmuv9o3BpbIJ%3Ascholar.google.com%2F&start=80,6.0,"https://scholar.google.com/scholar?cluster=12872907925381671862&hl=en&num=20&as_sdt=0,11",12872907925381671862,https://serpapi.com/search.json?as_sdt=0%2C11&cluster=12872907925381671862&engine=google_scholar&hl=en&num=20,,"https://scholar.googleusercontent.com/scholar?q=cache:tmuv9o3BpbIJ:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,11",,,,,,
10,Distributed differential games for control of multi-agent systems,1tXffryzqmoJ,https://ieeexplore.ieee.org/abstract/document/9599467/,"… solved via model-free online reinforcement learning techniques under assumptions of strongly … with its neighbors, and uses an actor/critic network to approximate both its optimal control …","D Cappello, T Mylvaganam - IEEE Transactions on Control of …, 2021 - ieeexplore.ieee.org",D Cappello,https://scholar.google.com/citations?user=xjmL-7IAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=xjmL-7IAAAAJ&engine=google_scholar_author&hl=en,xjmL-7IAAAAJ,T Mylvaganam,https://scholar.google.com/citations?user=fIEuk78AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=fIEuk78AAAAJ&engine=google_scholar_author&hl=en,fIEuk78AAAAJ,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=1tXffryzqmoJ,32.0,"https://scholar.google.com/scholar?cites=7686153336224994774&as_sdt=80005&sciodt=0,11&hl=en&num=20",7686153336224994774,https://serpapi.com/search.json?as_sdt=80005&cites=7686153336224994774&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:1tXffryzqmoJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,11",https://serpapi.com/search.json?as_sdt=0%2C11&engine=google_scholar&hl=en&num=20&q=related%3A1tXffryzqmoJ%3Ascholar.google.com%2F&start=80,,,,,,,,,,,,
11,Optimizing agent behavior over long time scales by transporting value,retue2oPlaAJ,https://www.nature.com/articles/s41467-019-13073-w,"Humans prolifically engage in mental time travel. We dwell on past actions and experience satisfaction or regret. More than storytelling, these recollections change how we act in the …","CC Hung, T Lillicrap, J Abramson, Y Wu… - Nature …, 2019 - nature.com",CC Hung,https://scholar.google.com/citations?user=zl0fMUYAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=zl0fMUYAAAAJ&engine=google_scholar_author&hl=en,zl0fMUYAAAAJ,T Lillicrap,https://scholar.google.com/citations?user=htPVdRMAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=htPVdRMAAAAJ&engine=google_scholar_author&hl=en,htPVdRMAAAAJ,Y Wu,https://scholar.google.com/citations?user=vYmSd0UAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=vYmSd0UAAAAJ&engine=google_scholar_author&hl=en,vYmSd0UAAAAJ,nature.com,PDF,https://www.nature.com/articles/s41467-019-13073-w.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=retue2oPlaAJ,141.0,"https://scholar.google.com/scholar?cites=11571171767610174381&as_sdt=80005&sciodt=0,11&hl=en&num=20",11571171767610174381,https://serpapi.com/search.json?as_sdt=80005&cites=11571171767610174381&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:retue2oPlaAJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,11",https://serpapi.com/search.json?as_sdt=0%2C11&engine=google_scholar&hl=en&num=20&q=related%3Aretue2oPlaAJ%3Ascholar.google.com%2F&start=80,11.0,"https://scholar.google.com/scholar?cluster=11571171767610174381&hl=en&num=20&as_sdt=0,11",11571171767610174381,https://serpapi.com/search.json?as_sdt=0%2C11&cluster=11571171767610174381&engine=google_scholar&hl=en&num=20,,,,,,,,
12,Data-Driven Adaptive Optimal Tracking and Its Applications to Intelligent Transportation Systems,OmnYI5omwS0J,https://search.proquest.com/openview/0add9ee9e58191c5e805504d203f04c3/1?pq-origsite=gscholar&cbl=18750,"… and optimization, reinforcement learning, adaptive dynamic … The actor-critic structure is usually used in ADP; see Fig. 1.1. … controller designed using actor-critic structure distinguishes …",W Gao - 2017 - search.proquest.com,W Gao,https://scholar.google.com/citations?user=XNYwzswAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=XNYwzswAAAAJ&engine=google_scholar_author&hl=en,XNYwzswAAAAJ,,,,,,,,,google.com,PDF,https://drive.google.com/file/d/1qHE-MIHtj3GZvGLaS-HH61iGNBdwodP3/view,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=OmnYI5omwS0J,,,,,"https://scholar.google.com/scholar?q=related:OmnYI5omwS0J:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,11",https://serpapi.com/search.json?as_sdt=0%2C11&engine=google_scholar&hl=en&num=20&q=related%3AOmnYI5omwS0J%3Ascholar.google.com%2F&start=80,2.0,"https://scholar.google.com/scholar?cluster=3296958845680118074&hl=en&num=20&as_sdt=0,11",3296958845680118074,https://serpapi.com/search.json?as_sdt=0%2C11&cluster=3296958845680118074&engine=google_scholar&hl=en&num=20,,,,,,,,
13,Decentralized stochastic bilevel optimization with improved per-iteration complexity,_bQSKYYUihYJ,https://proceedings.mlr.press/v202/chen23n.html,"Bilevel optimization recently has received tremendous attention due to its great success in solving important machine learning problems like meta learning, reinforcement learning, and …","X Chen, M Huang, S Ma… - … on Machine Learning, 2023 - proceedings.mlr.press",X Chen,https://scholar.google.com/citations?user=5l-RAfEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=5l-RAfEAAAAJ&engine=google_scholar_author&hl=en,5l-RAfEAAAAJ,M Huang,https://scholar.google.com/citations?user=5j_jAr8AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=5j_jAr8AAAAJ&engine=google_scholar_author&hl=en,5j_jAr8AAAAJ,S Ma,https://scholar.google.com/citations?user=kkzUrUgAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=kkzUrUgAAAAJ&engine=google_scholar_author&hl=en,kkzUrUgAAAAJ,mlr.press,PDF,https://proceedings.mlr.press/v202/chen23n/chen23n.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=_bQSKYYUihYJ,23.0,"https://scholar.google.com/scholar?cites=1624133182067750141&as_sdt=80005&sciodt=0,11&hl=en&num=20",1624133182067750141,https://serpapi.com/search.json?as_sdt=80005&cites=1624133182067750141&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:_bQSKYYUihYJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,11",https://serpapi.com/search.json?as_sdt=0%2C11&engine=google_scholar&hl=en&num=20&q=related%3A_bQSKYYUihYJ%3Ascholar.google.com%2F&start=80,10.0,"https://scholar.google.com/scholar?cluster=1624133182067750141&hl=en&num=20&as_sdt=0,11",1624133182067750141,https://serpapi.com/search.json?as_sdt=0%2C11&cluster=1624133182067750141&engine=google_scholar&hl=en&num=20,,"https://scholar.googleusercontent.com/scholar?q=cache:_bQSKYYUihYJ:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,11",,,,,,
14,Fuzzy observer constraint based on adaptive control for uncertain nonlinear MIMO systems with time-varying state constraints,n7tw0Yyun9QJ,https://ieeexplore.ieee.org/abstract/document/8815865/,This article presents an adaptive output feedback approach of nonlinear multi-input–multi-output (MIMO) systems with time-varying state constraints and unmeasured states. An …,"YJ Liu, M Gong, L Liu, S Tong… - IEEE transactions on …, 2019 - ieeexplore.ieee.org",YJ Liu,https://scholar.google.com/citations?user=x_Wk1jAAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=x_Wk1jAAAAAJ&engine=google_scholar_author&hl=en,x_Wk1jAAAAAJ,L Liu,https://scholar.google.com/citations?user=7r__F5EAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=7r__F5EAAAAJ&engine=google_scholar_author&hl=en,7r__F5EAAAAJ,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=n7tw0Yyun9QJ,90.0,"https://scholar.google.com/scholar?cites=15321156377170197407&as_sdt=80005&sciodt=0,11&hl=en&num=20",15321156377170197407,https://serpapi.com/search.json?as_sdt=80005&cites=15321156377170197407&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:n7tw0Yyun9QJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,11",https://serpapi.com/search.json?as_sdt=0%2C11&engine=google_scholar&hl=en&num=20&q=related%3An7tw0Yyun9QJ%3Ascholar.google.com%2F&start=80,4.0,"https://scholar.google.com/scholar?cluster=15321156377170197407&hl=en&num=20&as_sdt=0,11",15321156377170197407,https://serpapi.com/search.json?as_sdt=0%2C11&cluster=15321156377170197407&engine=google_scholar&hl=en&num=20,,,,,,,,
15,Gaussian Distribution.,Mjl0miLxX84J,https://www.academia.edu/download/96910032/978-0-387-30164-8_324.pdf,"… ⊲Reinforcement learning is a class of learning problems concerned with achieving long-term goals in unfamiliar, uncertain, and dynamic environments. Such tasks are conventionally …",X Zhang - 2010 - academia.edu,X Zhang,https://scholar.google.com/citations?user=jrkrn3sAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=jrkrn3sAAAAJ&engine=google_scholar_author&hl=en,jrkrn3sAAAAJ,,,,,,,,,academia.edu,PDF,https://www.academia.edu/download/96910032/978-0-387-30164-8_324.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=Mjl0miLxX84J,18.0,"https://scholar.google.com/scholar?cites=14870869625523157298&as_sdt=80005&sciodt=0,11&hl=en&num=20",14870869625523157298,https://serpapi.com/search.json?as_sdt=80005&cites=14870869625523157298&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:Mjl0miLxX84J:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,11",https://serpapi.com/search.json?as_sdt=0%2C11&engine=google_scholar&hl=en&num=20&q=related%3AMjl0miLxX84J%3Ascholar.google.com%2F&start=80,3.0,"https://scholar.google.com/scholar?cluster=14870869625523157298&hl=en&num=20&as_sdt=0,11",14870869625523157298,https://serpapi.com/search.json?as_sdt=0%2C11&cluster=14870869625523157298&engine=google_scholar&hl=en&num=20,Pdf,"https://scholar.googleusercontent.com/scholar?q=cache:Mjl0miLxX84J:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,11",,,,,,
16,A Data-Based Adaptive Consensus Control Algorithm of Discrete-Time Multi-Agent Systems,HCek8u11PFcJ,https://ieeexplore.ieee.org/abstract/document/10665478/,"… To overcome the shortcomings of existing reinforcement learning (RL) algorithms, we propose a novel data-based adaptive reinforcement learning algorithm by introducing adaptive …","A Xiang, X Zhao - 2024 14th Asian Control Conference (ASCC), 2024 - ieeexplore.ieee.org",A Xiang,https://scholar.google.com/citations?user=WSHo9YgAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=WSHo9YgAAAAJ&engine=google_scholar_author&hl=en,WSHo9YgAAAAJ,X Zhao,https://scholar.google.com/citations?user=nFG8lEYAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=nFG8lEYAAAAJ&engine=google_scholar_author&hl=en,nFG8lEYAAAAJ,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=HCek8u11PFcJ,,,,,"https://scholar.google.com/scholar?q=related:HCek8u11PFcJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,11",https://serpapi.com/search.json?as_sdt=0%2C11&engine=google_scholar&hl=en&num=20&q=related%3AHCek8u11PFcJ%3Ascholar.google.com%2F&start=80,,,,,,,,,,,,
17,Model-Free Containment Control of Fully Heterogeneous Linear Multiagent Systems,Jr7PgI6eLKoJ,https://ieeexplore.ieee.org/abstract/document/10387411/,"… control framework based on reinforcement learning (RL) for … Reinforcement learning (RL) technology has developed … Similarly, Vamvoudakis [32] employed an actor–critic neural …","F Wang, A Cao, Y Yin, Z Liu - IEEE Transactions on Systems …, 2024 - ieeexplore.ieee.org",,,,,,,,,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=Jr7PgI6eLKoJ,1.0,"https://scholar.google.com/scholar?cites=12262350220306660902&as_sdt=80005&sciodt=0,11&hl=en&num=20",12262350220306660902,https://serpapi.com/search.json?as_sdt=80005&cites=12262350220306660902&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:Jr7PgI6eLKoJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,11",https://serpapi.com/search.json?as_sdt=0%2C11&engine=google_scholar&hl=en&num=20&q=related%3AJr7PgI6eLKoJ%3Ascholar.google.com%2F&start=80,,,,,,,,,,,,
18,Balancing Fairness and Efficiency in Energy Resource Allocations,MdLjwNp2FMMJ,https://arxiv.org/abs/2403.15616,"… Lan, “Bringing fairness to actor-critic reinforcement learning for network utility optimization,” in IEEE INFOCOM 2021 - IEEE Conference on Computer Communications, 2021, pp. 1–10. …","J Li, M Motoki, B Zhang - arXiv preprint arXiv:2403.15616, 2024 - arxiv.org",J Li,https://scholar.google.com/citations?user=jHwxA4oAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=jHwxA4oAAAAJ&engine=google_scholar_author&hl=en,jHwxA4oAAAAJ,M Motoki,https://scholar.google.com/citations?user=fOWhQ3AAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=fOWhQ3AAAAAJ&engine=google_scholar_author&hl=en,fOWhQ3AAAAAJ,B Zhang,https://scholar.google.com/citations?user=3svZOGAAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=3svZOGAAAAAJ&engine=google_scholar_author&hl=en,3svZOGAAAAAJ,arxiv.org,PDF,https://arxiv.org/pdf/2403.15616,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=MdLjwNp2FMMJ,1.0,"https://scholar.google.com/scholar?cites=14056991018841264689&as_sdt=80005&sciodt=0,11&hl=en&num=20",14056991018841264689,https://serpapi.com/search.json?as_sdt=80005&cites=14056991018841264689&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:MdLjwNp2FMMJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,11",https://serpapi.com/search.json?as_sdt=0%2C11&engine=google_scholar&hl=en&num=20&q=related%3AMdLjwNp2FMMJ%3Ascholar.google.com%2F&start=80,2.0,"https://scholar.google.com/scholar?cluster=14056991018841264689&hl=en&num=20&as_sdt=0,11",14056991018841264689,https://serpapi.com/search.json?as_sdt=0%2C11&cluster=14056991018841264689&engine=google_scholar&hl=en&num=20,,"https://scholar.googleusercontent.com/scholar?q=cache:MdLjwNp2FMMJ:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,11",,,,,,
19,Multimodal Data Fusion Using Voice and Electromyography Data for Robotic Control,vNdlW36Kx4kJ,https://rave.ohiolink.edu/etdc/view?acc_num=toledo156440368925597,"… In this method, a potential-based reward-forming strategy instrument is joined with a sample proficient reinforcement learning algorithm to offer a principled structure to adapt to these …",T Khan Mohd - 2019 - rave.ohiolink.edu,T Khan Mohd,https://scholar.google.com/citations?user=nh1Jx2kAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=nh1Jx2kAAAAJ&engine=google_scholar_author&hl=en,nh1Jx2kAAAAJ,,,,,,,,,ohiolink.edu,PDF,https://etd.ohiolink.edu/acprod/odb_etd/ws/send_file/send?accession=toledo156440368925597&disposition=inline,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=vNdlW36Kx4kJ,,,,,"https://scholar.google.com/scholar?q=related:vNdlW36Kx4kJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,11",https://serpapi.com/search.json?as_sdt=0%2C11&engine=google_scholar&hl=en&num=20&q=related%3AvNdlW36Kx4kJ%3Ascholar.google.com%2F&start=80,3.0,"https://scholar.google.com/scholar?cluster=9928056178865461180&hl=en&num=20&as_sdt=0,11",9928056178865461180,https://serpapi.com/search.json?as_sdt=0%2C11&cluster=9928056178865461180&engine=google_scholar&hl=en&num=20,,"https://scholar.googleusercontent.com/scholar?q=cache:vNdlW36Kx4kJ:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,11",,,,,,
20,Jamming the Relay-Assisted Multi-User Wireless Communication System: A Zero-Sum Game Approach,ZOmt-b4_3UsJ,https://ieeexplore.ieee.org/abstract/document/10128833/,"… In addition, the reinforcement learning method was introduced to find the Stackelberg equilibrium of a jamming game in non-orthogonal multiple access (NOMA) system [24]. Although …","B Shi, H Shao, J Lin, S Zhao… - IEEE Transactions on …, 2023 - ieeexplore.ieee.org",,,,,,,,,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=ZOmt-b4_3UsJ,1.0,"https://scholar.google.com/scholar?cites=5466595612162910564&as_sdt=80005&sciodt=0,11&hl=en&num=20",5466595612162910564,https://serpapi.com/search.json?as_sdt=80005&cites=5466595612162910564&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:ZOmt-b4_3UsJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,11",https://serpapi.com/search.json?as_sdt=0%2C11&engine=google_scholar&hl=en&num=20&q=related%3AZOmt-b4_3UsJ%3Ascholar.google.com%2F&start=80,2.0,"https://scholar.google.com/scholar?cluster=5466595612162910564&hl=en&num=20&as_sdt=0,11",5466595612162910564,https://serpapi.com/search.json?as_sdt=0%2C11&cluster=5466595612162910564&engine=google_scholar&hl=en&num=20,,,,,,,,
1,The general theory of general intelligence: a pragmatic patternist perspective,yW3Ny7GO0X8J,https://arxiv.org/abs/2103.15100,"… This model is general enough to cover AIXI type systems as well as classic dynamic programming systems, Actor-Critic style reinforcement learning systems, and broader experience-…","B Goertzel - arXiv preprint arXiv:2103.15100, 2021 - arxiv.org",B Goertzel,https://scholar.google.com/citations?user=kTfdhRcAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=kTfdhRcAAAAJ&engine=google_scholar_author&hl=en,kTfdhRcAAAAJ,,,,,,,,,arxiv.org,PDF,https://arxiv.org/pdf/2103.15100,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=yW3Ny7GO0X8J,26.0,"https://scholar.google.com/scholar?cites=9210299607228968393&as_sdt=2005&sciodt=0,5&hl=en&num=20",9210299607228968393,https://serpapi.com/search.json?as_sdt=2005&cites=9210299607228968393&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:yW3Ny7GO0X8J:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3AyW3Ny7GO0X8J%3Ascholar.google.com%2F&start=100,2.0,"https://scholar.google.com/scholar?cluster=9210299607228968393&hl=en&num=20&as_sdt=0,5",9210299607228968393,https://serpapi.com/search.json?as_sdt=0%2C5&cluster=9210299607228968393&engine=google_scholar&hl=en&num=20,,"https://scholar.googleusercontent.com/scholar?q=cache:yW3Ny7GO0X8J:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",,,,,,
2,Fast and Resource-Efficient Control of Wireless Cyber-Physical Systems,PBOCVudHeO8J,https://www.diva-portal.org/smash/record.jsf?pid=diva2:1280065,"… reinforcement learning (RL) for event-triggered controllers has for example been proposed in [91], where an actor-critic … Solving scheduling problems with deep reinforcement learning (…",D Baumann - 2019 - diva-portal.org,D Baumann,https://scholar.google.com/citations?user=bJX8-CEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=bJX8-CEAAAAJ&engine=google_scholar_author&hl=en,bJX8-CEAAAAJ,,,,,,,,,diva-portal.org,PDF,https://www.diva-portal.org/smash/get/diva2:1280065/FULLTEXT02,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=PBOCVudHeO8J,,,,,"https://scholar.google.com/scholar?q=related:PBOCVudHeO8J:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3APBOCVudHeO8J%3Ascholar.google.com%2F&start=100,4.0,"https://scholar.google.com/scholar?cluster=17255621031184438076&hl=en&num=20&as_sdt=0,5",17255621031184438076,https://serpapi.com/search.json?as_sdt=0%2C5&cluster=17255621031184438076&engine=google_scholar&hl=en&num=20,,"https://scholar.googleusercontent.com/scholar?q=cache:PBOCVudHeO8J:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",,,,,,
3,Bioinspired Intelligent Control of Autonomous Robots with State Estimation,TdXYkZkdBpIJ,https://atrium.lib.uoguelph.ca/bitstream/10214/27600/1/Xu_Zhe_202305_PhD.pdf,"… reinforcement learning, Li developed an adaptive neural network tracking control for mobile robots based on reinforcement learning, in which the reinforcement learning … an actor-critic-…",Z Xu - 2023 - atrium.lib.uoguelph.ca,Z Xu,https://scholar.google.com/citations?user=hS8epm8AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=hS8epm8AAAAJ&engine=google_scholar_author&hl=en,hS8epm8AAAAJ,,,,,,,,,uoguelph.ca,PDF,https://atrium.lib.uoguelph.ca/bitstream/10214/27600/1/Xu_Zhe_202305_PhD.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=TdXYkZkdBpIJ,,,,,"https://scholar.google.com/scholar?q=related:TdXYkZkdBpIJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3ATdXYkZkdBpIJ%3Ascholar.google.com%2F&start=100,2.0,"https://scholar.google.com/scholar?cluster=10522130124811851085&hl=en&num=20&as_sdt=0,5",10522130124811851085,https://serpapi.com/search.json?as_sdt=0%2C5&cluster=10522130124811851085&engine=google_scholar&hl=en&num=20,Pdf,"https://scholar.googleusercontent.com/scholar?q=cache:TdXYkZkdBpIJ:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",,,,,,
4,Understanding and Incorporating Mathematical Inductive Biases in Neural Networks,FfF-XZXlr5AJ,https://search.proquest.com/openview/de08a4c306042b324b8032a068bcd355/1?pq-origsite=gscholar&cbl=18750&diss=y,"To overcome the enormous sample complexity of deep learning models, we can leverage basic elements of human and scientific knowledge and imbue these elements into our models. …",M Finzi - 2023 - search.proquest.com,M Finzi,https://scholar.google.com/citations?user=ysMAhlwAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=ysMAhlwAAAAJ&engine=google_scholar_author&hl=en,ysMAhlwAAAAJ,,,,,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=FfF-XZXlr5AJ,,,,,"https://scholar.google.com/scholar?q=related:FfF-XZXlr5AJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3AFfF-XZXlr5AJ%3Ascholar.google.com%2F&start=100,,,,,,,,,,,,
5,To What Extent do Open-loop and Feedback Nash Equilibria Diverge in General-Sum Linear Quadratic Dynamic Games?,XeR3cz3al08J,https://arxiv.org/abs/2409.11257,"Dynamic games offer a versatile framework for modeling the evolving interactions of strategic agents, whose steady-state behavior can be captured by the Nash equilibria of the games. …","CY Chiu, J Li, M Bhatt, N Mehr - arXiv preprint arXiv:2409.11257, 2024 - arxiv.org",CY Chiu,https://scholar.google.com/citations?user=cl9ModoAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=cl9ModoAAAAJ&engine=google_scholar_author&hl=en,cl9ModoAAAAJ,J Li,https://scholar.google.com/citations?user=V_M8l4IAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=V_M8l4IAAAAJ&engine=google_scholar_author&hl=en,V_M8l4IAAAAJ,M Bhatt,https://scholar.google.com/citations?user=O9WQVAEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=O9WQVAEAAAAJ&engine=google_scholar_author&hl=en,O9WQVAEAAAAJ,arxiv.org,PDF,https://arxiv.org/pdf/2409.11257,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=XeR3cz3al08J,,,,,"https://scholar.google.com/scholar?q=related:XeR3cz3al08J:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3AXeR3cz3al08J%3Ascholar.google.com%2F&start=100,2.0,"https://scholar.google.com/scholar?cluster=5735292607944713309&hl=en&num=20&as_sdt=0,5",5735292607944713309,https://serpapi.com/search.json?as_sdt=0%2C5&cluster=5735292607944713309&engine=google_scholar&hl=en&num=20,,"https://scholar.googleusercontent.com/scholar?q=cache:XeR3cz3al08J:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",N Mehr,https://scholar.google.com/citations?user=oAlCitYAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=oAlCitYAAAAJ&engine=google_scholar_author&hl=en,oAlCitYAAAAJ,,
6,Distributed Containment Control Strategy for the Dynamic Stabilization of Integrated Energy System With Multiple Virtual Leaders,8dm5RSZubd8J,https://ieeexplore.ieee.org/abstract/document/10410024/,"… Based on MPC ideas and deep reinforcement learning, the indoor temperature of the heating … Shi, “Generalized actor-critic learning optimal control in smart home energy management,” …","H Han, H Zhang, J Yang, L Yang - IEEE Transactions on …, 2024 - ieeexplore.ieee.org",H Zhang,https://scholar.google.com/citations?user=nbQu9moAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=nbQu9moAAAAJ&engine=google_scholar_author&hl=en,nbQu9moAAAAJ,,,,,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=8dm5RSZubd8J,3.0,"https://scholar.google.com/scholar?cites=16099645353577011697&as_sdt=2005&sciodt=0,5&hl=en&num=20",16099645353577011697,https://serpapi.com/search.json?as_sdt=2005&cites=16099645353577011697&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:8dm5RSZubd8J:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3A8dm5RSZubd8J%3Ascholar.google.com%2F&start=100,,,,,,,,,,,,
7,Compressible Non-Newtonian Fluid Based Road Traffic Flow Equation Solved by Physical-Informed Rational Neural Network,dVSRARs_wJsJ,https://ieeexplore.ieee.org/abstract/document/10409149/,"… to Bayesian optimization method: firstly, the kernel function of the Bayesian linear regression part in the Bayesian optimization … sampling function in Bayesian optimization methods is set …","Z Yang, D Li, W Nai, L Liu, J Sun, X Lv - IEEE Access, 2024 - ieeexplore.ieee.org",L Liu,https://scholar.google.com/citations?user=UCIsl-sAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=UCIsl-sAAAAJ&engine=google_scholar_author&hl=en,UCIsl-sAAAAJ,,,,,,,,,ieee.org,PDF,https://ieeexplore.ieee.org/iel7/6287639/6514899/10409149.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=dVSRARs_wJsJ,3.0,"https://scholar.google.com/scholar?cites=11223039656630244469&as_sdt=2005&sciodt=0,5&hl=en&num=20",11223039656630244469,https://serpapi.com/search.json?as_sdt=2005&cites=11223039656630244469&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:dVSRARs_wJsJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3AdVSRARs_wJsJ%3Ascholar.google.com%2F&start=100,2.0,"https://scholar.google.com/scholar?cluster=11223039656630244469&hl=en&num=20&as_sdt=0,5",11223039656630244469,https://serpapi.com/search.json?as_sdt=0%2C5&cluster=11223039656630244469&engine=google_scholar&hl=en&num=20,,,,,,,,
8,Drone Swarm Robust Cooperative Formation Pursuit through Relative Positioning in a Location Denial Environment.,Nw2HBdLDIRwJ,https://search.ebscohost.com/login.aspx?direct=true&profile=ehost&scope=site&authtype=crawler&jrnl=2504446X&AN=180021032&h=fAAJivGxHMI1Y0CcW9EX3BuRk2zq8eFEunl51TB2YBkgcTVq5S5%2F2Z%2Bj4ARIhiRx65RHdhD5pgm7p2zKCcA8%2FA%3D%3D&crl=c,… xi(t) = Aixi(t) + BiKizi(t) − XiΥiϖi(t) = Aixi (t) + BiKizi (t). … Optimized multi-agent formation control based on an identifier–actor–critic reinforcement learning algorithm. IEEE Trans. Fuzzy Syst…,"H Gao, A Zhang, W Li, H Cai - Drones (2504-446X), 2024 - search.ebscohost.com",W Li,https://scholar.google.com/citations?user=BZhoy6AAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=BZhoy6AAAAAJ&engine=google_scholar_author&hl=en,BZhoy6AAAAAJ,H Cai,https://scholar.google.com/citations?user=g3ugXlkAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=g3ugXlkAAAAJ&engine=google_scholar_author&hl=en,g3ugXlkAAAAJ,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=Nw2HBdLDIRwJ,,,,,"https://scholar.google.com/scholar?q=related:Nw2HBdLDIRwJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3ANw2HBdLDIRwJ%3Ascholar.google.com%2F&start=100,3.0,"https://scholar.google.com/scholar?cluster=2027116614096719159&hl=en&num=20&as_sdt=0,5",2027116614096719159,https://serpapi.com/search.json?as_sdt=0%2C5&cluster=2027116614096719159&engine=google_scholar&hl=en&num=20,,,,,,,,
9,Distributed tracking of leader-follower multiagent systems subject to disturbed leader's information,rFVfB88OMdgJ,https://ieeexplore.ieee.org/abstract/document/9298767/,"This paper considers the distributed tracking problem for leader-follower multiagent systems, where the followers are heterogenous multi-input multi-output systems which are subject to …","X Li, S Xu, H Gao, H Cai - IEEE Access, 2020 - ieeexplore.ieee.org",H Cai,https://scholar.google.com/citations?user=g3ugXlkAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=g3ugXlkAAAAJ&engine=google_scholar_author&hl=en,g3ugXlkAAAAJ,,,,,,,,,ieee.org,PDF,https://ieeexplore.ieee.org/iel7/6287639/8948470/09298767.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=rFVfB88OMdgJ,11.0,"https://scholar.google.com/scholar?cites=15578248868395963820&as_sdt=2005&sciodt=0,5&hl=en&num=20",15578248868395963820,https://serpapi.com/search.json?as_sdt=2005&cites=15578248868395963820&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:rFVfB88OMdgJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3ArFVfB88OMdgJ%3Ascholar.google.com%2F&start=100,2.0,"https://scholar.google.com/scholar?cluster=15578248868395963820&hl=en&num=20&as_sdt=0,5",15578248868395963820,https://serpapi.com/search.json?as_sdt=0%2C5&cluster=15578248868395963820&engine=google_scholar&hl=en&num=20,,,,,,,,
10,A leader-follower formation strategy for networked multi-agent systems based on the PI predictive control method,wBKJoYB56X0J,https://ieeexplore.ieee.org/abstract/document/9550321/,"In this paper, the formation control problem of a class of multi-agent systems with communication constraints is studied. A proportional integral predictive control strategy for the leader-…","L Cao, GP Liu, DW Zhang - 2021 40th Chinese Control …, 2021 - ieeexplore.ieee.org",GP Liu,https://scholar.google.com/citations?user=jWmF7IQAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=jWmF7IQAAAAJ&engine=google_scholar_author&hl=en,jWmF7IQAAAAJ,DW Zhang,https://scholar.google.com/citations?user=94h31jkAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=94h31jkAAAAJ&engine=google_scholar_author&hl=en,94h31jkAAAAJ,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=wBKJoYB56X0J,7.0,"https://scholar.google.com/scholar?cites=9072916517687464640&as_sdt=2005&sciodt=0,5&hl=en&num=20",9072916517687464640,https://serpapi.com/search.json?as_sdt=2005&cites=9072916517687464640&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:wBKJoYB56X0J:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3AwBKJoYB56X0J%3Ascholar.google.com%2F&start=100,2.0,"https://scholar.google.com/scholar?cluster=9072916517687464640&hl=en&num=20&as_sdt=0,5",9072916517687464640,https://serpapi.com/search.json?as_sdt=0%2C5&cluster=9072916517687464640&engine=google_scholar&hl=en&num=20,,,,,,,,
11,Robust adaptive dynamic programming: An overview of recent results,jZTC4Cs5cIoJ,https://www.researchgate.net/profile/Yu-Jiang/publication/265032822_ROBUST_ADAPTIVE_DYNAMIC_PROGRAMMING_AN_OVERVIEW_OF_RECENT_RESULTS/links/54d56f510cf2464758079689/ROBUST-ADAPTIVE-DYNAMIC-PROGRAMMING-AN-OVERVIEW-OF-RECENT-RESULTS.pdf,"This paper gives an overview of our recent progress on robust adaptive dynamic programming (for short, robust-ADP) for continuous-time dynamic systems with unknown system …","Y Jiang, ZP Jiang - … of Mathematical Theory of Networks and …, 2012 - researchgate.net",Y Jiang,https://scholar.google.com/citations?user=QYanTRsAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=QYanTRsAAAAJ&engine=google_scholar_author&hl=en,QYanTRsAAAAJ,ZP Jiang,https://scholar.google.com/citations?user=COrhJXcAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=COrhJXcAAAAJ&engine=google_scholar_author&hl=en,COrhJXcAAAAJ,,,,,researchgate.net,PDF,https://www.researchgate.net/profile/Yu-Jiang/publication/265032822_ROBUST_ADAPTIVE_DYNAMIC_PROGRAMMING_AN_OVERVIEW_OF_RECENT_RESULTS/links/54d56f510cf2464758079689/ROBUST-ADAPTIVE-DYNAMIC-PROGRAMMING-AN-OVERVIEW-OF-RECENT-RESULTS.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=jZTC4Cs5cIoJ,1.0,"https://scholar.google.com/scholar?cites=9975536035242873997&as_sdt=2005&sciodt=0,5&hl=en&num=20",9975536035242873997,https://serpapi.com/search.json?as_sdt=2005&cites=9975536035242873997&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:jZTC4Cs5cIoJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3AjZTC4Cs5cIoJ%3Ascholar.google.com%2F&start=100,2.0,"https://scholar.google.com/scholar?cluster=9975536035242873997&hl=en&num=20&as_sdt=0,5",9975536035242873997,https://serpapi.com/search.json?as_sdt=0%2C5&cluster=9975536035242873997&engine=google_scholar&hl=en&num=20,Pdf,"https://scholar.googleusercontent.com/scholar?q=cache:jZTC4Cs5cIoJ:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",,,,,,
12,Distributed cooperative formation control for multi-agent systems based on robust adaptive strategy,nAozUO5zO8UJ,http://www.icicel.org/ell/contents/2020/7/el-14-07-04.pdf,"In this study, a distributed robust cooperative control based on robust adaptive strategy is investigated for multi-agent systems (MASs) to achieve formation consensus and formation …","W He, B Yan, C Wu - ICIC Exp. Lett., 2020 - icicel.org",,,,,,,,,,,,,icicel.org,PDF,http://www.icicel.org/ell/contents/2020/7/el-14-07-04.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=nAozUO5zO8UJ,3.0,"https://scholar.google.com/scholar?cites=14212080516482665116&as_sdt=2005&sciodt=0,5&hl=en&num=20",14212080516482665116,https://serpapi.com/search.json?as_sdt=2005&cites=14212080516482665116&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:nAozUO5zO8UJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3AnAozUO5zO8UJ%3Ascholar.google.com%2F&start=100,3.0,"https://scholar.google.com/scholar?cluster=14212080516482665116&hl=en&num=20&as_sdt=0,5",14212080516482665116,https://serpapi.com/search.json?as_sdt=0%2C5&cluster=14212080516482665116&engine=google_scholar&hl=en&num=20,Pdf,"http://scholar.googleusercontent.com/scholar?q=cache:nAozUO5zO8UJ:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",,,,,,
13,A blockchain-based shamir's threshold cryptography scheme for data protection in industrial internet of things settings,3eh7iHC_VbIJ,https://ieeexplore.ieee.org/abstract/document/9600469/,"… The scheme created a reliable and secure environment using blockchain and deep reinforcement learning. For data storage, Khan and Byun [34] proposed a blockchainbased image …","K Yu, L Tan, C Yang, KKR Choo… - IEEE Internet of …, 2021 - ieeexplore.ieee.org",,,,,,,,,,,,,mmu.ac.uk,PDF,https://e-space.mmu.ac.uk/631056/1/IEEE%20IoTJ-A%20Blockchain-based%20Sham.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=3eh7iHC_VbIJ,134.0,"https://scholar.google.com/scholar?cites=12850387601818642653&as_sdt=2005&sciodt=0,5&hl=en&num=20",12850387601818642653,https://serpapi.com/search.json?as_sdt=2005&cites=12850387601818642653&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:3eh7iHC_VbIJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3A3eh7iHC_VbIJ%3Ascholar.google.com%2F&start=100,6.0,"https://scholar.google.com/scholar?cluster=12850387601818642653&hl=en&num=20&as_sdt=0,5",12850387601818642653,https://serpapi.com/search.json?as_sdt=0%2C5&cluster=12850387601818642653&engine=google_scholar&hl=en&num=20,,,,,,,,
14,VARIABLE COUPLING CONTAINMENT CONTROL IN HETEROGENEOUS MULTI-AGENT SWITCHING TOPOLOGY BASED ON EVENT-TRIGGERED,34VP5P4_L1cJ,http://www.ijicic.org/ijicic-200306.pdf,This paper addresses the containment control problem of heterogeneous Multi-Agent Systems (MAS) under switched topology and proposes an event-triggered decoupling coefficient …,W Ma - ijicic.org,,,,,,,,,,,,,ijicic.org,PDF,http://www.ijicic.org/ijicic-200306.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=34VP5P4_L1cJ,,,,,"https://scholar.google.com/scholar?q=related:34VP5P4_L1cJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3A34VP5P4_L1cJ%3Ascholar.google.com%2F&start=100,,,,,Pdf,"http://scholar.googleusercontent.com/scholar?q=cache:34VP5P4_L1cJ:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",,,,,,
15,Robust adaptive dynamic programming: recent results and applications,lMKfeF3O88oJ,https://ieeexplore.ieee.org/abstract/document/6639567/,The field of adaptive dynamic programming and its applications to control engineering problems has undergone rapid progress over the past few years. A new theory called “Robust …,"ZP Jiang, Y Jiang - Proceedings of the 32nd Chinese Control …, 2013 - ieeexplore.ieee.org",ZP Jiang,https://scholar.google.com/citations?user=COrhJXcAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=COrhJXcAAAAJ&engine=google_scholar_author&hl=en,COrhJXcAAAAJ,Y Jiang,https://scholar.google.com/citations?user=QYanTRsAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=QYanTRsAAAAJ&engine=google_scholar_author&hl=en,QYanTRsAAAAJ,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=lMKfeF3O88oJ,4.0,"https://scholar.google.com/scholar?cites=14624259315853148820&as_sdt=2005&sciodt=0,5&hl=en&num=20",14624259315853148820,https://serpapi.com/search.json?as_sdt=2005&cites=14624259315853148820&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:lMKfeF3O88oJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3AlMKfeF3O88oJ%3Ascholar.google.com%2F&start=100,3.0,"https://scholar.google.com/scholar?cluster=14624259315853148820&hl=en&num=20&as_sdt=0,5",14624259315853148820,https://serpapi.com/search.json?as_sdt=0%2C5&cluster=14624259315853148820&engine=google_scholar&hl=en&num=20,,,,,,,,
16,Advances in Neural Information Processing Systems 34,DMj2WFr4Yv8J,https://www.proceedings.com/content/063/063069webtoc.pdf,… Offline Reinforcement Learning as One Big Sequence Modeling Problem ..................................… Parallel Bayesian Optimization of Multiple Noisy Objectives with Expected Hypervolume …,"M Ranzato, A Beygelzimer, Y Dauphin, PS Liang… - 2021 - proceedings.com",M Ranzato,https://scholar.google.com/citations?user=NbXF7T8AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=NbXF7T8AAAAJ&engine=google_scholar_author&hl=en,NbXF7T8AAAAJ,Y Dauphin,https://scholar.google.com/citations?user=XSforroAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=XSforroAAAAJ&engine=google_scholar_author&hl=en,XSforroAAAAJ,PS Liang,https://scholar.google.com/citations?user=pouyVyUAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=pouyVyUAAAAJ&engine=google_scholar_author&hl=en,pouyVyUAAAAJ,proceedings.com,PDF,https://www.proceedings.com/content/063/063069webtoc.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=DMj2WFr4Yv8J,69.0,"https://scholar.google.com/scholar?cites=18402544094312581132&as_sdt=2005&sciodt=0,5&hl=en&num=20",18402544094312581132,https://serpapi.com/search.json?as_sdt=2005&cites=18402544094312581132&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:DMj2WFr4Yv8J:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3ADMj2WFr4Yv8J%3Ascholar.google.com%2F&start=100,3.0,"https://scholar.google.com/scholar?cluster=18402544094312581132&hl=en&num=20&as_sdt=0,5",18402544094312581132,https://serpapi.com/search.json?as_sdt=0%2C5&cluster=18402544094312581132&engine=google_scholar&hl=en&num=20,Pdf,"https://scholar.googleusercontent.com/scholar?q=cache:DMj2WFr4Yv8J:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",,,,,,
17,The complexity of Markov decision processes,-dy55KRdj98J,https://pubsonline.informs.org/doi/abs/10.1287/moor.12.3.441,"We investigate the complexity of the classical problem of optimal policy computation in Markov decision processes. All three variants of the problem (finite horizon, infinite horizon …","CH Papadimitriou, JN Tsitsiklis - Mathematics of operations …, 1987 - pubsonline.informs.org",CH Papadimitriou,https://scholar.google.com/citations?user=rXYLXJMAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=rXYLXJMAAAAJ&engine=google_scholar_author&hl=en,rXYLXJMAAAAJ,JN Tsitsiklis,https://scholar.google.com/citations?user=bWTPrLEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=bWTPrLEAAAAJ&engine=google_scholar_author&hl=en,bWTPrLEAAAAJ,,,,,mit.edu,PDF,http://w3.mit.edu/jnt/www/Papers/J016-87-mdp-complexity.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=-dy55KRdj98J,1898.0,"https://scholar.google.com/scholar?cites=16109197354920959225&as_sdt=2005&sciodt=0,5&hl=en&num=20",16109197354920959225,https://serpapi.com/search.json?as_sdt=2005&cites=16109197354920959225&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:-dy55KRdj98J:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3A-dy55KRdj98J%3Ascholar.google.com%2F&start=100,15.0,"https://scholar.google.com/scholar?cluster=16109197354920959225&hl=en&num=20&as_sdt=0,5",16109197354920959225,https://serpapi.com/search.json?as_sdt=0%2C5&cluster=16109197354920959225&engine=google_scholar&hl=en&num=20,,,,,,,,
18,UČENJE USLOVLJAVANJEM UZ POŠTOVANJE SIGURNOSNIH MEHANIZAMA–STUDIJA SLUČAJA RADA UZ SAMO-MODIFIKACIJU,EwyQ6Ia6VqcJ,http://www.ftn.uns.ac.rs/ojs/index.php/zbornik/article/view/1478,… The focus of this paper is testing the reinforcement learning algorithms on the issue of self… U ovim radovima autori formulišu teorijske univerzalne agente na osnovu AIXI [15] agenta: …,"O Francuski - Zbornik radova Fakulteta tehničkih nauka u Novom …, 2021 - ftn.uns.ac.rs",,,,,,,,,,,,,uns.ac.rs,PDF,http://www.ftn.uns.ac.rs/ojs/index.php/zbornik/article/download/1478/1231,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=EwyQ6Ia6VqcJ,,,,,"https://scholar.google.com/scholar?q=related:EwyQ6Ia6VqcJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3AEwyQ6Ia6VqcJ%3Ascholar.google.com%2F&start=100,2.0,"https://scholar.google.com/scholar?cluster=12058030140921220115&hl=en&num=20&as_sdt=0,5",12058030140921220115,https://serpapi.com/search.json?as_sdt=0%2C5&cluster=12058030140921220115&engine=google_scholar&hl=en&num=20,,"http://scholar.googleusercontent.com/scholar?q=cache:EwyQ6Ia6VqcJ:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",,,,,,
19,UČENJE USLOVLJAVANJEM UZ POŠTOVANJE SIGURNOSNIH MEHANIZAMA–STUDIJA SLUČAJA RADA UZ SIGURNOSNI PREKID,hF5IL3kCLD4J,http://www.ftn.uns.ac.rs/ojs/index.php/zbornik/article/view/1475,… The focus of this paper is to test reinforcement learning algorithms and their modifications (… The results show that the tested reinforcement learning algorithms are all safely interruptible …,"M Pavlić - Zbornik radova Fakulteta tehničkih nauka u Novom …, 2021 - ftn.uns.ac.rs",,,,,,,,,,,,,uns.ac.rs,PDF,http://www.ftn.uns.ac.rs/ojs/index.php/zbornik/article/download/1475/1229,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=hF5IL3kCLD4J,,,,,"https://scholar.google.com/scholar?q=related:hF5IL3kCLD4J:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3AhF5IL3kCLD4J%3Ascholar.google.com%2F&start=100,2.0,"https://scholar.google.com/scholar?cluster=4479958448834371204&hl=en&num=20&as_sdt=0,5",4479958448834371204,https://serpapi.com/search.json?as_sdt=0%2C5&cluster=4479958448834371204&engine=google_scholar&hl=en&num=20,,"http://scholar.googleusercontent.com/scholar?q=cache:hF5IL3kCLD4J:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",,,,,,
20,Reinforcement Learning with Abstraction,zoNeDINqqAwJ,https://dai.fmph.uniba.sk/upload/b/b3/Thesis.pdf,"… We can use function approximators, such as neural networks, to build the Actor/Critic mechanism. The neural network can generalize the experiences by making a good approximation. …",RNDM Malý - dai.fmph.uniba.sk,,,,,,,,,,,,,uniba.sk,PDF,https://dai.fmph.uniba.sk/upload/b/b3/Thesis.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=zoNeDINqqAwJ,,,,,"https://scholar.google.com/scholar?q=related:zoNeDINqqAwJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3AzoNeDINqqAwJ%3Ascholar.google.com%2F&start=100,,,,,Pdf,"https://scholar.googleusercontent.com/scholar?q=cache:zoNeDINqqAwJ:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",,,,,,
1,Efficient Reinforcement Learning through Improved Cognitive Capabilities (認知能力の改善による効率的な強化学習),C3mQMACJRv0J,https://ir.soken.ac.jp/record/6415/files/%E7%94%B22242.pdf,"… in reinforcement learning: how to overcome the lack of curiosity in … reinforcement learning to real-world domains, we present our contributions towards endowing reinforcement learning …",ニコラスブーギー - ir.soken.ac.jp,,,,,,,,,,,,,soken.ac.jp,PDF,https://ir.soken.ac.jp/record/6415/files/%E7%94%B22242.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=C3mQMACJRv0J,,,,,"https://scholar.google.com/scholar?q=related:C3mQMACJRv0J:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3AC3mQMACJRv0J%3Ascholar.google.com%2F&start=120,2.0,"https://scholar.google.com/scholar?cluster=18250425173873293579&hl=en&num=20&as_sdt=0,5",18250425173873293579,https://serpapi.com/search.json?as_sdt=0%2C5&cluster=18250425173873293579&engine=google_scholar&hl=en&num=20,Pdf,"https://scholar.googleusercontent.com/scholar?q=cache:C3mQMACJRv0J:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",,,,,,
2,基於深度學習及遷移式學習之機器人操作平板電腦虛擬鍵盤的視覺與動作協調系統,msRIl7H5-U4J,https://tdr.lib.ntu.edu.tw/handle/123456789/686,"… The robot control is related to a trajectory in continues action spaces for reinforcement learning. So we apply DDPG [30] as a model-free, off-policy actor-critic algorithm that can learn …",張邵瑀 - 2019 - tdr.lib.ntu.edu.tw,,,,,,,,,,,,,ntu.edu.tw,PDF,https://tdr.lib.ntu.edu.tw/bitstream/123456789/686/1/ntu-108-1.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=msRIl7H5-U4J,,,,,"https://scholar.google.com/scholar?q=related:msRIl7H5-U4J:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3AmsRIl7H5-U4J%3Ascholar.google.com%2F&start=120,,,,,,"https://scholar.googleusercontent.com/scholar?q=cache:msRIl7H5-U4J:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",,,,,,
3,强化学习与自适应动态规划: 从基础理论到多智能体系统中的应用进展综述,LD5RC6RwVhcJ,https://math.seu.edu.cn/_upload/article/files/b5/84/396d592740569c5c041e2225c64a/4a3939b2-b963-444c-a84c-7ad632ffca69.pdf,"… agent reinforcement learning, SARL) 和多智能体强化学习(multi-agent reinforcement learning… 值函数近似和策略梯度算法,并最终发展成将 二者有机融合的演员-评论家(actor-critic, AC)算法. …","温广辉， 杨涛， 周佳玲， 付俊杰， 徐磊 - 控制与决策, 2023 - math.seu.edu.cn",,,,,,,,,,,,,seu.edu.cn,PDF,https://math.seu.edu.cn/_upload/article/files/b5/84/396d592740569c5c041e2225c64a/4a3939b2-b963-444c-a84c-7ad632ffca69.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=LD5RC6RwVhcJ,,,,,"https://scholar.google.com/scholar?q=related:LD5RC6RwVhcJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3ALD5RC6RwVhcJ%3Ascholar.google.com%2F&start=120,2.0,"https://scholar.google.com/scholar?cluster=1681655360736280108&hl=en&num=20&as_sdt=0,5",1681655360736280108,https://serpapi.com/search.json?as_sdt=0%2C5&cluster=1681655360736280108&engine=google_scholar&hl=en&num=20,Pdf,"https://scholar.googleusercontent.com/scholar?q=cache:LD5RC6RwVhcJ:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",,,,,,
4,Metode d'explicabilitat per Aprenentatge Per Reforç Multiagent en presència de comunicació,RED7OWcbPaMJ,https://upcommons.upc.edu/handle/2117/415541,… Reinforcement learning techniques create unpredictable … an environment for reinforcement learning that forces agents to … Advantage actor-critic (A2C) és un algorisme fonamental de …,A García Belmonte - 2024 - upcommons.upc.edu,,,,,,,,,,,,,upc.edu,PDF,https://upcommons.upc.edu/bitstream/handle/2117/415541/188924.pdf?sequence=2,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=RED7OWcbPaMJ,,,,,"https://scholar.google.com/scholar?q=related:RED7OWcbPaMJ:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3ARED7OWcbPaMJ%3Ascholar.google.com%2F&start=120,,,,,,"https://scholar.googleusercontent.com/scholar?q=cache:RED7OWcbPaMJ:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",,,,,,
5,Řízení robotů pomocí stukturovaného hlubokého učení,W2vkQoxdRf4J,https://dspace.cvut.cz/handle/10467/105236,"… Hutter’s AIXI agent formulation [65]. AIXI is a general Reinforcement Learning agent that … Furthermore, this algorithm requires an actor-critic architecture that is unstable for training …",A Teymur - 2022 - dspace.cvut.cz,,,,,,,,,,,,,cvut.cz,PDF,https://dspace.cvut.cz/bitstream/handle/10467/105236/F3-D-2022-Azayev-Teymur-PHD_THESIS_CURRENT.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=W2vkQoxdRf4J,,,,,"https://scholar.google.com/scholar?q=related:W2vkQoxdRf4J:scholar.google.com/&scioq=%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3AW2vkQoxdRf4J%3Ascholar.google.com%2F&start=120,,,,,,"https://scholar.googleusercontent.com/scholar?q=cache:W2vkQoxdRf4J:scholar.google.com/+%22reinforcement+learning%22+OR+%22POMDP%22+OR+%22Bayesian+optimization%22+OR+%22Bayesian+reinforcement+learning%22+OR+%22Partial+observability%22+OR+%22monte+carlo+planning%22+OR+%22BADDr%22+OR+%22monte+carlo+tree+search%22+OR+%22temporal+difference%22+or+%22actor-critic%22+or+%22AIXI%22&hl=en&num=20&as_sdt=0,5",,,,,,
