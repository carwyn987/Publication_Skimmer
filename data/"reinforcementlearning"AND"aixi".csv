position,title,result_id,link,snippet,publication_info_summary,publication_info_authors_0_name,publication_info_authors_0_link,publication_info_authors_0_serpapi_scholar_link,publication_info_authors_0_author_id,publication_info_authors_1_name,publication_info_authors_1_link,publication_info_authors_1_serpapi_scholar_link,publication_info_authors_1_author_id,publication_info_authors_2_name,publication_info_authors_2_link,publication_info_authors_2_serpapi_scholar_link,publication_info_authors_2_author_id,publication_info_authors_3_name,publication_info_authors_3_link,publication_info_authors_3_serpapi_scholar_link,publication_info_authors_3_author_id,resources_0_title,resources_0_file_format,resources_0_link,inline_links_serpapi_cite_link,inline_links_cited_by_total,inline_links_cited_by_link,inline_links_cited_by_cites_id,inline_links_cited_by_serpapi_scholar_link,inline_links_related_pages_link,inline_links_serpapi_related_pages_link,inline_links_versions_total,inline_links_versions_link,inline_links_versions_cluster_id,inline_links_versions_serpapi_scholar_link,inline_links_cached_page_link,publication_info_authors_4_name,publication_info_authors_4_link,publication_info_authors_4_serpapi_scholar_link,publication_info_authors_4_author_id,type,inline_links_html_version
0,Reinforcement learning via AIXI approximation,E43KFfGjZ64J,https://ojs.aaai.org/index.php/AAAI/article/view/7667,"… general reinforcement learning agent. This approach is based on a direct approximation of AIXI, a … Previously, it has been unclear whether the theory of AIXI could motivate the design of …","J Veness, KS Ng, M Hutter, D Silver - Proceedings of the AAAI …, 2010 - ojs.aaai.org",J Veness,https://scholar.google.com/citations?user=_iYrAxEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=_iYrAxEAAAAJ&engine=google_scholar_author&hl=en,_iYrAxEAAAAJ,KS Ng,https://scholar.google.com/citations?user=4bL3ThUAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=4bL3ThUAAAAJ&engine=google_scholar_author&hl=en,4bL3ThUAAAAJ,M Hutter,https://scholar.google.com/citations?user=7hmCntEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=7hmCntEAAAAJ&engine=google_scholar_author&hl=en,7hmCntEAAAAJ,D Silver,https://scholar.google.com/citations?user=-8DNE4UAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=-8DNE4UAAAAJ&engine=google_scholar_author&hl=en,-8DNE4UAAAAJ,aaai.org,PDF,https://ojs.aaai.org/index.php/AAAI/article/view/7667/7528,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=E43KFfGjZ64J,32.0,"https://scholar.google.com/scholar?cites=12567193541048700179&as_sdt=5,38&sciodt=0,38&hl=en&num=20",12567193541048700179,https://serpapi.com/search.json?as_sdt=5%2C38&cites=12567193541048700179&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:E43KFfGjZ64J:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,38",https://serpapi.com/search.json?as_sdt=0%2C38&engine=google_scholar&hl=en&num=20&q=related%3AE43KFfGjZ64J%3Ascholar.google.com%2F&start=0,22.0,"https://scholar.google.com/scholar?cluster=12567193541048700179&hl=en&num=20&as_sdt=0,38",12567193541048700179,https://serpapi.com/search.json?as_sdt=0%2C38&cluster=12567193541048700179&engine=google_scholar&hl=en&num=20,"https://scholar.googleusercontent.com/scholar?q=cache:E43KFfGjZ64J:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,38",,,,,,
1,Dynamic Knowledge Injection for AIXI Agents,rZ89RFh1-EkJ,https://ojs.aaai.org/index.php/AAAI/article/view/29575,"… of AIXI, a Bayesian optimality notion for general reinforcement learning, can only approximate AIXI… The DynamicHedgeAIXI agent is the richest direct approximation of AIXI known to date …","S Yang-Zhao, KS Ng, M Hutter - … of the AAAI Conference on Artificial …, 2024 - ojs.aaai.org",S Yang-Zhao,https://scholar.google.com/citations?user=FwtCLh0AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=FwtCLh0AAAAJ&engine=google_scholar_author&hl=en,FwtCLh0AAAAJ,KS Ng,https://scholar.google.com/citations?user=4bL3ThUAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=4bL3ThUAAAAJ&engine=google_scholar_author&hl=en,4bL3ThUAAAAJ,M Hutter,https://scholar.google.com/citations?user=7hmCntEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=7hmCntEAAAAJ&engine=google_scholar_author&hl=en,7hmCntEAAAAJ,,,,,aaai.org,PDF,https://ojs.aaai.org/index.php/AAAI/article/view/29575/30966,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=rZ89RFh1-EkJ,1.0,"https://scholar.google.com/scholar?cites=5330139180955443117&as_sdt=5,38&sciodt=0,38&hl=en&num=20",5330139180955443117,https://serpapi.com/search.json?as_sdt=5%2C38&cites=5330139180955443117&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:rZ89RFh1-EkJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,38",https://serpapi.com/search.json?as_sdt=0%2C38&engine=google_scholar&hl=en&num=20&q=related%3ArZ89RFh1-EkJ%3Ascholar.google.com%2F&start=0,2.0,"https://scholar.google.com/scholar?cluster=5330139180955443117&hl=en&num=20&as_sdt=0,38",5330139180955443117,https://serpapi.com/search.json?as_sdt=0%2C38&cluster=5330139180955443117&engine=google_scholar&hl=en&num=20,"https://scholar.googleusercontent.com/scholar?q=cache:rZ89RFh1-EkJ:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,38",,,,,,
2,AIXIjs: A software demo for general reinforcement learning,sxf05s7RjbwJ,https://arxiv.org/abs/1705.07615,"… is the famous AIXI model, which is … AIXI is formulated as a Bayesian reinforcement learner, and makes few assumptions about the nature of its environment; notably, when studying AIXI …","J Aslanides - arXiv preprint arXiv:1705.07615, 2017 - arxiv.org",J Aslanides,https://scholar.google.com/citations?user=jWIWqfcAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=jWIWqfcAAAAJ&engine=google_scholar_author&hl=en,jWIWqfcAAAAJ,,,,,,,,,,,,,arxiv.org,PDF,https://arxiv.org/pdf/1705.07615,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=sxf05s7RjbwJ,6.0,"https://scholar.google.com/scholar?cites=13586746337414879155&as_sdt=5,38&sciodt=0,38&hl=en&num=20",13586746337414879155,https://serpapi.com/search.json?as_sdt=5%2C38&cites=13586746337414879155&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:sxf05s7RjbwJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,38",https://serpapi.com/search.json?as_sdt=0%2C38&engine=google_scholar&hl=en&num=20&q=related%3Asxf05s7RjbwJ%3Ascholar.google.com%2F&start=0,3.0,"https://scholar.google.com/scholar?cluster=13586746337414879155&hl=en&num=20&as_sdt=0,38",13586746337414879155,https://serpapi.com/search.json?as_sdt=0%2C38&cluster=13586746337414879155&engine=google_scholar&hl=en&num=20,"https://scholar.googleusercontent.com/scholar?q=cache:sxf05s7RjbwJ:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,38",,,,,,
3,A Direct Approximation of AIXI Using Logical State Abstractions,SGG8UIN0FQsJ,https://proceedings.neurips.cc/paper_files/paper/2022/hash/ed91353f700d113e5d848c7e04a858b0-Abstract-Conference.html,"… We propose a practical integration of logical state abstraction with AIXI, a Bayesian optimality notion for reinforcement learning agents, to significantly expand the model class that AIXI …","S Yang-Zhao, T Wang, KS Ng - Advances in Neural …, 2022 - proceedings.neurips.cc",S Yang-Zhao,https://scholar.google.com/citations?user=FwtCLh0AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=FwtCLh0AAAAJ&engine=google_scholar_author&hl=en,FwtCLh0AAAAJ,T Wang,https://scholar.google.com/citations?user=OfOGvmUAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=OfOGvmUAAAAJ&engine=google_scholar_author&hl=en,OfOGvmUAAAAJ,KS Ng,https://scholar.google.com/citations?user=4bL3ThUAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=4bL3ThUAAAAJ&engine=google_scholar_author&hl=en,4bL3ThUAAAAJ,,,,,neurips.cc,PDF,https://proceedings.neurips.cc/paper_files/paper/2022/file/ed91353f700d113e5d848c7e04a858b0-Paper-Conference.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=SGG8UIN0FQsJ,4.0,"https://scholar.google.com/scholar?cites=798672616272191816&as_sdt=5,38&sciodt=0,38&hl=en&num=20",798672616272191816,https://serpapi.com/search.json?as_sdt=5%2C38&cites=798672616272191816&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:SGG8UIN0FQsJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,38",https://serpapi.com/search.json?as_sdt=0%2C38&engine=google_scholar&hl=en&num=20&q=related%3ASGG8UIN0FQsJ%3Ascholar.google.com%2F&start=0,4.0,"https://scholar.google.com/scholar?cluster=798672616272191816&hl=en&num=20&as_sdt=0,38",798672616272191816,https://serpapi.com/search.json?as_sdt=0%2C38&cluster=798672616272191816&engine=google_scholar&hl=en&num=20,"https://scholar.googleusercontent.com/scholar?q=cache:SGG8UIN0FQsJ:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,38",,,,,,
4,Ideas for a reinforcement learning algorithm that learns programs,Uk5Fg5P_WYIJ,https://link.springer.com/chapter/10.1007/978-3-319-41649-6_36,"… AIXI, which is a general framework for reinforcement learning, can learn programs as the … AIXI has a computable and computationally tractable approximation, MC-AIXI(FAC-CTW), but it …","S Katayama - … General Intelligence: 9th International Conference, AGI …, 2016 - Springer",,,,,,,,,,,,,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=Uk5Fg5P_WYIJ,4.0,"https://scholar.google.com/scholar?cites=9392819506885512786&as_sdt=5,38&sciodt=0,38&hl=en&num=20",9392819506885512786,https://serpapi.com/search.json?as_sdt=5%2C38&cites=9392819506885512786&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:Uk5Fg5P_WYIJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,38",https://serpapi.com/search.json?as_sdt=0%2C38&engine=google_scholar&hl=en&num=20&q=related%3AUk5Fg5P_WYIJ%3Ascholar.google.com%2F&start=0,,,,,,,,,,,
5,A monte-carlo aixi approximation,Kx94lyXOheMJ,https://www.jair.org/index.php/jair/article/view/10685,… general reinforcement learning agent. Our approach is based on a direct approximation of … evaluates a practical reinforcement learning agent that is directly inspired by the AIXI theory. …,"J Veness, KS Ng, M Hutter, W Uther, D Silver - Journal of Artificial …, 2011 - jair.org",J Veness,https://scholar.google.com/citations?user=_iYrAxEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=_iYrAxEAAAAJ&engine=google_scholar_author&hl=en,_iYrAxEAAAAJ,KS Ng,https://scholar.google.com/citations?user=4bL3ThUAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=4bL3ThUAAAAJ&engine=google_scholar_author&hl=en,4bL3ThUAAAAJ,M Hutter,https://scholar.google.com/citations?user=7hmCntEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=7hmCntEAAAAJ&engine=google_scholar_author&hl=en,7hmCntEAAAAJ,W Uther,https://scholar.google.com/citations?user=-qvphmUAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=-qvphmUAAAAJ&engine=google_scholar_author&hl=en,-qvphmUAAAAJ,jair.org,PDF,https://www.jair.org/index.php/jair/article/download/10685/25533/,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=Kx94lyXOheMJ,221.0,"https://scholar.google.com/scholar?cites=16394736679362502443&as_sdt=5,38&sciodt=0,38&hl=en&num=20",16394736679362502443,https://serpapi.com/search.json?as_sdt=5%2C38&cites=16394736679362502443&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:Kx94lyXOheMJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,38",https://serpapi.com/search.json?as_sdt=0%2C38&engine=google_scholar&hl=en&num=20&q=related%3AKx94lyXOheMJ%3Ascholar.google.com%2F&start=0,28.0,"https://scholar.google.com/scholar?cluster=16394736679362502443&hl=en&num=20&as_sdt=0,38",16394736679362502443,https://serpapi.com/search.json?as_sdt=0%2C38&cluster=16394736679362502443&engine=google_scholar&hl=en&num=20,"https://scholar.googleusercontent.com/scholar?q=cache:Kx94lyXOheMJ:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,38",D Silver,https://scholar.google.com/citations?user=-8DNE4UAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=-8DNE4UAAAAJ&engine=google_scholar_author&hl=en,-8DNE4UAAAAJ,,
6,A gentle introduction to the universal algorithmic agent AIXI,Hg-qF1NmLsIJ,https://www.researchgate.net/profile/Marcus-Hutter/publication/228851452_A_gentle_introduction_to_the_universal_algorithmic_agent_AIXI/links/0fcfd513fb2bb4148d000000/A-gentle-introduction-to-the-universal-algorithmic-agent-AIXI.pdf,"… We give strong arguments that the resulting AIXI model is the … supervised learning, how the AIXI model can formally solve … AIXI model, a parameter-free optimal reinforcement learning …","M Hutter - Artificial General Intelligence, 2003 - researchgate.net",M Hutter,https://scholar.google.com/citations?user=7hmCntEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=7hmCntEAAAAJ&engine=google_scholar_author&hl=en,7hmCntEAAAAJ,,,,,,,,,,,,,researchgate.net,PDF,https://www.researchgate.net/profile/Marcus-Hutter/publication/228851452_A_gentle_introduction_to_the_universal_algorithmic_agent_AIXI/links/0fcfd513fb2bb4148d000000/A-gentle-introduction-to-the-universal-algorithmic-agent-AIXI.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=Hg-qF1NmLsIJ,18.0,"https://scholar.google.com/scholar?cites=13992233599352049438&as_sdt=5,38&sciodt=0,38&hl=en&num=20",13992233599352049438,https://serpapi.com/search.json?as_sdt=5%2C38&cites=13992233599352049438&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:Hg-qF1NmLsIJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,38",https://serpapi.com/search.json?as_sdt=0%2C38&engine=google_scholar&hl=en&num=20&q=related%3AHg-qF1NmLsIJ%3Ascholar.google.com%2F&start=0,3.0,"https://scholar.google.com/scholar?cluster=13992233599352049438&hl=en&num=20&as_sdt=0,38",13992233599352049438,https://serpapi.com/search.json?as_sdt=0%2C38&cluster=13992233599352049438&engine=google_scholar&hl=en&num=20,"https://scholar.googleusercontent.com/scholar?q=cache:Hg-qF1NmLsIJ:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,38",,,,,Pdf,
7,On the computability of AIXI,GN4WxQFmLrMJ,https://arxiv.org/abs/1510.05572,"… the reinforcement learning agent AIXI … AIXI is not limit computable, thus it cannot be approximated using finite computation. Our main result is a limit-computable ε-optimal version of AIXI …","J Leike, M Hutter - arXiv preprint arXiv:1510.05572, 2015 - arxiv.org",J Leike,https://scholar.google.com/citations?user=beiWcokAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=beiWcokAAAAJ&engine=google_scholar_author&hl=en,beiWcokAAAAJ,M Hutter,https://scholar.google.com/citations?user=7hmCntEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=7hmCntEAAAAJ&engine=google_scholar_author&hl=en,7hmCntEAAAAJ,,,,,,,,,arxiv.org,PDF,https://arxiv.org/pdf/1510.05572,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=GN4WxQFmLrMJ,9.0,"https://scholar.google.com/scholar?cites=12911369339505401368&as_sdt=5,38&sciodt=0,38&hl=en&num=20",12911369339505401368,https://serpapi.com/search.json?as_sdt=5%2C38&cites=12911369339505401368&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:GN4WxQFmLrMJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,38",https://serpapi.com/search.json?as_sdt=0%2C38&engine=google_scholar&hl=en&num=20&q=related%3AGN4WxQFmLrMJ%3Ascholar.google.com%2F&start=0,14.0,"https://scholar.google.com/scholar?cluster=12911369339505401368&hl=en&num=20&as_sdt=0,38",12911369339505401368,https://serpapi.com/search.json?as_sdt=0%2C38&cluster=12911369339505401368&engine=google_scholar&hl=en&num=20,"https://scholar.googleusercontent.com/scholar?q=cache:GN4WxQFmLrMJ:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,38",,,,,,
8,Q-learning for history-based reinforcement learning,wVMBto02QzIJ,https://proceedings.mlr.press/v29/Daswani13.html,"… reinforcement learning framework, which chooses maps based on a cost criteria. The cost criterion used so far for feature reinforcement learning … We also compare against MC-AIXI on …","M Daswani, P Sunehag… - Asian Conference on …, 2013 - proceedings.mlr.press",M Daswani,https://scholar.google.com/citations?user=GU-CgkEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=GU-CgkEAAAAJ&engine=google_scholar_author&hl=en,GU-CgkEAAAAJ,P Sunehag,https://scholar.google.com/citations?user=7QcT04EAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=7QcT04EAAAAJ&engine=google_scholar_author&hl=en,7QcT04EAAAAJ,,,,,,,,,mlr.press,PDF,http://proceedings.mlr.press/v29/Daswani13.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=wVMBto02QzIJ,23.0,"https://scholar.google.com/scholar?cites=3621798507607839681&as_sdt=5,38&sciodt=0,38&hl=en&num=20",3621798507607839681,https://serpapi.com/search.json?as_sdt=5%2C38&cites=3621798507607839681&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:wVMBto02QzIJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,38",https://serpapi.com/search.json?as_sdt=0%2C38&engine=google_scholar&hl=en&num=20&q=related%3AwVMBto02QzIJ%3Ascholar.google.com%2F&start=0,13.0,"https://scholar.google.com/scholar?cluster=3621798507607839681&hl=en&num=20&as_sdt=0,38",3621798507607839681,https://serpapi.com/search.json?as_sdt=0%2C38&cluster=3621798507607839681&engine=google_scholar&hl=en&num=20,"http://scholar.googleusercontent.com/scholar?q=cache:wVMBto02QzIJ:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,38",,,,,,
9,Self-Predictive Universal AI,obLq9N9NL2YJ,https://proceedings.neurips.cc/paper_files/paper/2023/hash/56a225639da77e8f7c0409f6d5ba996b-Abstract-Conference.html,"… we call Self-AIXI, that on the contrary to AIXI, maximally exploits … We prove that Self-AIXI converges to AIXI, and inherits a … planning problem are distributional reinforcement learning [44], …","E Catt, J Grau-Moya, M Hutter… - Advances in …, 2023 - proceedings.neurips.cc",E Catt,https://scholar.google.com/citations?user=d1JYeMIAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=d1JYeMIAAAAJ&engine=google_scholar_author&hl=en,d1JYeMIAAAAJ,J Grau-Moya,https://scholar.google.com/citations?user=u8ccN8sAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=u8ccN8sAAAAJ&engine=google_scholar_author&hl=en,u8ccN8sAAAAJ,M Hutter,https://scholar.google.com/citations?user=7hmCntEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=7hmCntEAAAAJ&engine=google_scholar_author&hl=en,7hmCntEAAAAJ,,,,,neurips.cc,PDF,https://proceedings.neurips.cc/paper_files/paper/2023/file/56a225639da77e8f7c0409f6d5ba996b-Paper-Conference.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=obLq9N9NL2YJ,,,,,"https://scholar.google.com/scholar?q=related:obLq9N9NL2YJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,38",https://serpapi.com/search.json?as_sdt=0%2C38&engine=google_scholar&hl=en&num=20&q=related%3AobLq9N9NL2YJ%3Ascholar.google.com%2F&start=0,2.0,"https://scholar.google.com/scholar?cluster=7363189540056117921&hl=en&num=20&as_sdt=0,38",7363189540056117921,https://serpapi.com/search.json?as_sdt=0%2C38&cluster=7363189540056117921&engine=google_scholar&hl=en&num=20,"https://scholar.googleusercontent.com/scholar?q=cache:obLq9N9NL2YJ:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,38",,,,,,
10,Optimistic AIXI,dbYFpahznb4J,https://link.springer.com/chapter/10.1007/978-3-642-35506-6_32,"… In Section 2, we discuss the rational betting theory that has recently been used to derive AIXI [SH11a] and in Section 3, after introducing the reinforcement learning agent setting, we …","P Sunehag, M Hutter - … : 5th International Conference, AGI 2012, Oxford …, 2012 - Springer",P Sunehag,https://scholar.google.com/citations?user=7QcT04EAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=7QcT04EAAAAJ&engine=google_scholar_author&hl=en,7QcT04EAAAAJ,M Hutter,https://scholar.google.com/citations?user=7hmCntEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=7hmCntEAAAAJ&engine=google_scholar_author&hl=en,7hmCntEAAAAJ,,,,,,,,,hutter1.net,PDF,http://www.hutter1.net/publ/aixiopt.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=dbYFpahznb4J,11.0,"https://scholar.google.com/scholar?cites=13735261606710195829&as_sdt=5,38&sciodt=0,38&hl=en&num=20",13735261606710195829,https://serpapi.com/search.json?as_sdt=5%2C38&cites=13735261606710195829&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:dbYFpahznb4J:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,38",https://serpapi.com/search.json?as_sdt=0%2C38&engine=google_scholar&hl=en&num=20&q=related%3AdbYFpahznb4J%3Ascholar.google.com%2F&start=0,11.0,"https://scholar.google.com/scholar?cluster=13735261606710195829&hl=en&num=20&as_sdt=0,38",13735261606710195829,https://serpapi.com/search.json?as_sdt=0%2C38&cluster=13735261606710195829&engine=google_scholar&hl=en&num=20,,,,,,,
11,Non-Markovian State Aggregation for Reinforcement Learning,rJ5Sjtw5HDgJ,,,D Johnston - 2015,,,,,,,,,,,,,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=rJ5Sjtw5HDgJ,,,,,"https://scholar.google.com/scholar?q=related:rJ5Sjtw5HDgJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,38",https://serpapi.com/search.json?as_sdt=0%2C38&engine=google_scholar&hl=en&num=20&q=related%3ArJ5Sjtw5HDgJ%3Ascholar.google.com%2F&start=0,,,,,,,,,,Citation,
12,On the computability of Solomonoff induction and AIXI,7VLoT6vM0zEJ,https://www.sciencedirect.com/science/article/pii/S0304397517308502,"… and the reinforcement learning agent AIXI are proposed … Moreover, we show that AIXI is not limit computable, thus it … ε-optimal approximations to AIXI. We also derive computability …","J Leike, M Hutter - Theoretical Computer Science, 2018 - Elsevier",J Leike,https://scholar.google.com/citations?user=beiWcokAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=beiWcokAAAAJ&engine=google_scholar_author&hl=en,beiWcokAAAAJ,M Hutter,https://scholar.google.com/citations?user=7hmCntEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=7hmCntEAAAAJ&engine=google_scholar_author&hl=en,7hmCntEAAAAJ,,,,,,,,,sciencedirect.com,HTML,https://www.sciencedirect.com/science/article/pii/S0304397517308502,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=7VLoT6vM0zEJ,8.0,"https://scholar.google.com/scholar?cites=3590438364096516845&as_sdt=5,38&sciodt=0,38&hl=en&num=20",3590438364096516845,https://serpapi.com/search.json?as_sdt=5%2C38&cites=3590438364096516845&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:7VLoT6vM0zEJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,38",https://serpapi.com/search.json?as_sdt=0%2C38&engine=google_scholar&hl=en&num=20&q=related%3A7VLoT6vM0zEJ%3Ascholar.google.com%2F&start=0,4.0,"https://scholar.google.com/scholar?cluster=3590438364096516845&hl=en&num=20&as_sdt=0,38",3590438364096516845,https://serpapi.com/search.json?as_sdt=0%2C38&cluster=3590438364096516845&engine=google_scholar&hl=en&num=20,,,,,,Html,https://www.sciencedirect.com/science/article/pii/S0304397517308502
13,"AIXI, FEP-AI, and integrated world models: Towards a unified understanding of intelligence and consciousness",O1XO9px8W-cJ,https://link.springer.com/chapter/10.1007/978-3-031-28719-0_18,"… With RNN-realized predictive coding mechanisms combined with reinforcement learning, systems learn to efficiently represent themselves, with these symbols becoming activated in the …","A Safron - International Workshop on Active Inference, 2022 - Springer",A Safron,https://scholar.google.com/citations?user=dZOoilgAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=dZOoilgAAAAJ&engine=google_scholar_author&hl=en,dZOoilgAAAAJ,,,,,,,,,,,,,osf.io,PDF,https://osf.io/preprints/psyarxiv/4qkjp/download,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=O1XO9px8W-cJ,2.0,"https://scholar.google.com/scholar?cites=16671055459239482683&as_sdt=5,38&sciodt=0,38&hl=en&num=20",16671055459239482683,https://serpapi.com/search.json?as_sdt=5%2C38&cites=16671055459239482683&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:O1XO9px8W-cJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,38",https://serpapi.com/search.json?as_sdt=0%2C38&engine=google_scholar&hl=en&num=20&q=related%3AO1XO9px8W-cJ%3Ascholar.google.com%2F&start=0,11.0,"https://scholar.google.com/scholar?cluster=16671055459239482683&hl=en&num=20&as_sdt=0,38",16671055459239482683,https://serpapi.com/search.json?as_sdt=0%2C38&cluster=16671055459239482683&engine=google_scholar&hl=en&num=20,,,,,,,
14,Universal reinforcement learning algorithms: Survey and experiments,rEcXSdzkQkYJ,https://arxiv.org/abs/1705.10557,"… In the setting of universal reinforcement learning, we lift the Markov, ergodic, and full-… -AIXI-Dirichlet appears to have asymptotically higher variance in its average reward than MC-AIXI. …","J Aslanides, J Leike, M Hutter - arXiv preprint arXiv:1705.10557, 2017 - arxiv.org",J Aslanides,https://scholar.google.com/citations?user=jWIWqfcAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=jWIWqfcAAAAJ&engine=google_scholar_author&hl=en,jWIWqfcAAAAJ,J Leike,https://scholar.google.com/citations?user=beiWcokAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=beiWcokAAAAJ&engine=google_scholar_author&hl=en,beiWcokAAAAJ,M Hutter,https://scholar.google.com/citations?user=7hmCntEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=7hmCntEAAAAJ&engine=google_scholar_author&hl=en,7hmCntEAAAAJ,,,,,arxiv.org,PDF,https://arxiv.org/pdf/1705.10557,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=rEcXSdzkQkYJ,25.0,"https://scholar.google.com/scholar?cites=5062860565888059308&as_sdt=5,38&sciodt=0,38&hl=en&num=20",5062860565888059308,https://serpapi.com/search.json?as_sdt=5%2C38&cites=5062860565888059308&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:rEcXSdzkQkYJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,38",https://serpapi.com/search.json?as_sdt=0%2C38&engine=google_scholar&hl=en&num=20&q=related%3ArEcXSdzkQkYJ%3Ascholar.google.com%2F&start=0,8.0,"https://scholar.google.com/scholar?cluster=5062860565888059308&hl=en&num=20&as_sdt=0,38",5062860565888059308,https://serpapi.com/search.json?as_sdt=0%2C38&cluster=5062860565888059308&engine=google_scholar&hl=en&num=20,"https://scholar.googleusercontent.com/scholar?q=cache:rEcXSdzkQkYJ:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,38",,,,,,
15,Feature reinforcement learning in practice,WbY-3gNrzKkJ,https://link.springer.com/chapter/10.1007/978-3-642-29946-9_10,… methods: The first approximation and implementation of AIXI for repeated matrix games [22]; … reinforcement learning setting and assumes n-Markov models of environments; and MC-AIXI…,"P Nguyen, P Sunehag, M Hutter - … Workshop on Reinforcement Learning, 2011 - Springer",P Nguyen,https://scholar.google.com/citations?user=cUGY-akAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=cUGY-akAAAAJ&engine=google_scholar_author&hl=en,cUGY-akAAAAJ,P Sunehag,https://scholar.google.com/citations?user=7QcT04EAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=7QcT04EAAAAJ&engine=google_scholar_author&hl=en,7QcT04EAAAAJ,M Hutter,https://scholar.google.com/citations?user=7hmCntEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=7hmCntEAAAAJ&engine=google_scholar_author&hl=en,7hmCntEAAAAJ,,,,,arxiv.org,PDF,https://arxiv.org/pdf/1108.3614,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=WbY-3gNrzKkJ,13.0,"https://scholar.google.com/scholar?cites=12235271952016520793&as_sdt=5,38&sciodt=0,38&hl=en&num=20",12235271952016520793,https://serpapi.com/search.json?as_sdt=5%2C38&cites=12235271952016520793&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:WbY-3gNrzKkJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,38",https://serpapi.com/search.json?as_sdt=0%2C38&engine=google_scholar&hl=en&num=20&q=related%3AWbY-3gNrzKkJ%3Ascholar.google.com%2F&start=0,17.0,"https://scholar.google.com/scholar?cluster=12235271952016520793&hl=en&num=20&as_sdt=0,38",12235271952016520793,https://serpapi.com/search.json?as_sdt=0%2C38&cluster=12235271952016520793&engine=google_scholar&hl=en&num=20,,,,,,,
16,Learning what to value,iMuqAqFCOrgJ,https://link.springer.com/chapter/10.1007/978-3-642-22887-2_35,"… In order to think clearly about ultraintelligent reinforcement learning agent implementations, we make use of Hutter’s AIXI[3][4], an optimality notion that solves the reinforcement learning …","D Dewey - International conference on artificial general …, 2011 - Springer",,,,,,,,,,,,,,,,,danieldewey.net,PDF,https://www.danieldewey.net/learning-what-to-value.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=iMuqAqFCOrgJ,74.0,"https://scholar.google.com/scholar?cites=13274996110929873800&as_sdt=5,38&sciodt=0,38&hl=en&num=20",13274996110929873800,https://serpapi.com/search.json?as_sdt=5%2C38&cites=13274996110929873800&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:iMuqAqFCOrgJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,38",https://serpapi.com/search.json?as_sdt=0%2C38&engine=google_scholar&hl=en&num=20&q=related%3AiMuqAqFCOrgJ%3Ascholar.google.com%2F&start=0,15.0,"https://scholar.google.com/scholar?cluster=13274996110929873800&hl=en&num=20&as_sdt=0,38",13274996110929873800,https://serpapi.com/search.json?as_sdt=0%2C38&cluster=13274996110929873800&engine=google_scholar&hl=en&num=20,,,,,,,
17,Nonparametric general reinforcement learning,cuVH0soPsrkJ,https://search.proquest.com/openview/e53820eee348a04eb3d2c0e95ce3d208/1?pq-origsite=gscholar&cbl=2026366,"… MDPs and consider reinforcement learning in environments … theory and put them in a reinforcement learning context: they … on Bayesian reinforcement learning agents, in particular AIXI. …",J Leike - 2016 - search.proquest.com,J Leike,https://scholar.google.com/citations?user=beiWcokAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=beiWcokAAAAJ&engine=google_scholar_author&hl=en,beiWcokAAAAJ,,,,,,,,,,,,,arxiv.org,PDF,https://arxiv.org/pdf/1611.08944,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=cuVH0soPsrkJ,29.0,"https://scholar.google.com/scholar?cites=13380774806656902514&as_sdt=5,38&sciodt=0,38&hl=en&num=20",13380774806656902514,https://serpapi.com/search.json?as_sdt=5%2C38&cites=13380774806656902514&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:cuVH0soPsrkJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,38",https://serpapi.com/search.json?as_sdt=0%2C38&engine=google_scholar&hl=en&num=20&q=related%3AcuVH0soPsrkJ%3Ascholar.google.com%2F&start=0,6.0,"https://scholar.google.com/scholar?cluster=13380774806656902514&hl=en&num=20&as_sdt=0,38",13380774806656902514,https://serpapi.com/search.json?as_sdt=0%2C38&cluster=13380774806656902514&engine=google_scholar&hl=en&num=20,,,,,,,
18,"Delusion, survival, and intelligent agents",6qs_ZGLEWGkJ,https://link.springer.com/chapter/10.1007/978-3-642-22887-2_2,"… : reinforcementlearning, goal-seeking, prediction-seeking, and knowledge-seeking agents. Our main results are that: 1) The reinforcement-learning … As for AIXI, we expect the learning …","M Ring, L Orseau - … General Intelligence: 4th International Conference, AGI …, 2011 - Springer",M Ring,https://scholar.google.com/citations?user=9MlmvQUAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=9MlmvQUAAAAJ&engine=google_scholar_author&hl=en,9MlmvQUAAAAJ,L Orseau,https://scholar.google.com/citations?user=HVJjWlEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=HVJjWlEAAAAJ&engine=google_scholar_author&hl=en,HVJjWlEAAAAJ,,,,,,,,,hal.science,PDF,https://hal.science/hal-01000226/document,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=6qs_ZGLEWGkJ,93.0,"https://scholar.google.com/scholar?cites=7591033098800704490&as_sdt=5,38&sciodt=0,38&hl=en&num=20",7591033098800704490,https://serpapi.com/search.json?as_sdt=5%2C38&cites=7591033098800704490&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:6qs_ZGLEWGkJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,38",https://serpapi.com/search.json?as_sdt=0%2C38&engine=google_scholar&hl=en&num=20&q=related%3A6qs_ZGLEWGkJ%3Ascholar.google.com%2F&start=0,17.0,"https://scholar.google.com/scholar?cluster=7591033098800704490&hl=en&num=20&as_sdt=0,38",7591033098800704490,https://serpapi.com/search.json?as_sdt=0%2C38&cluster=7591033098800704490&engine=google_scholar&hl=en&num=20,,,,,,,
19,Evaluating a reinforcement learning algorithm with a general intelligence test,lPr14VauaKkJ,https://link.springer.com/chapter/10.1007/978-3-642-25274-7_1,"… In fact, what we have done here for reinforcement learning … ’ classic and new reinforcement learning problems that look … is a Monte Carlo approximation to AIXI [16], which is showing …","J Insa-Cabrera, DL Dowe… - Conference of the Spanish …, 2011 - Springer",DL Dowe,https://scholar.google.com/citations?user=8ZlclUsAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=8ZlclUsAAAAJ&engine=google_scholar_author&hl=en,8ZlclUsAAAAJ,,,,,,,,,,,,,upv.es,PDF,https://riunet.upv.es/bitstream/handle/10251/37392/caepia11-jho-jic-dld.pdf?sequence=2,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=lPr14VauaKkJ,48.0,"https://scholar.google.com/scholar?cites=12207198478169143956&as_sdt=5,38&sciodt=0,38&hl=en&num=20",12207198478169143956,https://serpapi.com/search.json?as_sdt=5%2C38&cites=12207198478169143956&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:lPr14VauaKkJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,38",https://serpapi.com/search.json?as_sdt=0%2C38&engine=google_scholar&hl=en&num=20&q=related%3AlPr14VauaKkJ%3Ascholar.google.com%2F&start=0,17.0,"https://scholar.google.com/scholar?cluster=12207198478169143956&hl=en&num=20&as_sdt=0,38",12207198478169143956,https://serpapi.com/search.json?as_sdt=0%2C38&cluster=12207198478169143956&engine=google_scholar&hl=en&num=20,,,,,,,
0,Feature reinforcement learning: state of the art,tAO0I0I3I_UJ,https://cdn.aaai.org/ocs/ws/ws1209/8791-37973-1-PB.pdf,Feature reinforcement learning was introduced five years ago as a principled and practical approach to history-based learning. This paper examines the progress since its inception. We …,"M Daswani, P Sunehag, M Hutter - Workshops at the Twenty-Eighth …, 2014 - cdn.aaai.org",M Daswani,https://scholar.google.com/citations?user=GU-CgkEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=GU-CgkEAAAAJ&engine=google_scholar_author&hl=en,GU-CgkEAAAAJ,P Sunehag,https://scholar.google.com/citations?user=7QcT04EAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=7QcT04EAAAAJ&engine=google_scholar_author&hl=en,7QcT04EAAAAJ,M Hutter,https://scholar.google.com/citations?user=7hmCntEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=7hmCntEAAAAJ&engine=google_scholar_author&hl=en,7hmCntEAAAAJ,,,,,aaai.org,PDF,https://cdn.aaai.org/ocs/ws/ws1209/8791-37973-1-PB.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=tAO0I0I3I_UJ,16.0,"https://scholar.google.com/scholar?cites=17664022920683586484&as_sdt=2005&sciodt=0,5&hl=en&num=20",17664022920683586484,https://serpapi.com/search.json?as_sdt=2005&cites=17664022920683586484&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:tAO0I0I3I_UJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3AtAO0I0I3I_UJ%3Ascholar.google.com%2F&start=20,7.0,"https://scholar.google.com/scholar?cluster=17664022920683586484&hl=en&num=20&as_sdt=0,5",17664022920683586484,https://serpapi.com/search.json?as_sdt=0%2C5&cluster=17664022920683586484&engine=google_scholar&hl=en&num=20,"https://scholar.googleusercontent.com/scholar?q=cache:tAO0I0I3I_UJ:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,5",,,,,Pdf,
1,Reinforcement learning algorithms for solving classification problems,UlqYEg2IP8UJ,https://ieeexplore.ieee.org/abstract/document/5967372/,"… Furthermore, other reinforcement learning algorithms such as the AIXI agent [11], [12], the … It may also be worthwhile to look at ensembles of reinforcement learning algorithms [16], …","MA Wiering, H Van Hasselt… - … and Reinforcement …, 2011 - ieeexplore.ieee.org",MA Wiering,https://scholar.google.com/citations?user=880vFIgAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=880vFIgAAAAJ&engine=google_scholar_author&hl=en,880vFIgAAAAJ,H Van Hasselt,https://scholar.google.com/citations?user=W80oBMkAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=W80oBMkAAAAJ&engine=google_scholar_author&hl=en,W80oBMkAAAAJ,,,,,,,,,researchgate.net,PDF,https://www.researchgate.net/profile/Lambert-Schomaker/publication/224250582_Reinforcement_learning_algorithms_for_solving_classification_problems/links/0fcfd508e4b86cc5a5000000/Reinforcement-learning-algorithms-for-solving-classification-problems.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=UlqYEg2IP8UJ,82.0,"https://scholar.google.com/scholar?cites=14213228538732501586&as_sdt=2005&sciodt=0,5&hl=en&num=20",14213228538732501586,https://serpapi.com/search.json?as_sdt=2005&cites=14213228538732501586&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:UlqYEg2IP8UJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3AUlqYEg2IP8UJ%3Ascholar.google.com%2F&start=20,10.0,"https://scholar.google.com/scholar?cluster=14213228538732501586&hl=en&num=20&as_sdt=0,5",14213228538732501586,https://serpapi.com/search.json?as_sdt=0%2C5&cluster=14213228538732501586&engine=google_scholar&hl=en&num=20,,,,,,,
2,Feature reinforcement learning: Part I. unstructured MDPs,jamGgd3HJIYJ,https://intapi.sciendo.com/pdf/10.2478/v10229-011-0002-8,"… On the other hand, reinforcement learning is well-developed for small finite state Markov … expand the scope of many existing reinforcement learning algorithms and the agents that …",M Hutter - 2009 - intapi.sciendo.com,M Hutter,https://scholar.google.com/citations?user=7hmCntEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=7hmCntEAAAAJ&engine=google_scholar_author&hl=en,7hmCntEAAAAJ,,,,,,,,,,,,,sciendo.com,PDF,https://intapi.sciendo.com/pdf/10.2478/v10229-011-0002-8,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=jamGgd3HJIYJ,76.0,"https://scholar.google.com/scholar?cites=9666070454418712973&as_sdt=2005&sciodt=0,5&hl=en&num=20",9666070454418712973,https://serpapi.com/search.json?as_sdt=2005&cites=9666070454418712973&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:jamGgd3HJIYJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3AjamGgd3HJIYJ%3Ascholar.google.com%2F&start=20,16.0,"https://scholar.google.com/scholar?cluster=9666070454418712973&hl=en&num=20&as_sdt=0,5",9666070454418712973,https://serpapi.com/search.json?as_sdt=0%2C5&cluster=9666070454418712973&engine=google_scholar&hl=en&num=20,"https://scholar.googleusercontent.com/scholar?q=cache:jamGgd3HJIYJ:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,5",,,,,Pdf,
3,"Rationality, optimism and guarantees in general reinforcement learning",BKIxZzHLflYJ,https://www.jmlr.org/papers/volume16/sunehag15a/sunehag15a.pdf,… In Section 4 we show the importance of optimism for asymptotic optimality for a generic Bayesian reinforcement learning agent called AIXI and we extend this agent to an optimistic …,"P Sunehag, M Hutter - The Journal of Machine Learning Research, 2015 - jmlr.org",P Sunehag,https://scholar.google.com/citations?user=7QcT04EAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=7QcT04EAAAAJ&engine=google_scholar_author&hl=en,7QcT04EAAAAJ,M Hutter,https://scholar.google.com/citations?user=7hmCntEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=7hmCntEAAAAJ&engine=google_scholar_author&hl=en,7hmCntEAAAAJ,,,,,,,,,jmlr.org,PDF,https://www.jmlr.org/papers/volume16/sunehag15a/sunehag15a.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=BKIxZzHLflYJ,18.0,"https://scholar.google.com/scholar?cites=6232642347372487172&as_sdt=2005&sciodt=0,5&hl=en&num=20",6232642347372487172,https://serpapi.com/search.json?as_sdt=2005&cites=6232642347372487172&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:BKIxZzHLflYJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3ABKIxZzHLflYJ%3Ascholar.google.com%2F&start=20,9.0,"https://scholar.google.com/scholar?cluster=6232642347372487172&hl=en&num=20&as_sdt=0,5",6232642347372487172,https://serpapi.com/search.json?as_sdt=0%2C5&cluster=6232642347372487172&engine=google_scholar&hl=en&num=20,"https://scholar.googleusercontent.com/scholar?q=cache:BKIxZzHLflYJ:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,5",,,,,Pdf,
4,QKSA: Quantum Knowledge Seeking Agent--resource-optimized reinforcement learning using quantum process tomography,N0tpEpcmI1AJ,https://arxiv.org/abs/2112.03643,… UAGI is formulated in a general reinforcement learning (GRL… The canonical model of UAGI is the AIXI model [2]. It is the … the extrinsic reward function in AIXI to a utility function defined as …,"A Sarkar, Z Al-Ars, H Gandhi, K Bertels - arXiv preprint arXiv:2112.03643, 2021 - arxiv.org",A Sarkar,https://scholar.google.com/citations?user=Qo84iBgAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=Qo84iBgAAAAJ&engine=google_scholar_author&hl=en,Qo84iBgAAAAJ,Z Al-Ars,https://scholar.google.com/citations?user=RWpNIJ8AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=RWpNIJ8AAAAJ&engine=google_scholar_author&hl=en,RWpNIJ8AAAAJ,H Gandhi,https://scholar.google.com/citations?user=5r0CxM4AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=5r0CxM4AAAAJ&engine=google_scholar_author&hl=en,5r0CxM4AAAAJ,K Bertels,https://scholar.google.com/citations?user=a3c2b_wAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=a3c2b_wAAAAJ&engine=google_scholar_author&hl=en,a3c2b_wAAAAJ,arxiv.org,PDF,https://arxiv.org/pdf/2112.03643,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=N0tpEpcmI1AJ,5.0,"https://scholar.google.com/scholar?cites=5774501577509915447&as_sdt=2005&sciodt=0,5&hl=en&num=20",5774501577509915447,https://serpapi.com/search.json?as_sdt=2005&cites=5774501577509915447&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:N0tpEpcmI1AJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3AN0tpEpcmI1AJ%3Ascholar.google.com%2F&start=20,3.0,"https://scholar.google.com/scholar?cluster=5774501577509915447&hl=en&num=20&as_sdt=0,5",5774501577509915447,https://serpapi.com/search.json?as_sdt=0%2C5&cluster=5774501577509915447&engine=google_scholar&hl=en&num=20,"https://scholar.googleusercontent.com/scholar?q=cache:N0tpEpcmI1AJ:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,5",,,,,,
5,The benefits of model-based generalization in reinforcement learning,949fPq4cAbAJ,https://arxiv.org/abs/2211.02222,"… Model-based reinforcement learning (RL) refers to the class of RL … An AIXI agent maximizes expected return over all computable world … While AIXI is itself not computable, computable …","K Young, A Ramesh, L Kirsch… - arXiv preprint arXiv …, 2022 - arxiv.org",K Young,https://scholar.google.com/citations?user=zI2uHi8AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=zI2uHi8AAAAJ&engine=google_scholar_author&hl=en,zI2uHi8AAAAJ,A Ramesh,https://scholar.google.com/citations?user=60K82BkAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=60K82BkAAAAJ&engine=google_scholar_author&hl=en,60K82BkAAAAJ,L Kirsch,https://scholar.google.com/citations?user=w8AkOEAAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=w8AkOEAAAAAJ&engine=google_scholar_author&hl=en,w8AkOEAAAAAJ,,,,,arxiv.org,PDF,https://arxiv.org/pdf/2211.02222,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=949fPq4cAbAJ,13.0,"https://scholar.google.com/scholar?cites=12682449560348364791&as_sdt=2005&sciodt=0,5&hl=en&num=20",12682449560348364791,https://serpapi.com/search.json?as_sdt=2005&cites=12682449560348364791&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:949fPq4cAbAJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3A949fPq4cAbAJ%3Ascholar.google.com%2F&start=20,7.0,"https://scholar.google.com/scholar?cluster=12682449560348364791&hl=en&num=20&as_sdt=0,5",12682449560348364791,https://serpapi.com/search.json?as_sdt=0%2C5&cluster=12682449560348364791&engine=google_scholar&hl=en&num=20,"https://scholar.googleusercontent.com/scholar?q=cache:949fPq4cAbAJ:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,5",,,,,,
6,Causeoccam: Learning interpretable abstract representations in reinforcement learning environments via model sparsity,7MJl3quaF4MJ,https://infoscience.epfl.ch/record/287445,"… Thank you to Lê Nguyên Hoang for our discussions on Kolmogorov-Solomonoff complexity and AIXI, and to Marcus Hutter and his students for the feedback on our work in 2019. …",S Volodin - 2021 - infoscience.epfl.ch,S Volodin,https://scholar.google.com/citations?user=AsDOBXIAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=AsDOBXIAAAAJ&engine=google_scholar_author&hl=en,AsDOBXIAAAAJ,,,,,,,,,,,,,epfl.ch,PDF,https://infoscience.epfl.ch/record/287445/files/CauseOccam_Learning_Interpretable_Abstract_Representations_in_Reinforcement_Learning_Environments_via_Model_Sparsity.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=7MJl3quaF4MJ,6.0,"https://scholar.google.com/scholar?cites=9446188806394200812&as_sdt=2005&sciodt=0,5&hl=en&num=20",9446188806394200812,https://serpapi.com/search.json?as_sdt=2005&cites=9446188806394200812&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:7MJl3quaF4MJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3A7MJl3quaF4MJ%3Ascholar.google.com%2F&start=20,,,,,"https://scholar.googleusercontent.com/scholar?q=cache:7MJl3quaF4MJ:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,5",,,,,,
7,On the Optimality of General Reinforcement Learners,FCgzGtee0IkJ,https://ewrl.wordpress.com/wp-content/uploads/2015/02/ewrl12_2015_submission_3.pdf,"… AIXI. While it may still serve as a gold standard for general reinforcement learning, our results imply that AIXI … Keywords: AIXI, general reinforcement learning, universal Turing machine, …","J Leike, M Hutter - Journal of Machine Learning Research, 2015 - ewrl.wordpress.com",M Hutter,https://scholar.google.com/citations?user=7hmCntEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=7hmCntEAAAAJ&engine=google_scholar_author&hl=en,7hmCntEAAAAJ,,,,,,,,,,,,,wordpress.com,PDF,https://ewrl.wordpress.com/wp-content/uploads/2015/02/ewrl12_2015_submission_3.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=FCgzGtee0IkJ,,,,,"https://scholar.google.com/scholar?q=related:FCgzGtee0IkJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3AFCgzGtee0IkJ%3Ascholar.google.com%2F&start=20,,,,,"https://scholar.googleusercontent.com/scholar?q=cache:FCgzGtee0IkJ:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,5",,,,,Pdf,
8,Multi-Agent Reinforcement Learning Based Cooperative Multitype Task Offloading Strategy for Internet of Vehicles in B5G/6G Network,lKB11Y3k6AcJ,https://ieeexplore.ieee.org/abstract/document/10045759/,"With the development of intelligent transportation, various computation intensive and delay sensitive applications are emerging in the Internet of Vehicles (IoV). The B5G/6G (Beyond 5th …","Y Cui, H Li, D Zhang, A Zhu, Y Li… - IEEE Internet of Things …, 2023 - ieeexplore.ieee.org",,,,,,,,,,,,,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=lKB11Y3k6AcJ,8.0,"https://scholar.google.com/scholar?cites=569956650685145236&as_sdt=2005&sciodt=0,5&hl=en&num=20",569956650685145236,https://serpapi.com/search.json?as_sdt=2005&cites=569956650685145236&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:lKB11Y3k6AcJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3AlKB11Y3k6AcJ%3Ascholar.google.com%2F&start=20,,,,,,,,,,,
9,Context tree maximizing,KCM36gEm0yUJ,https://ojs.aaai.org/index.php/AAAI/article/view/8310,"… in reinforcement learning for … -AIXI-CTW. ΦMDP attempts to reduce the general RL problem, where the environment’s states and dynamics are both unknown, to an MDP, while MCAIXI-…","P Nguyen, P Sunehag, M Hutter - … of the AAAI Conference on Artificial …, 2012 - ojs.aaai.org",P Nguyen,https://scholar.google.com/citations?user=cUGY-akAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=cUGY-akAAAAJ&engine=google_scholar_author&hl=en,cUGY-akAAAAJ,P Sunehag,https://scholar.google.com/citations?user=7QcT04EAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=7QcT04EAAAAJ&engine=google_scholar_author&hl=en,7QcT04EAAAAJ,M Hutter,https://scholar.google.com/citations?user=7hmCntEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=7hmCntEAAAAJ&engine=google_scholar_author&hl=en,7hmCntEAAAAJ,,,,,aaai.org,PDF,https://ojs.aaai.org/index.php/AAAI/article/download/8310/8169,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=KCM36gEm0yUJ,14.0,"https://scholar.google.com/scholar?cites=2725563989155586856&as_sdt=2005&sciodt=0,5&hl=en&num=20",2725563989155586856,https://serpapi.com/search.json?as_sdt=2005&cites=2725563989155586856&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:KCM36gEm0yUJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3AKCM36gEm0yUJ%3Ascholar.google.com%2F&start=20,11.0,"https://scholar.google.com/scholar?cluster=2725563989155586856&hl=en&num=20&as_sdt=0,5",2725563989155586856,https://serpapi.com/search.json?as_sdt=0%2C5&cluster=2725563989155586856&engine=google_scholar&hl=en&num=20,"https://scholar.googleusercontent.com/scholar?q=cache:KCM36gEm0yUJ:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,5",,,,,,
10,Asymptotic non-learnability of universal agents with computable horizon functions,jWFKM-3t1ywJ,https://www.sciencedirect.com/science/article/pii/S0304397512009358,"… definition of AIXI can sometimes be suboptimal in a certain sense, but that this behavior is still the most rational one, hence emphasizing the difficulty of universal reinforcement learning. …","L Orseau - Theoretical Computer Science, 2013 - Elsevier",L Orseau,https://scholar.google.com/citations?user=HVJjWlEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=HVJjWlEAAAAJ&engine=google_scholar_author&hl=en,HVJjWlEAAAAJ,,,,,,,,,,,,,sciencedirect.com,PDF,https://www.sciencedirect.com/science/article/pii/S0304397512009358/pdf?md5=37301e0d14c1910c652252999dec4c67&pid=1-s2.0-S0304397512009358-main.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=jWFKM-3t1ywJ,14.0,"https://scholar.google.com/scholar?cites=3231312860685164941&as_sdt=2005&sciodt=0,5&hl=en&num=20",3231312860685164941,https://serpapi.com/search.json?as_sdt=2005&cites=3231312860685164941&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:jWFKM-3t1ywJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3AjWFKM-3t1ywJ%3Ascholar.google.com%2F&start=20,7.0,"https://scholar.google.com/scholar?cluster=3231312860685164941&hl=en&num=20&as_sdt=0,5",3231312860685164941,https://serpapi.com/search.json?as_sdt=0%2C5&cluster=3231312860685164941&engine=google_scholar&hl=en&num=20,,,,,,,
11,Context Tree Maximizing Reinforcement Learning,GXLI35Jr4HkJ,https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=bd00b83b43e04a018579adfcf461a5da68f9bd60,"… MC-AIXI-CTW, CTMRL is dramatically more efficient in both computation time and memory • Unlike MC-AIXI-… that enables a connection with traditional reinforcement learning methods. …",C Maze - Citeseer,,,,,,,,,,,,,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=GXLI35Jr4HkJ,,,,,"https://scholar.google.com/scholar?q=related:GXLI35Jr4HkJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3AGXLI35Jr4HkJ%3Ascholar.google.com%2F&start=20,2.0,"https://scholar.google.com/scholar?cluster=8782137551936320025&hl=en&num=20&as_sdt=0,5",8782137551936320025,https://serpapi.com/search.json?as_sdt=0%2C5&cluster=8782137551936320025&engine=google_scholar&hl=en&num=20,"https://scholar.googleusercontent.com/scholar?q=cache:GXLI35Jr4HkJ:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,5",,,,,,
12,Value driven representation for human-in-the-loop reinforcement learning,ZMxkaYhWcF0J,https://dl.acm.org/doi/abs/10.1145/3320435.3320471,"… reinforcement learning, where a human system designer can modify the observation space definition used by a reinforcement learning … We compare to MC-AIXI-CTW that outperformed …","R Keramati, E Brunskill - Proceedings of the 27th ACM Conference on …, 2019 - dl.acm.org",R Keramati,https://scholar.google.com/citations?user=oStofYwAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=oStofYwAAAAJ&engine=google_scholar_author&hl=en,oStofYwAAAAJ,E Brunskill,https://scholar.google.com/citations?user=HaN8b2YAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=HaN8b2YAAAAJ&engine=google_scholar_author&hl=en,HaN8b2YAAAAJ,,,,,,,,,acm.org,PDF,https://dl.acm.org/doi/pdf/10.1145/3320435.3320471,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=ZMxkaYhWcF0J,3.0,"https://scholar.google.com/scholar?cites=6732976586802646116&as_sdt=2005&sciodt=0,5&hl=en&num=20",6732976586802646116,https://serpapi.com/search.json?as_sdt=2005&cites=6732976586802646116&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:ZMxkaYhWcF0J:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3AZMxkaYhWcF0J%3Ascholar.google.com%2F&start=20,4.0,"https://scholar.google.com/scholar?cluster=6732976586802646116&hl=en&num=20&as_sdt=0,5",6732976586802646116,https://serpapi.com/search.json?as_sdt=0%2C5&cluster=6732976586802646116&engine=google_scholar&hl=en&num=20,,,,,,,
13,Self-modification and mortality in artificial agents,uazOByGVKRwJ,https://link.springer.com/chapter/10.1007/978-3-642-22887-2_1,"… To pursue these issues rigorously, we place AIXI [1] within an … of four common agents: reinforcement-learning, goal-seeking… They are (1) a (fairly traditional) reinforcement-learning agent…","L Orseau, M Ring - … General Intelligence: 4th International Conference, AGI …, 2011 - Springer",L Orseau,https://scholar.google.com/citations?user=HVJjWlEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=HVJjWlEAAAAJ&engine=google_scholar_author&hl=en,HVJjWlEAAAAJ,M Ring,https://scholar.google.com/citations?user=9MlmvQUAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=9MlmvQUAAAAJ&engine=google_scholar_author&hl=en,9MlmvQUAAAAJ,,,,,,,,,hal.science,PDF,https://hal.science/hal-01000195/document,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=uazOByGVKRwJ,44.0,"https://scholar.google.com/scholar?cites=2029317076204563641&as_sdt=2005&sciodt=0,5&hl=en&num=20",2029317076204563641,https://serpapi.com/search.json?as_sdt=2005&cites=2029317076204563641&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:uazOByGVKRwJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3AuazOByGVKRwJ%3Ascholar.google.com%2F&start=20,12.0,"https://scholar.google.com/scholar?cluster=2029317076204563641&hl=en&num=20&as_sdt=0,5",2029317076204563641,https://serpapi.com/search.json?as_sdt=0%2C5&cluster=2029317076204563641&engine=google_scholar&hl=en&num=20,,,,,,,
14,Using Localization and Factorization to Reduce the Complexity of Reinforcement Learning,AypFbYIoFzgJ,https://link.springer.com/chapter/10.1007/978-3-319-21365-1_19,"… Sometimes AIXI refers to the case of a certain universal class and a Solomonoff style prior [2]. The above agent, and only agents of that form, satisfies the strict rationality axioms …","P Sunehag, M Hutter - … : 8th International Conference, AGI 2015, AGI 2015 …, 2015 - Springer",P Sunehag,https://scholar.google.com/citations?user=7QcT04EAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=7QcT04EAAAAJ&engine=google_scholar_author&hl=en,7QcT04EAAAAJ,M Hutter,https://scholar.google.com/citations?user=7hmCntEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=7hmCntEAAAAJ&engine=google_scholar_author&hl=en,7hmCntEAAAAJ,,,,,,,,,agi-conf.org,PDF,https://agi-conf.org/2015/wp-content/uploads/2015/07/agi15_sunehag.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=AypFbYIoFzgJ,1.0,"https://scholar.google.com/scholar?cites=4041743731232418307&as_sdt=2005&sciodt=0,5&hl=en&num=20",4041743731232418307,https://serpapi.com/search.json?as_sdt=2005&cites=4041743731232418307&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:AypFbYIoFzgJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3AAypFbYIoFzgJ%3Ascholar.google.com%2F&start=20,9.0,"https://scholar.google.com/scholar?cluster=4041743731232418307&hl=en&num=20&as_sdt=0,5",4041743731232418307,https://serpapi.com/search.json?as_sdt=0%2C5&cluster=4041743731232418307&engine=google_scholar&hl=en&num=20,,,,,,,
15,An Introduction to Universal Artificial Intelligence,F7UM9JYTJTQJ,https://books.google.com/books?hl=en&lr=&id=jqQIEQAAQBAJ&oi=fnd&pg=PP1&dq=%22reinforcement+learning%22+AND+%22aixi%22&ots=lo0UM7H8y3&sig=G9gyfQarzsI_-FFm9s5fjVouCjw,"… construct AIXI, an optimal reinforcement learning agent that learns to act optimally in unknown environments. AIXI … These algorithms are used to approximate AIXI. The book ends with a …","M Hutter, D Quarel, E Catt - 2024 - books.google.com",M Hutter,https://scholar.google.com/citations?user=7hmCntEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=7hmCntEAAAAJ&engine=google_scholar_author&hl=en,7hmCntEAAAAJ,E Catt,https://scholar.google.com/citations?user=d1JYeMIAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=d1JYeMIAAAAJ&engine=google_scholar_author&hl=en,d1JYeMIAAAAJ,,,,,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=F7UM9JYTJTQJ,1.0,"https://scholar.google.com/scholar?cites=3757431003171042583&as_sdt=2005&sciodt=0,5&hl=en&num=20",3757431003171042583,https://serpapi.com/search.json?as_sdt=2005&cites=3757431003171042583&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:F7UM9JYTJTQJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3AF7UM9JYTJTQJ%3Ascholar.google.com%2F&start=20,,,,,,,,,,Book,
16,Texplore: real-time sample-efficient reinforcement learning for robots,k_V5_I1bhvIJ,https://link.springer.com/article/10.1007/s10994-012-5322-7,… The use of robots in society could be expanded by using reinforcement learning (RL) to allow robots to learn and adapt to new situations online. RL is a paradigm for learning sequential …,"T Hester, P Stone - Machine learning, 2013 - Springer",T Hester,https://scholar.google.com/citations?user=kGFh1QIAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=kGFh1QIAAAAJ&engine=google_scholar_author&hl=en,kGFh1QIAAAAJ,P Stone,https://scholar.google.com/citations?user=qnwjcfAAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=qnwjcfAAAAAJ&engine=google_scholar_author&hl=en,qnwjcfAAAAAJ,,,,,,,,,springer.com,PDF,https://link.springer.com/content/pdf/10.1007/s10994-012-5322-7.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=k_V5_I1bhvIJ,178.0,"https://scholar.google.com/scholar?cites=17475756069442155923&as_sdt=2005&sciodt=0,5&hl=en&num=20",17475756069442155923,https://serpapi.com/search.json?as_sdt=2005&cites=17475756069442155923&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:k_V5_I1bhvIJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3Ak_V5_I1bhvIJ%3Ascholar.google.com%2F&start=20,19.0,"https://scholar.google.com/scholar?cluster=17475756069442155923&hl=en&num=20&as_sdt=0,5",17475756069442155923,https://serpapi.com/search.json?as_sdt=0%2C5&cluster=17475756069442155923&engine=google_scholar&hl=en&num=20,,,,,,,
17,Generic Reinforcement Learning Beyond Small MDPs,2AnKjAPPe0AJ,,,M Daswani - 2015 - The Australian National University,M Daswani,https://scholar.google.com/citations?user=GU-CgkEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=GU-CgkEAAAAJ&engine=google_scholar_author&hl=en,GU-CgkEAAAAJ,,,,,,,,,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=2AnKjAPPe0AJ,2.0,"https://scholar.google.com/scholar?cites=4646535054716701144&as_sdt=2005&sciodt=0,5&hl=en&num=20",4646535054716701144,https://serpapi.com/search.json?as_sdt=2005&cites=4646535054716701144&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:2AnKjAPPe0AJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3A2AnKjAPPe0AJ%3Ascholar.google.com%2F&start=20,2.0,"https://scholar.google.com/scholar?cluster=4646535054716701144&hl=en&num=20&as_sdt=0,5",4646535054716701144,https://serpapi.com/search.json?as_sdt=0%2C5&cluster=4646535054716701144&engine=google_scholar&hl=en&num=20,,,,,,Citation,
18,"Fast, Scalable Algorithms for Reinforcement Learning in High Dimensional Domains",LmHkpXbnJGQJ,https://search.proquest.com/openview/c61b8387693a8bcc1daaff5814313c05/1.pdf?pq-origsite=gscholar&cbl=18750,"… reinforcement learning problem allows us to emphasize the need for more general (non-Markov) agents. In this section I focus on the AIXI … For a gentle introduction to the full AIXI setting, …",M Gendron-Bellemare - 2013 - search.proquest.com,M Gendron-Bellemare,https://scholar.google.com/citations?user=uyYPun0AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=uyYPun0AAAAJ&engine=google_scholar_author&hl=en,uyYPun0AAAAJ,,,,,,,,,,,,,ualberta.ca,PDF,https://era.library.ualberta.ca/items/ad1d3563-eb66-4695-aeda-ebae2e63cf28/download/4cddcd4a-fd11-40d4-8f58-ccabfab1a924,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=LmHkpXbnJGQJ,4.0,"https://scholar.google.com/scholar?cites=7216146999729742126&as_sdt=2005&sciodt=0,5&hl=en&num=20",7216146999729742126,https://serpapi.com/search.json?as_sdt=2005&cites=7216146999729742126&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:LmHkpXbnJGQJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3ALmHkpXbnJGQJ%3Ascholar.google.com%2F&start=20,7.0,"https://scholar.google.com/scholar?cluster=7216146999729742126&hl=en&num=20&as_sdt=0,5",7216146999729742126,https://serpapi.com/search.json?as_sdt=0%2C5&cluster=7216146999729742126&engine=google_scholar&hl=en&num=20,,,,,,Book,
19,Universal RL: Applications and approximations,rEnTD6HSVmwJ,https://ewrl.wordpress.com/wp-content/uploads/2012/10/ewrl11_submission_27.pdf,"… data compression setting to reinforcement learning. The … scaling model-based reinforcement learning to larger obser… Furthermore, optimistic variants of the AIXI optimality notion …",J Veness - 2011 - ewrl.wordpress.com,J Veness,https://scholar.google.com/citations?user=_iYrAxEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=_iYrAxEAAAAJ&engine=google_scholar_author&hl=en,_iYrAxEAAAAJ,,,,,,,,,,,,,wordpress.com,PDF,https://ewrl.wordpress.com/wp-content/uploads/2012/10/ewrl11_submission_27.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=rEnTD6HSVmwJ,1.0,"https://scholar.google.com/scholar?cites=7806658593290406316&as_sdt=2005&sciodt=0,5&hl=en&num=20",7806658593290406316,https://serpapi.com/search.json?as_sdt=2005&cites=7806658593290406316&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:rEnTD6HSVmwJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,5",https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&num=20&q=related%3ArEnTD6HSVmwJ%3Ascholar.google.com%2F&start=20,,,,,"https://scholar.googleusercontent.com/scholar?q=cache:rEnTD6HSVmwJ:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,5",,,,,Pdf,
0,Virtuously safe reinforcement learning,TSEXqoe1zRQJ,https://arxiv.org/abs/1805.11447,"… However, the universal reinforcement learning agent AIXI … to achieve virtuous safe reinforcement learning in seven steps. In … towards virtuous safe reinforcement learning by …","H Aslund, EME Mhamdi, R Guerraoui… - arXiv preprint arXiv …, 2018 - arxiv.org",EME Mhamdi,https://scholar.google.com/citations?user=kNA-WLQAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=kNA-WLQAAAAJ&engine=google_scholar_author&hl=en,kNA-WLQAAAAJ,R Guerraoui,https://scholar.google.com/citations?user=_TR-7CEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=_TR-7CEAAAAJ&engine=google_scholar_author&hl=en,_TR-7CEAAAAJ,,,,,,,,,arxiv.org,PDF,https://arxiv.org/pdf/1805.11447,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=TSEXqoe1zRQJ,6.0,"https://scholar.google.com/scholar?cites=1499053845263098189&as_sdt=20005&sciodt=0,9&hl=en&num=20",1499053845263098189,https://serpapi.com/search.json?as_sdt=20005&cites=1499053845263098189&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:TSEXqoe1zRQJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,9",https://serpapi.com/search.json?as_sdt=0%2C9&engine=google_scholar&hl=en&num=20&q=related%3ATSEXqoe1zRQJ%3Ascholar.google.com%2F&start=40,2.0,"https://scholar.google.com/scholar?cluster=1499053845263098189&hl=en&num=20&as_sdt=0,9",1499053845263098189,https://serpapi.com/search.json?as_sdt=0%2C9&cluster=1499053845263098189&engine=google_scholar&hl=en&num=20,"https://scholar.googleusercontent.com/scholar?q=cache:TSEXqoe1zRQJ:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,9",,,,,,
1,Bad universal priors and notions of optimality,01wih_mhK60J,https://proceedings.mlr.press/v40/Leike15.html,"… Does AIXI succeed in every partially observable Markov decision process (POMDP)/(ergodic… be no invariance theorem for AIXI. As a reinforcement learning agent, AIXI has to balance …","J Leike, M Hutter - Conference on Learning Theory, 2015 - proceedings.mlr.press",J Leike,https://scholar.google.com/citations?user=beiWcokAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=beiWcokAAAAJ&engine=google_scholar_author&hl=en,beiWcokAAAAJ,M Hutter,https://scholar.google.com/citations?user=7hmCntEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=7hmCntEAAAAJ&engine=google_scholar_author&hl=en,7hmCntEAAAAJ,,,,,,,,,mlr.press,PDF,http://proceedings.mlr.press/v40/Leike15.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=01wih_mhK60J,46.0,"https://scholar.google.com/scholar?cites=12478245285646195923&as_sdt=20005&sciodt=0,9&hl=en&num=20",12478245285646195923,https://serpapi.com/search.json?as_sdt=20005&cites=12478245285646195923&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:01wih_mhK60J:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,9",https://serpapi.com/search.json?as_sdt=0%2C9&engine=google_scholar&hl=en&num=20&q=related%3A01wih_mhK60J%3Ascholar.google.com%2F&start=40,9.0,"https://scholar.google.com/scholar?cluster=12478245285646195923&hl=en&num=20&as_sdt=0,9",12478245285646195923,https://serpapi.com/search.json?as_sdt=0%2C9&cluster=12478245285646195923&engine=google_scholar&hl=en&num=20,"http://scholar.googleusercontent.com/scholar?q=cache:01wih_mhK60J:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,9",,,,,,
2,Communication-efficient and collision-free motion planning of underwater vehicles via integral reinforcement learning,tGpS00gpEicJ,https://ieeexplore.ieee.org/abstract/document/9983989/,"… We first develop a model-based integral reinforcement learning (IRL) estimator to predict the stochastic signal-to-noise ratio (SNR). With the estimated SNR, an integrated optimization …","J Yan, W Cao, X Yang, C Chen… - IEEE Transactions on …, 2022 - ieeexplore.ieee.org",J Yan,https://scholar.google.com/citations?user=HtjDQ1AAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=HtjDQ1AAAAAJ&engine=google_scholar_author&hl=en,HtjDQ1AAAAAJ,X Yang,https://scholar.google.com/citations?user=ISADH70AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=ISADH70AAAAJ&engine=google_scholar_author&hl=en,ISADH70AAAAJ,C Chen,https://scholar.google.com/citations?user=NGQrfUwAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=NGQrfUwAAAAJ&engine=google_scholar_author&hl=en,NGQrfUwAAAAJ,,,,,ieee.org,PDF,https://ieeexplore.ieee.org/iel7/5962385/6104215/09983989.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=tGpS00gpEicJ,10.0,"https://scholar.google.com/scholar?cites=2815358109819759284&as_sdt=20005&sciodt=0,9&hl=en&num=20",2815358109819759284,https://serpapi.com/search.json?as_sdt=20005&cites=2815358109819759284&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:tGpS00gpEicJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,9",https://serpapi.com/search.json?as_sdt=0%2C9&engine=google_scholar&hl=en&num=20&q=related%3AtGpS00gpEicJ%3Ascholar.google.com%2F&start=40,3.0,"https://scholar.google.com/scholar?cluster=2815358109819759284&hl=en&num=20&as_sdt=0,9",2815358109819759284,https://serpapi.com/search.json?as_sdt=0%2C9&cluster=2815358109819759284&engine=google_scholar&hl=en&num=20,,,,,,,
3,Reinforcement learning with limited prior knowledge in long-term environments,jeIACDbijtkJ,https://drive.google.com/file/d/1wIPPk9huo_QsQb_pYpVHBA3Tko0R1i8M/view,"… Two typical algorithms are mentioned in Goertzel’s classification, both of which are reinforcement learners: the AIXI [81] which maximises future reward with a simplicity bias which …",D Bossens - 2020 - drive.google.com,D Bossens,https://scholar.google.com/citations?user=w2feGIoAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=w2feGIoAAAAJ&engine=google_scholar_author&hl=en,w2feGIoAAAAJ,,,,,,,,,,,,,google.com,PDF,https://drive.google.com/file/d/1wIPPk9huo_QsQb_pYpVHBA3Tko0R1i8M/view,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=jeIACDbijtkJ,,,,,"https://scholar.google.com/scholar?q=related:jeIACDbijtkJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,9",https://serpapi.com/search.json?as_sdt=0%2C9&engine=google_scholar&hl=en&num=20&q=related%3AjeIACDbijtkJ%3Ascholar.google.com%2F&start=40,3.0,"https://scholar.google.com/scholar?cluster=15676716074613662349&hl=en&num=20&as_sdt=0,9",15676716074613662349,https://serpapi.com/search.json?as_sdt=0%2C9&cluster=15676716074613662349&engine=google_scholar&hl=en&num=20,,,,,,Pdf,
4,Safely interruptible agents,-u8Vo2hQfQQJ,https://ora.ox.ac.uk/objects/uuid:17c0e095-4e13-47fc-bace-64ec46134a3f,"… Reinforcement learning agents interacting with a complex … We show that even ideal, uncomputable reinforcement learning … As a consequence, AIXI is not a good candidate for …","L Orseau, M Armstrong - Conference on Uncertainty in Artificial …, 2016 - ora.ox.ac.uk",L Orseau,https://scholar.google.com/citations?user=HVJjWlEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=HVJjWlEAAAAJ&engine=google_scholar_author&hl=en,HVJjWlEAAAAJ,M Armstrong,https://scholar.google.com/citations?user=bSJaYzgAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=bSJaYzgAAAAJ&engine=google_scholar_author&hl=en,bSJaYzgAAAAJ,,,,,,,,,ox.ac.uk,PDF,https://ora.ox.ac.uk/objects/uuid:17c0e095-4e13-47fc-bace-64ec46134a3f/download_file?file_format=pdf&safe_filename=Armstrong%2Band%2BOrseau%2C%2BSafely%2BInterruptible%2BAgents.pdf&type_of_work=Conference+item,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=-u8Vo2hQfQQJ,144.0,"https://scholar.google.com/scholar?cites=323503158583488506&as_sdt=20005&sciodt=0,9&hl=en&num=20",323503158583488506,https://serpapi.com/search.json?as_sdt=20005&cites=323503158583488506&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:-u8Vo2hQfQQJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,9",https://serpapi.com/search.json?as_sdt=0%2C9&engine=google_scholar&hl=en&num=20&q=related%3A-u8Vo2hQfQQJ%3Ascholar.google.com%2F&start=40,19.0,"https://scholar.google.com/scholar?cluster=323503158583488506&hl=en&num=20&as_sdt=0,9",323503158583488506,https://serpapi.com/search.json?as_sdt=0%2C9&cluster=323503158583488506&engine=google_scholar&hl=en&num=20,"https://scholar.googleusercontent.com/scholar?q=cache:-u8Vo2hQfQQJ:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,9",,,,,,
5,Bayesian reinforcement learning,18PTp0x1v2cJ,https://link.springer.com/chapter/10.1007/978-3-642-27645-3_11,"… This chapter surveys recent lines of work that use Bayesian techniques for reinforcement learning. In Bayesian learning, uncertainty is expressed by a prior distribution over unknown …","N Vlassis, M Ghavamzadeh, S Mannor… - Reinforcement Learning …, 2012 - Springer",N Vlassis,https://scholar.google.com/citations?user=JJWWPjsAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=JJWWPjsAAAAJ&engine=google_scholar_author&hl=en,JJWWPjsAAAAJ,M Ghavamzadeh,https://scholar.google.com/citations?user=Bo-wyrkAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=Bo-wyrkAAAAJ&engine=google_scholar_author&hl=en,Bo-wyrkAAAAJ,S Mannor,https://scholar.google.com/citations?user=q1HlbIUAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=q1HlbIUAAAAJ&engine=google_scholar_author&hl=en,q1HlbIUAAAAJ,,,,,uni.lu,PDF,https://orbilu.uni.lu/bitstream/10993/3390/1/BRLchapter.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=18PTp0x1v2cJ,101.0,"https://scholar.google.com/scholar?cites=7475822878551950295&as_sdt=20005&sciodt=0,9&hl=en&num=20",7475822878551950295,https://serpapi.com/search.json?as_sdt=20005&cites=7475822878551950295&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:18PTp0x1v2cJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,9",https://serpapi.com/search.json?as_sdt=0%2C9&engine=google_scholar&hl=en&num=20&q=related%3A18PTp0x1v2cJ%3Ascholar.google.com%2F&start=40,14.0,"https://scholar.google.com/scholar?cluster=7475822878551950295&hl=en&num=20&as_sdt=0,9",7475822878551950295,https://serpapi.com/search.json?as_sdt=0%2C9&cluster=7475822878551950295&engine=google_scholar&hl=en&num=20,,,,,,,
6,Optimal robust output containment of unknown heterogeneous multiagent system using off-policy reinforcement learning,iYNSRU0xfUwJ,https://ieeexplore.ieee.org/abstract/document/8089381/,"… Then, using this Bellman equation, a model-free off-policy integral reinforcement learning algorithm is proposed to solve the optimal robust output containment problem of …","S Zuo, Y Song, FL Lewis… - IEEE Transactions on …, 2017 - ieeexplore.ieee.org",S Zuo,https://scholar.google.com/citations?user=_tCX5K4AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=_tCX5K4AAAAJ&engine=google_scholar_author&hl=en,_tCX5K4AAAAJ,FL Lewis,https://scholar.google.com/citations?user=rMRit3UAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=rMRit3UAAAAJ&engine=google_scholar_author&hl=en,rMRit3UAAAAJ,,,,,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=iYNSRU0xfUwJ,66.0,"https://scholar.google.com/scholar?cites=5511615726916633481&as_sdt=20005&sciodt=0,9&hl=en&num=20",5511615726916633481,https://serpapi.com/search.json?as_sdt=20005&cites=5511615726916633481&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:iYNSRU0xfUwJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,9",https://serpapi.com/search.json?as_sdt=0%2C9&engine=google_scholar&hl=en&num=20&q=related%3AiYNSRU0xfUwJ%3Ascholar.google.com%2F&start=40,3.0,"https://scholar.google.com/scholar?cluster=5511615726916633481&hl=en&num=20&as_sdt=0,9",5511615726916633481,https://serpapi.com/search.json?as_sdt=0%2C9&cluster=5511615726916633481&engine=google_scholar&hl=en&num=20,,,,,,,
7,Universal knowledge-seeking agents,Qxi3y2oIF1kJ,https://www.sciencedirect.com/science/article/pii/S0304397513007160,"… However, it was recently proved that AIXI can in certain situations stop exploring, leading to … why reinforcement learning is intrinsically difficult; Furthermore the Pareto optimality of AIXI …","L Orseau - Theoretical Computer Science, 2014 - Elsevier",L Orseau,https://scholar.google.com/citations?user=HVJjWlEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=HVJjWlEAAAAJ&engine=google_scholar_author&hl=en,HVJjWlEAAAAJ,,,,,,,,,,,,,sciencedirect.com,HTML,https://www.sciencedirect.com/science/article/pii/S0304397513007160,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=Qxi3y2oIF1kJ,30.0,"https://scholar.google.com/scholar?cites=6419609048617261123&as_sdt=20005&sciodt=0,9&hl=en&num=20",6419609048617261123,https://serpapi.com/search.json?as_sdt=20005&cites=6419609048617261123&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:Qxi3y2oIF1kJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,9",https://serpapi.com/search.json?as_sdt=0%2C9&engine=google_scholar&hl=en&num=20&q=related%3AQxi3y2oIF1kJ%3Ascholar.google.com%2F&start=40,14.0,"https://scholar.google.com/scholar?cluster=6419609048617261123&hl=en&num=20&as_sdt=0,9",6419609048617261123,https://serpapi.com/search.json?as_sdt=0%2C9&cluster=6419609048617261123&engine=google_scholar&hl=en&num=20,,,,,,Html,https://www.sciencedirect.com/science/article/pii/S0304397513007160
8,Path planning for vehicle platoons under routing decisions: a distributed approach combining deep reinforcement learning and model predictive control,31LaERpC6A0J,https://ieeexplore.ieee.org/abstract/document/9803896/,"… Deep Reinforcement Learning is a reinforcement learning whose Q-function is defined as a neural network (see [16]). In a distributed framework, Deep Reinforcement Learning is able …","F Giannini, G Fortino, G Franzè… - 2022 8th International …, 2022 - ieeexplore.ieee.org",F Giannini,https://scholar.google.com/citations?user=xV6XPOQAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=xV6XPOQAAAAJ&engine=google_scholar_author&hl=en,xV6XPOQAAAAJ,G Fortino,https://scholar.google.com/citations?user=ecImn1MAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=ecImn1MAAAAJ&engine=google_scholar_author&hl=en,ecImn1MAAAAJ,G Franzè,https://scholar.google.com/citations?user=j4zeuuoAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=j4zeuuoAAAAJ&engine=google_scholar_author&hl=en,j4zeuuoAAAAJ,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=31LaERpC6A0J,7.0,"https://scholar.google.com/scholar?cites=1002123596826039007&as_sdt=20005&sciodt=0,9&hl=en&num=20",1002123596826039007,https://serpapi.com/search.json?as_sdt=20005&cites=1002123596826039007&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:31LaERpC6A0J:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,9",https://serpapi.com/search.json?as_sdt=0%2C9&engine=google_scholar&hl=en&num=20&q=related%3A31LaERpC6A0J%3Ascholar.google.com%2F&start=40,2.0,"https://scholar.google.com/scholar?cluster=1002123596826039007&hl=en&num=20&as_sdt=0,9",1002123596826039007,https://serpapi.com/search.json?as_sdt=0%2C9&cluster=1002123596826039007&engine=google_scholar&hl=en&num=20,,,,,,,
9,Optimality issues of universal greedy agents with static priors,W_tREDwj-7oJ,https://link.springer.com/chapter/10.1007/978-3-642-16108-7_28,"… Hutter defined AIXI which extends the latter to the Reinforcement Learning framework, where almost all if not all AI problems can be formulated. However, new difficulties arise, because …","L Orseau - … Learning Theory: 21st International Conference, ALT …, 2010 - Springer",L Orseau,https://scholar.google.com/citations?user=HVJjWlEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=HVJjWlEAAAAJ&engine=google_scholar_author&hl=en,HVJjWlEAAAAJ,,,,,,,,,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=W_tREDwj-7oJ,25.0,"https://scholar.google.com/scholar?cites=13473401451087788891&as_sdt=20005&sciodt=0,9&hl=en&num=20",13473401451087788891,https://serpapi.com/search.json?as_sdt=20005&cites=13473401451087788891&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:W_tREDwj-7oJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,9",https://serpapi.com/search.json?as_sdt=0%2C9&engine=google_scholar&hl=en&num=20&q=related%3AW_tREDwj-7oJ%3Ascholar.google.com%2F&start=40,9.0,"https://scholar.google.com/scholar?cluster=13473401451087788891&hl=en&num=20&as_sdt=0,9",13473401451087788891,https://serpapi.com/search.json?as_sdt=0%2C9&cluster=13473401451087788891&engine=google_scholar&hl=en&num=20,,,,,,,
10,A deep reinforcement learning framework for column generation,w3trbd3uKF4J,https://proceedings.neurips.cc/paper_files/paper/2022/hash/3ecfe5c632afb7d96a2337b18ff99b1f-Abstract-Conference.html,"… We propose RLCG, the first Reinforcement Learning (RL) approach for CG. Unlike typical column selection rules which myopically select a column based on local information at each …","C Chi, A Aboussalah, E Khalil, J Wang… - Advances in …, 2022 - proceedings.neurips.cc",C Chi,https://scholar.google.com/citations?user=-BZtJogAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=-BZtJogAAAAJ&engine=google_scholar_author&hl=en,-BZtJogAAAAJ,A Aboussalah,https://scholar.google.com/citations?user=c2QKp_MAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=c2QKp_MAAAAJ&engine=google_scholar_author&hl=en,c2QKp_MAAAAJ,E Khalil,https://scholar.google.com/citations?user=juqDWQMAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=juqDWQMAAAAJ&engine=google_scholar_author&hl=en,juqDWQMAAAAJ,J Wang,https://scholar.google.com/citations?user=dg3QSDcAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=dg3QSDcAAAAJ&engine=google_scholar_author&hl=en,dg3QSDcAAAAJ,neurips.cc,PDF,https://proceedings.neurips.cc/paper_files/paper/2022/file/3ecfe5c632afb7d96a2337b18ff99b1f-Paper-Conference.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=w3trbd3uKF4J,21.0,"https://scholar.google.com/scholar?cites=6784935473424595907&as_sdt=20005&sciodt=0,9&hl=en&num=20",6784935473424595907,https://serpapi.com/search.json?as_sdt=20005&cites=6784935473424595907&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:w3trbd3uKF4J:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,9",https://serpapi.com/search.json?as_sdt=0%2C9&engine=google_scholar&hl=en&num=20&q=related%3Aw3trbd3uKF4J%3Ascholar.google.com%2F&start=40,5.0,"https://scholar.google.com/scholar?cluster=6784935473424595907&hl=en&num=20&as_sdt=0,9",6784935473424595907,https://serpapi.com/search.json?as_sdt=0%2C9&cluster=6784935473424595907&engine=google_scholar&hl=en&num=20,"https://scholar.googleusercontent.com/scholar?q=cache:w3trbd3uKF4J:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,9",,,,,,
11,Observer-based consensus control for MASs with prescribed constraints via reinforcement learning algorithm,6VgQlrs4n6YJ,https://ieeexplore.ieee.org/abstract/document/10225438/,"… In addition, the updating laws of actor-critic NNs are established by using a simplified reinforcement learning (RL) algorithm based on the uniqueness of optimal solution, and the …","A Luo, Q Zhou, H Ma, H Li - IEEE Transactions on Neural …, 2023 - ieeexplore.ieee.org",,,,,,,,,,,,,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=6VgQlrs4n6YJ,10.0,"https://scholar.google.com/scholar?cites=12006377509920725225&as_sdt=20005&sciodt=0,9&hl=en&num=20",12006377509920725225,https://serpapi.com/search.json?as_sdt=20005&cites=12006377509920725225&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:6VgQlrs4n6YJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,9",https://serpapi.com/search.json?as_sdt=0%2C9&engine=google_scholar&hl=en&num=20&q=related%3A6VgQlrs4n6YJ%3Ascholar.google.com%2F&start=40,3.0,"https://scholar.google.com/scholar?cluster=12006377509920725225&hl=en&num=20&as_sdt=0,9",12006377509920725225,https://serpapi.com/search.json?as_sdt=0%2C9&cluster=12006377509920725225&engine=google_scholar&hl=en&num=20,,,,,,,
12,Self-correcting models for model-based reinforcement learning,hggom8DB49oJ,https://ojs.aaai.org/index.php/AAAI/article/view/10850,… In model-based reinforcement learning (MBRL) the agent learns a predictive model of its environment and uses it to make decisions. The overall MBRL approach is intuitively appealing …,"E Talvitie - Proceedings of the AAAI conference on artificial …, 2017 - ojs.aaai.org",E Talvitie,https://scholar.google.com/citations?user=mAvu7aYAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=mAvu7aYAAAAJ&engine=google_scholar_author&hl=en,mAvu7aYAAAAJ,,,,,,,,,,,,,aaai.org,PDF,https://ojs.aaai.org/index.php/AAAI/article/view/10850/10709,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=hggom8DB49oJ,90.0,"https://scholar.google.com/scholar?cites=15772663352962582662&as_sdt=20005&sciodt=0,9&hl=en&num=20",15772663352962582662,https://serpapi.com/search.json?as_sdt=20005&cites=15772663352962582662&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:hggom8DB49oJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,9",https://serpapi.com/search.json?as_sdt=0%2C9&engine=google_scholar&hl=en&num=20&q=related%3Ahggom8DB49oJ%3Ascholar.google.com%2F&start=40,7.0,"https://scholar.google.com/scholar?cluster=15772663352962582662&hl=en&num=20&as_sdt=0,9",15772663352962582662,https://serpapi.com/search.json?as_sdt=0%2C9&cluster=15772663352962582662&engine=google_scholar&hl=en&num=20,"https://scholar.googleusercontent.com/scholar?q=cache:hggom8DB49oJ:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,9",,,,,,
13,Deep reinforcement learning aided platoon control relying on V2X information,VWX58wP_MSsJ,https://ieeexplore.ieee.org/abstract/document/9743615/,"… Platoon control is essentially a sequential stochastic decision problem (SSDP), which can be solved by Deep Reinforcement Learning (DRL) to deal with both the control constraints …","L Lei, T Liu, K Zheng, L Hanzo - IEEE Transactions on …, 2022 - ieeexplore.ieee.org",L Lei,https://scholar.google.com/citations?user=Ut0-14IAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=Ut0-14IAAAAJ&engine=google_scholar_author&hl=en,Ut0-14IAAAAJ,K Zheng,https://scholar.google.com/citations?user=IX_id5UAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=IX_id5UAAAAJ&engine=google_scholar_author&hl=en,IX_id5UAAAAJ,L Hanzo,https://scholar.google.com/citations?user=p0jnEW0AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=p0jnEW0AAAAJ&engine=google_scholar_author&hl=en,p0jnEW0AAAAJ,,,,,arxiv.org,PDF,https://arxiv.org/pdf/2203.15781,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=VWX58wP_MSsJ,28.0,"https://scholar.google.com/scholar?cites=3112549209932916053&as_sdt=20005&sciodt=0,9&hl=en&num=20",3112549209932916053,https://serpapi.com/search.json?as_sdt=20005&cites=3112549209932916053&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:VWX58wP_MSsJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,9",https://serpapi.com/search.json?as_sdt=0%2C9&engine=google_scholar&hl=en&num=20&q=related%3AVWX58wP_MSsJ%3Ascholar.google.com%2F&start=40,5.0,"https://scholar.google.com/scholar?cluster=3112549209932916053&hl=en&num=20&as_sdt=0,9",3112549209932916053,https://serpapi.com/search.json?as_sdt=0%2C9&cluster=3112549209932916053&engine=google_scholar&hl=en&num=20,,,,,,,
14,Distributional monte carlo tree search for risk-aware and multi-objective reinforcement learning,5w8JGfAL4rgJ,https://www.ifaamas.org/Proceedings/aamas2021/pdfs/p1530.pdf,"… and multi-objective reinforcement learning settings, the utility … return – known in reinforcement learning as the value – cannot … reinforcement learning for the expected utility of the returns. …","CF Hayes, M Reymond, DM Roijers, E Howley… - Proceedings of the 20th …, 2021 - ifaamas.org",CF Hayes,https://scholar.google.com/citations?user=p4-eiAkAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=p4-eiAkAAAAJ&engine=google_scholar_author&hl=en,p4-eiAkAAAAJ,M Reymond,https://scholar.google.com/citations?user=b9bYrHUAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=b9bYrHUAAAAJ&engine=google_scholar_author&hl=en,b9bYrHUAAAAJ,DM Roijers,https://scholar.google.com/citations?user=oi25V4EAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=oi25V4EAAAAJ&engine=google_scholar_author&hl=en,oi25V4EAAAAJ,E Howley,https://scholar.google.com/citations?user=29vbDa0AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=29vbDa0AAAAJ&engine=google_scholar_author&hl=en,29vbDa0AAAAJ,ifaamas.org,PDF,https://www.ifaamas.org/Proceedings/aamas2021/pdfs/p1530.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=5w8JGfAL4rgJ,23.0,"https://scholar.google.com/scholar?cites=13322223773555429351&as_sdt=20005&sciodt=0,9&hl=en&num=20",13322223773555429351,https://serpapi.com/search.json?as_sdt=20005&cites=13322223773555429351&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:5w8JGfAL4rgJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,9",https://serpapi.com/search.json?as_sdt=0%2C9&engine=google_scholar&hl=en&num=20&q=related%3A5w8JGfAL4rgJ%3Ascholar.google.com%2F&start=40,5.0,"https://scholar.google.com/scholar?cluster=13322223773555429351&hl=en&num=20&as_sdt=0,9",13322223773555429351,https://serpapi.com/search.json?as_sdt=0%2C9&cluster=13322223773555429351&engine=google_scholar&hl=en&num=20,"https://scholar.googleusercontent.com/scholar?q=cache:5w8JGfAL4rgJ:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,9",,,,,Pdf,
15,Event-triggered distributed control of nonlinear interconnected systems using online reinforcement learning with exploration,dIzKaYwe6hoJ,https://ieeexplore.ieee.org/abstract/document/8027115/,"… His current research interests include nonlinear control, neural network control, and applications of reinforcement learning to feedback control of uncertain nonlinear systems. …","V Narayanan, S Jagannathan - IEEE transactions on …, 2017 - ieeexplore.ieee.org",V Narayanan,https://scholar.google.com/citations?user=rirGB10AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=rirGB10AAAAJ&engine=google_scholar_author&hl=en,rirGB10AAAAJ,S Jagannathan,https://scholar.google.com/citations?user=RTewL_wAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=RTewL_wAAAAJ&engine=google_scholar_author&hl=en,RTewL_wAAAAJ,,,,,,,,,ieee.org,PDF,https://ieeexplore.ieee.org/ielaam/6221036/8438339/8027115-aam.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=dIzKaYwe6hoJ,86.0,"https://scholar.google.com/scholar?cites=1939396177955556468&as_sdt=20005&sciodt=0,9&hl=en&num=20",1939396177955556468,https://serpapi.com/search.json?as_sdt=20005&cites=1939396177955556468&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:dIzKaYwe6hoJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,9",https://serpapi.com/search.json?as_sdt=0%2C9&engine=google_scholar&hl=en&num=20&q=related%3AdIzKaYwe6hoJ%3Ascholar.google.com%2F&start=40,5.0,"https://scholar.google.com/scholar?cluster=1939396177955556468&hl=en&num=20&as_sdt=0,9",1939396177955556468,https://serpapi.com/search.json?as_sdt=0%2C9&cluster=1939396177955556468&engine=google_scholar&hl=en&num=20,,,,,,,
16,Distributed reinforcement learning for cyber-physical system with multiple remote state estimation under DoS attacker,z7GbU_huiy8J,https://ieeexplore.ieee.org/abstract/document/9174773/,"… the reinforcement learning has great application prospects in the problem of network attack with unknown environments, the distributed reinforcement learning in … reinforcement learning …","P Dai, W Yu, H Wang, G Wen… - IEEE Transactions on …, 2020 - ieeexplore.ieee.org",W Yu,https://scholar.google.com/citations?user=I7XxngUAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=I7XxngUAAAAJ&engine=google_scholar_author&hl=en,I7XxngUAAAAJ,G Wen,https://scholar.google.com/citations?user=Hyh2yUIAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=Hyh2yUIAAAAJ&engine=google_scholar_author&hl=en,Hyh2yUIAAAAJ,,,,,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=z7GbU_huiy8J,35.0,"https://scholar.google.com/scholar?cites=3425953954379051471&as_sdt=20005&sciodt=0,9&hl=en&num=20",3425953954379051471,https://serpapi.com/search.json?as_sdt=20005&cites=3425953954379051471&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:z7GbU_huiy8J:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,9",https://serpapi.com/search.json?as_sdt=0%2C9&engine=google_scholar&hl=en&num=20&q=related%3Az7GbU_huiy8J%3Ascholar.google.com%2F&start=40,3.0,"https://scholar.google.com/scholar?cluster=3425953954379051471&hl=en&num=20&as_sdt=0,9",3425953954379051471,https://serpapi.com/search.json?as_sdt=0%2C9&cluster=3425953954379051471&engine=google_scholar&hl=en&num=20,,,,,,,
17,On ensemble techniques for AIXI approximation,9LETn26PAmkJ,https://link.springer.com/chapter/10.1007/978-3-642-35506-6_35,"… ensemble techniques for universal reinforcement learning agents. Each technique works by … an important role in future AIXI approximations. For example, the MC-AIXI agent could be …","J Veness, P Sunehag, M Hutter - International Conference on Artificial …, 2012 - Springer",J Veness,https://scholar.google.com/citations?user=_iYrAxEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=_iYrAxEAAAAJ&engine=google_scholar_author&hl=en,_iYrAxEAAAAJ,P Sunehag,https://scholar.google.com/citations?user=7QcT04EAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=7QcT04EAAAAJ&engine=google_scholar_author&hl=en,7QcT04EAAAAJ,M Hutter,https://scholar.google.com/citations?user=7hmCntEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=7hmCntEAAAAJ&engine=google_scholar_author&hl=en,7hmCntEAAAAJ,,,,,agi-conf.org,PDF,https://www.agi-conf.org/2012/wp-content/uploads/2012/12/paper_23.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=9LETn26PAmkJ,9.0,"https://scholar.google.com/scholar?cites=7566768029213897204&as_sdt=20005&sciodt=0,9&hl=en&num=20",7566768029213897204,https://serpapi.com/search.json?as_sdt=20005&cites=7566768029213897204&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:9LETn26PAmkJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,9",https://serpapi.com/search.json?as_sdt=0%2C9&engine=google_scholar&hl=en&num=20&q=related%3A9LETn26PAmkJ%3Ascholar.google.com%2F&start=40,14.0,"https://scholar.google.com/scholar?cluster=7566768029213897204&hl=en&num=20&as_sdt=0,9",7566768029213897204,https://serpapi.com/search.json?as_sdt=0%2C9&cluster=7566768029213897204&engine=google_scholar&hl=en&num=20,,,,,,,
18,Autonomous platoon control with integrated deep reinforcement learning and dynamic programming,OBOFKEZUGeoJ,https://ieeexplore.ieee.org/abstract/document/9951132/,"… As another promising method, Reinforcement Learning (RL) can learn an optimal control … Moreover, the more powerful Deep Reinforcement Learning (DRL) methods can deal with the …","T Liu, L Lei, K Zheng, K Zhang - IEEE Internet of Things Journal, 2022 - ieeexplore.ieee.org",L Lei,https://scholar.google.com/citations?user=Ut0-14IAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=Ut0-14IAAAAJ&engine=google_scholar_author&hl=en,Ut0-14IAAAAJ,K Zheng,https://scholar.google.com/citations?user=IX_id5UAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=IX_id5UAAAAJ&engine=google_scholar_author&hl=en,IX_id5UAAAAJ,K Zhang,https://scholar.google.com/citations?user=hLT3_twAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=hLT3_twAAAAJ&engine=google_scholar_author&hl=en,hLT3_twAAAAJ,,,,,arxiv.org,PDF,https://arxiv.org/pdf/2206.07536,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=OBOFKEZUGeoJ,20.0,"https://scholar.google.com/scholar?cites=16868606539597157176&as_sdt=20005&sciodt=0,9&hl=en&num=20",16868606539597157176,https://serpapi.com/search.json?as_sdt=20005&cites=16868606539597157176&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:OBOFKEZUGeoJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,9",https://serpapi.com/search.json?as_sdt=0%2C9&engine=google_scholar&hl=en&num=20&q=related%3AOBOFKEZUGeoJ%3Ascholar.google.com%2F&start=40,3.0,"https://scholar.google.com/scholar?cluster=16868606539597157176&hl=en&num=20&as_sdt=0,9",16868606539597157176,https://serpapi.com/search.json?as_sdt=0%2C9&cluster=16868606539597157176&engine=google_scholar&hl=en&num=20,,,,,,,
19,Quantum Algorithms for Universal Prediction,A6xwtFlP6qkJ,,,E Catt - 2018 - The Australian National University,E Catt,https://scholar.google.com/citations?user=d1JYeMIAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=d1JYeMIAAAAJ&engine=google_scholar_author&hl=en,d1JYeMIAAAAJ,,,,,,,,,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=A6xwtFlP6qkJ,,,,,"https://scholar.google.com/scholar?q=related:A6xwtFlP6qkJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,9",https://serpapi.com/search.json?as_sdt=0%2C9&engine=google_scholar&hl=en&num=20&q=related%3AA6xwtFlP6qkJ%3Ascholar.google.com%2F&start=40,,,,,,,,,,Citation,
0,On Reward Binarisation and Bayesian Agents,eggW9SeK_dEJ,https://ewrl.wordpress.com/wp-content/uploads/2022/09/ewrl22_submission.pdf,… reinforcement learning agents. Furthermore we show that binary Bayesian reinforcement learning … The Bayesian-optimal agent AIXI considers a Bayesian mixture of the class of possible …,"E Catt, J Veness, M Hutter - … on Reinforcement Learning, 2022 - ewrl.wordpress.com",E Catt,https://scholar.google.com/citations?user=d1JYeMIAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=d1JYeMIAAAAJ&engine=google_scholar_author&hl=en,d1JYeMIAAAAJ,J Veness,https://scholar.google.com/citations?user=_iYrAxEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=_iYrAxEAAAAJ&engine=google_scholar_author&hl=en,_iYrAxEAAAAJ,M Hutter,https://scholar.google.com/citations?user=7hmCntEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=7hmCntEAAAAJ&engine=google_scholar_author&hl=en,7hmCntEAAAAJ,,,,,wordpress.com,PDF,https://ewrl.wordpress.com/wp-content/uploads/2022/09/ewrl22_submission.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=eggW9SeK_dEJ,1.0,"https://scholar.google.com/scholar?cites=15131402227254954106&as_sdt=80000005&sciodt=0,23&hl=en&num=20",15131402227254954106,https://serpapi.com/search.json?as_sdt=80000005&cites=15131402227254954106&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:eggW9SeK_dEJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,23",https://serpapi.com/search.json?as_sdt=0%2C23&engine=google_scholar&hl=en&num=20&q=related%3AeggW9SeK_dEJ%3Ascholar.google.com%2F&start=60,,,,,"https://scholar.googleusercontent.com/scholar?q=cache:eggW9SeK_dEJ:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,23",,,,,Pdf,
1,On monte carlo tree search and reinforcement learning,FKodzdQNklAJ,https://www.jair.org/index.php/jair/article/view/11099,"… Reinforcementlearning experts might regard this first part of our study as an extended background, which might also help them appreciate some of the qualities of MCTS. As our second …","T Vodopivec, S Samothrakis, B Ster - Journal of Artificial Intelligence …, 2017 - jair.org",S Samothrakis,https://scholar.google.com/citations?user=6qhmeekAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=6qhmeekAAAAJ&engine=google_scholar_author&hl=en,6qhmeekAAAAJ,B Ster,https://scholar.google.com/citations?user=yuTy0ZQAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=yuTy0ZQAAAAJ&engine=google_scholar_author&hl=en,yuTy0ZQAAAAJ,,,,,,,,,jair.org,PDF,https://www.jair.org/index.php/jair/article/download/11099/26289,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=FKodzdQNklAJ,92.0,"https://scholar.google.com/scholar?cites=5805718077259491860&as_sdt=80000005&sciodt=0,23&hl=en&num=20",5805718077259491860,https://serpapi.com/search.json?as_sdt=80000005&cites=5805718077259491860&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:FKodzdQNklAJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,23",https://serpapi.com/search.json?as_sdt=0%2C23&engine=google_scholar&hl=en&num=20&q=related%3AFKodzdQNklAJ%3Ascholar.google.com%2F&start=60,8.0,"https://scholar.google.com/scholar?cluster=5805718077259491860&hl=en&num=20&as_sdt=0,23",5805718077259491860,https://serpapi.com/search.json?as_sdt=0%2C23&cluster=5805718077259491860&engine=google_scholar&hl=en&num=20,"https://scholar.googleusercontent.com/scholar?q=cache:FKodzdQNklAJ:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,23",,,,,,
2,Categorizing Wireheading in Partially Embedded Agents,hrYDrX1za7YJ,https://arxiv.org/abs/1906.09136,"… Starting from the fully dualistic universal agent AIXI, we introduce a spectrum of partially … Aligning the goals of a reinforcement learning agent with the goals of its human designers is …","A Majha, S Sarkar, D Zagami - arXiv preprint arXiv:1906.09136, 2019 - arxiv.org",,,,,,,,,,,,,,,,,arxiv.org,PDF,https://arxiv.org/pdf/1906.09136,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=hrYDrX1za7YJ,2.0,"https://scholar.google.com/scholar?cites=13144726921021732486&as_sdt=80000005&sciodt=0,23&hl=en&num=20",13144726921021732486,https://serpapi.com/search.json?as_sdt=80000005&cites=13144726921021732486&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:hrYDrX1za7YJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,23",https://serpapi.com/search.json?as_sdt=0%2C23&engine=google_scholar&hl=en&num=20&q=related%3AhrYDrX1za7YJ%3Ascholar.google.com%2F&start=60,3.0,"https://scholar.google.com/scholar?cluster=13144726921021732486&hl=en&num=20&as_sdt=0,23",13144726921021732486,https://serpapi.com/search.json?as_sdt=0%2C23&cluster=13144726921021732486&engine=google_scholar&hl=en&num=20,"https://scholar.googleusercontent.com/scholar?q=cache:hrYDrX1za7YJ:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,23",,,,,,
3,Bayesian nonparametric methods for partially-observable reinforcement learning,r05fgIUC3IcJ,https://ieeexplore.ieee.org/abstract/document/6616533/,… We divide the knowledge representation for the reinforcementlearning problem into two … work on appropriate choices of knowledge representations in reinforcement learning settings. …,"F Doshi-Velez, D Pfau, F Wood… - IEEE transactions on …, 2013 - ieeexplore.ieee.org",F Doshi-Velez,https://scholar.google.com/citations?user=hwQtFB0AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=hwQtFB0AAAAJ&engine=google_scholar_author&hl=en,hwQtFB0AAAAJ,D Pfau,https://scholar.google.com/citations?user=rgRibJwAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=rgRibJwAAAAJ&engine=google_scholar_author&hl=en,rgRibJwAAAAJ,F Wood,https://scholar.google.com/citations?user=d4yNzXIAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=d4yNzXIAAAAJ&engine=google_scholar_author&hl=en,d4yNzXIAAAAJ,,,,,mit.edu,PDF,https://dspace.mit.edu/bitstream/handle/1721.1/97034/Roy_Bayesian%20nonparametric.pdf?isAllowed=y&sequence=1,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=r05fgIUC3IcJ,77.0,"https://scholar.google.com/scholar?cites=9789702462404251311&as_sdt=80000005&sciodt=0,23&hl=en&num=20",9789702462404251311,https://serpapi.com/search.json?as_sdt=80000005&cites=9789702462404251311&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:r05fgIUC3IcJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,23",https://serpapi.com/search.json?as_sdt=0%2C23&engine=google_scholar&hl=en&num=20&q=related%3Ar05fgIUC3IcJ%3Ascholar.google.com%2F&start=60,21.0,"https://scholar.google.com/scholar?cluster=9789702462404251311&hl=en&num=20&as_sdt=0,23",9789702462404251311,https://serpapi.com/search.json?as_sdt=0%2C23&cluster=9789702462404251311&engine=google_scholar&hl=en&num=20,,,,,,,
4,Deep reinforcement learning for wireless scheduling in distributed networked control,OwFFUczrHpkJ,https://arxiv.org/abs/2109.12562,"… in terms of a finite-length countable vector state, we formulate the optimal transmission scheduling problem into a Markov decision process and develop a deep-reinforcementlearning-…","W Liu, K Huang, DE Quevedo, B Vucetic… - arXiv preprint arXiv …, 2021 - arxiv.org",W Liu,https://scholar.google.com/citations?user=NGBiMJEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=NGBiMJEAAAAJ&engine=google_scholar_author&hl=en,NGBiMJEAAAAJ,DE Quevedo,https://scholar.google.com/citations?user=s0MIz4sAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=s0MIz4sAAAAJ&engine=google_scholar_author&hl=en,s0MIz4sAAAAJ,B Vucetic,https://scholar.google.com/citations?user=uS5QyboAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=uS5QyboAAAAJ&engine=google_scholar_author&hl=en,uS5QyboAAAAJ,,,,,arxiv.org,PDF,https://arxiv.org/pdf/2109.12562,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=OwFFUczrHpkJ,17.0,"https://scholar.google.com/scholar?cites=11033515399873626427&as_sdt=80000005&sciodt=0,23&hl=en&num=20",11033515399873626427,https://serpapi.com/search.json?as_sdt=80000005&cites=11033515399873626427&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:OwFFUczrHpkJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,23",https://serpapi.com/search.json?as_sdt=0%2C23&engine=google_scholar&hl=en&num=20&q=related%3AOwFFUczrHpkJ%3Ascholar.google.com%2F&start=60,2.0,"https://scholar.google.com/scholar?cluster=11033515399873626427&hl=en&num=20&as_sdt=0,23",11033515399873626427,https://serpapi.com/search.json?as_sdt=0%2C23&cluster=11033515399873626427&engine=google_scholar&hl=en&num=20,"https://scholar.googleusercontent.com/scholar?q=cache:OwFFUczrHpkJ:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,23",,,,,,
5,Death and suicide in universal artificial intelligence,46nfq71Ai5sJ,https://link.springer.com/chapter/10.1007/978-3-319-41649-6_3,"… Reinforcement learning (RL) is a general paradigm for studying intelligent behaviour, with … AIXI may maintain high probability of death off-sequence in certain situations. Put simply, AIXI …","J Martin, T Everitt, M Hutter - … : 9th International Conference, AGI 2016, New …, 2016 - Springer",J Martin,https://scholar.google.com/citations?user=44RyHy4AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=44RyHy4AAAAJ&engine=google_scholar_author&hl=en,44RyHy4AAAAJ,T Everitt,https://scholar.google.com/citations?user=BdulyjIAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=BdulyjIAAAAJ&engine=google_scholar_author&hl=en,BdulyjIAAAAJ,M Hutter,https://scholar.google.com/citations?user=7hmCntEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=7hmCntEAAAAJ&engine=google_scholar_author&hl=en,7hmCntEAAAAJ,,,,,arxiv.org,PDF,https://arxiv.org/pdf/1606.00652,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=46nfq71Ai5sJ,15.0,"https://scholar.google.com/scholar?cites=11208123281018169827&as_sdt=80000005&sciodt=0,23&hl=en&num=20",11208123281018169827,https://serpapi.com/search.json?as_sdt=80000005&cites=11208123281018169827&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:46nfq71Ai5sJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,23",https://serpapi.com/search.json?as_sdt=0%2C23&engine=google_scholar&hl=en&num=20&q=related%3A46nfq71Ai5sJ%3Ascholar.google.com%2F&start=60,8.0,"https://scholar.google.com/scholar?cluster=11208123281018169827&hl=en&num=20&as_sdt=0,23",11208123281018169827,https://serpapi.com/search.json?as_sdt=0%2C23&cluster=11208123281018169827&engine=google_scholar&hl=en&num=20,,,,,,,
6,Model-Free Cooperative Optimal Output Regulation for Linear Discrete-Time Multi-Agent Systems Using Reinforcement Learning,asz1iR1ouXkJ,https://www.hindawi.com/journals/mpe/2023/6350647/,"… Based on the reinforcement learning method, an adaptive algorithm is presented to find the optimal feedback gains via online data collection from system trajectory. By designing a …","B Wu, W Wu - Mathematical Problems in Engineering, 2023 - hindawi.com",B Wu,https://scholar.google.com/citations?user=UX2048QAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=UX2048QAAAAJ&engine=google_scholar_author&hl=en,UX2048QAAAAJ,,,,,,,,,,,,,hindawi.com,HTML,https://www.hindawi.com/journals/mpe/2023/6350647/,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=asz1iR1ouXkJ,1.0,"https://scholar.google.com/scholar?cites=8771156225358679146&as_sdt=80000005&sciodt=0,23&hl=en&num=20",8771156225358679146,https://serpapi.com/search.json?as_sdt=80000005&cites=8771156225358679146&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:asz1iR1ouXkJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,23",https://serpapi.com/search.json?as_sdt=0%2C23&engine=google_scholar&hl=en&num=20&q=related%3Aasz1iR1ouXkJ%3Ascholar.google.com%2F&start=60,8.0,"https://scholar.google.com/scholar?cluster=8771156225358679146&hl=en&num=20&as_sdt=0,23",8771156225358679146,https://serpapi.com/search.json?as_sdt=0%2C23&cluster=8771156225358679146&engine=google_scholar&hl=en&num=20,"https://scholar.googleusercontent.com/scholar?q=cache:asz1iR1ouXkJ:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,23",,,,,Html,https://www.hindawi.com/journals/mpe/2023/6350647/
7,Towards safe artificial general intelligence,LqiIWBriRfAJ,https://search.proquest.com/openview/dab94a88b22c1240a8c791ca0178099c/1?pq-origsite=gscholar&cbl=2026366&diss=y,… misalignment problems in reinforcement learning agents as … when a powerful reinforcement learning system will permit … Some aspects of reasoning are swept under the rug by AIXI and …,T Everitt - 2019 - search.proquest.com,T Everitt,https://scholar.google.com/citations?user=BdulyjIAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=BdulyjIAAAAJ&engine=google_scholar_author&hl=en,BdulyjIAAAAJ,,,,,,,,,,,,,core.ac.uk,PDF,https://core.ac.uk/download/pdf/222805813.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=LqiIWBriRfAJ,32.0,"https://scholar.google.com/scholar?cites=17313492945278117934&as_sdt=80000005&sciodt=0,23&hl=en&num=20",17313492945278117934,https://serpapi.com/search.json?as_sdt=80000005&cites=17313492945278117934&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:LqiIWBriRfAJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,23",https://serpapi.com/search.json?as_sdt=0%2C23&engine=google_scholar&hl=en&num=20&q=related%3ALqiIWBriRfAJ%3Ascholar.google.com%2F&start=60,6.0,"https://scholar.google.com/scholar?cluster=17313492945278117934&hl=en&num=20&as_sdt=0,23",17313492945278117934,https://serpapi.com/search.json?as_sdt=0%2C23&cluster=17313492945278117934&engine=google_scholar&hl=en&num=20,,,,,,,
8,An approximation of the universal intelligence measure,YUu-y8EV8pUJ,https://link.springer.com/chapter/10.1007/978-3-642-44958-1_18,"… We have also implemented a number of reinforcement learning … AIXI [21,22], a more advanced reinforcement learning agent that can be viewed as an approximation to Hutter’s AIXI…","S Legg, J Veness - … Probability and Friends. Bayesian Prediction and …, 2013 - Springer",J Veness,https://scholar.google.com/citations?user=_iYrAxEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=_iYrAxEAAAAJ&engine=google_scholar_author&hl=en,_iYrAxEAAAAJ,,,,,,,,,,,,,vetta.org,PDF,http://www.vetta.org/documents/AIQ.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=YUu-y8EV8pUJ,60.0,"https://scholar.google.com/scholar?cites=10804722378106358625&as_sdt=80000005&sciodt=0,23&hl=en&num=20",10804722378106358625,https://serpapi.com/search.json?as_sdt=80000005&cites=10804722378106358625&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:YUu-y8EV8pUJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,23",https://serpapi.com/search.json?as_sdt=0%2C23&engine=google_scholar&hl=en&num=20&q=related%3AYUu-y8EV8pUJ%3Ascholar.google.com%2F&start=60,6.0,"https://scholar.google.com/scholar?cluster=10804722378106358625&hl=en&num=20&as_sdt=0,23",10804722378106358625,https://serpapi.com/search.json?as_sdt=0%2C23&cluster=10804722378106358625&engine=google_scholar&hl=en&num=20,,,,,,,
9,Enactivism & Objectively Optimal Super-Intelligence,BdK8A8xTBJIJ,https://arxiv.org/abs/2302.00843,"… The general reinforcement learning agent AIXI is pareto optimal. However, this claim regarding AIXI’s performance is highly subjective, because that performance depends upon the …","MT Bennett - arXiv preprint arXiv:2302.00843, 2023 - arxiv.org",MT Bennett,https://scholar.google.com/citations?user=M4sNTn8AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=M4sNTn8AAAAJ&engine=google_scholar_author&hl=en,M4sNTn8AAAAJ,,,,,,,,,,,,,arxiv.org,PDF,https://arxiv.org/pdf/2302.00843,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=BdK8A8xTBJIJ,1.0,"https://scholar.google.com/scholar?cites=10521626765145461253&as_sdt=80000005&sciodt=0,23&hl=en&num=20",10521626765145461253,https://serpapi.com/search.json?as_sdt=80000005&cites=10521626765145461253&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:BdK8A8xTBJIJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,23",https://serpapi.com/search.json?as_sdt=0%2C23&engine=google_scholar&hl=en&num=20&q=related%3ABdK8A8xTBJIJ%3Ascholar.google.com%2F&start=60,7.0,"https://scholar.google.com/scholar?cluster=10521626765145461253&hl=en&num=20&as_sdt=0,23",10521626765145461253,https://serpapi.com/search.json?as_sdt=0%2C23&cluster=10521626765145461253&engine=google_scholar&hl=en&num=20,"https://scholar.googleusercontent.com/scholar?q=cache:BdK8A8xTBJIJ:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,23",,,,,,
10,Adaptive Output Synchronization With Designated Convergence Rate of Multiagent Systems Based on Off-Policy Reinforcement Learning,CDuLWuB-88IJ,https://ieeexplore.ieee.org/abstract/document/10517758/,… The data-efficient off-policy reinforcement learning and outputfeedback technique are applied to solve the enhanced Bellman equations with a designated convergence rate. This …,"C Huang, C Chen, K Xie, Z Li… - IEEE Transactions on …, 2024 - ieeexplore.ieee.org",C Chen,https://scholar.google.com/citations?user=PGgxCSwAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=PGgxCSwAAAAJ&engine=google_scholar_author&hl=en,PGgxCSwAAAAJ,,,,,,,,,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=CDuLWuB-88IJ,,,,,"https://scholar.google.com/scholar?q=related:CDuLWuB-88IJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,23",https://serpapi.com/search.json?as_sdt=0%2C23&engine=google_scholar&hl=en&num=20&q=related%3ACDuLWuB-88IJ%3Ascholar.google.com%2F&start=60,,,,,,,,,,,
11,An adaptive V2G capacity-based frequency regulation scheme with integral reinforcement learning against DoS attacks,UgXDsFyIW1EJ,https://ieeexplore.ieee.org/abstract/document/10109154/,… optimize frequency control via reinforcement learning (RL) [11… utilizing a multi-agent deep reinforcement learning (MA-DRL) … be removed from integral reinforcement learning (IRL) which …,"J Sun, G Qi, Y Chai, Z Zhu… - IEEE Transactions on …, 2023 - ieeexplore.ieee.org",J Sun,https://scholar.google.com/citations?user=ALVSZAYAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=ALVSZAYAAAAJ&engine=google_scholar_author&hl=en,ALVSZAYAAAAJ,Z Zhu,https://scholar.google.com/citations?user=E15PyhoAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=E15PyhoAAAAJ&engine=google_scholar_author&hl=en,E15PyhoAAAAJ,,,,,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=UgXDsFyIW1EJ,4.0,"https://scholar.google.com/scholar?cites=5862429271636772178&as_sdt=80000005&sciodt=0,23&hl=en&num=20",5862429271636772178,https://serpapi.com/search.json?as_sdt=80000005&cites=5862429271636772178&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:UgXDsFyIW1EJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,23",https://serpapi.com/search.json?as_sdt=0%2C23&engine=google_scholar&hl=en&num=20&q=related%3AUgXDsFyIW1EJ%3Ascholar.google.com%2F&start=60,2.0,"https://scholar.google.com/scholar?cluster=5862429271636772178&hl=en&num=20&as_sdt=0,23",5862429271636772178,https://serpapi.com/search.json?as_sdt=0%2C23&cluster=5862429271636772178&engine=google_scholar&hl=en&num=20,,,,,,,
12,Virtuously Safe Reinforcement Learning (Master Thesis version),nlj9SaaHWDAJ,https://infoscience.epfl.ch/record/270380/files/Henriks_master_thesis.pdf,"… However, the universal reinforcement learning agent AIXI … to achieve virtuous safe reinforcement learning in seven steps. … towards virtuous safe reinforcement learning by introducing …",H Aslund - 2018 - infoscience.epfl.ch,,,,,,,,,,,,,,,,,epfl.ch,PDF,https://infoscience.epfl.ch/record/270380/files/Henriks_master_thesis.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=nlj9SaaHWDAJ,,,,,"https://scholar.google.com/scholar?q=related:nlj9SaaHWDAJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,23",https://serpapi.com/search.json?as_sdt=0%2C23&engine=google_scholar&hl=en&num=20&q=related%3Anlj9SaaHWDAJ%3Ascholar.google.com%2F&start=60,,,,,"https://scholar.googleusercontent.com/scholar?q=cache:nlj9SaaHWDAJ:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,23",,,,,Pdf,
13,"Measures of Intelligence, Perception and Intelligent Agents",GpKO2MJsOGUJ,https://link.springer.com/chapter/10.1007/978-3-030-93758-4_18,"… a discrete-time reinforcement learning agent model with … of his universal reinforcement learning (RL) model AIXI [6], … which is again based on the AIXI model. Their universal intelligence …","E Özkural - … Intelligence: 14th International Conference, AGI 2021 …, 2022 - Springer",E Özkural,https://scholar.google.com/citations?user=r_-Vi64AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=r_-Vi64AAAAJ&engine=google_scholar_author&hl=en,r_-Vi64AAAAJ,,,,,,,,,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=GpKO2MJsOGUJ,2.0,"https://scholar.google.com/scholar?cites=7293699180639195674&as_sdt=80000005&sciodt=0,23&hl=en&num=20",7293699180639195674,https://serpapi.com/search.json?as_sdt=80000005&cites=7293699180639195674&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:GpKO2MJsOGUJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,23",https://serpapi.com/search.json?as_sdt=0%2C23&engine=google_scholar&hl=en&num=20&q=related%3AGpKO2MJsOGUJ%3Ascholar.google.com%2F&start=60,3.0,"https://scholar.google.com/scholar?cluster=7293699180639195674&hl=en&num=20&as_sdt=0,23",7293699180639195674,https://serpapi.com/search.json?as_sdt=0%2C23&cluster=7293699180639195674&engine=google_scholar&hl=en&num=20,,,,,,,
14,Autonomous Vehicle Platoons in Urban Road Networks: A Joint Distributed Reinforcement Learning and Model Predictive Control Approach,ytfwjwy6euQJ,https://ieeexplore.ieee.org/abstract/document/10400390/,"… reinforcement learning and model predictive control. On one hand, the routing decisions are obtained by using a distributed reinforcement learning … deep reinforcement learning output (…","L D'Alfonso, F Giannini, G Franzè… - IEEE/CAA Journal of …, 2024 - ieeexplore.ieee.org",L D'Alfonso,https://scholar.google.com/citations?user=6smqydsAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=6smqydsAAAAJ&engine=google_scholar_author&hl=en,6smqydsAAAAJ,F Giannini,https://scholar.google.com/citations?user=xV6XPOQAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=xV6XPOQAAAAJ&engine=google_scholar_author&hl=en,xV6XPOQAAAAJ,G Franzè,https://scholar.google.com/citations?user=j4zeuuoAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=j4zeuuoAAAAJ&engine=google_scholar_author&hl=en,j4zeuuoAAAAJ,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=ytfwjwy6euQJ,1.0,"https://scholar.google.com/scholar?cites=16463675950923569098&as_sdt=80000005&sciodt=0,23&hl=en&num=20",16463675950923569098,https://serpapi.com/search.json?as_sdt=80000005&cites=16463675950923569098&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:ytfwjwy6euQJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,23",https://serpapi.com/search.json?as_sdt=0%2C23&engine=google_scholar&hl=en&num=20&q=related%3Aytfwjwy6euQJ%3Ascholar.google.com%2F&start=60,4.0,"https://scholar.google.com/scholar?cluster=16463675950923569098&hl=en&num=20&as_sdt=0,23",16463675950923569098,https://serpapi.com/search.json?as_sdt=0%2C23&cluster=16463675950923569098&engine=google_scholar&hl=en&num=20,,,,,,,
15,A Bayesian Approach for Learning and Planning in Partially Observable Markov Decision Processes.,oGXZ4-Wa58UJ,https://www.jmlr.org/papers/volume12/ross11a/ross11a.pdf,"… In this section we discuss the problem of model-based Bayesian reinforcement learning in the fully observable case, in preparation for the extension of these ideas to the partially …","S Ross, J Pineau, B Chaib-draa, P Kreitmann - Journal of Machine …, 2011 - jmlr.org",J Pineau,https://scholar.google.com/citations?user=CEt6_mMAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=CEt6_mMAAAAJ&engine=google_scholar_author&hl=en,CEt6_mMAAAAJ,B Chaib-draa,https://scholar.google.com/citations?user=JycXWO0AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=JycXWO0AAAAJ&engine=google_scholar_author&hl=en,JycXWO0AAAAJ,,,,,,,,,jmlr.org,PDF,https://www.jmlr.org/papers/volume12/ross11a/ross11a.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=oGXZ4-Wa58UJ,201.0,"https://scholar.google.com/scholar?cites=14260537057252828576&as_sdt=80000005&sciodt=0,23&hl=en&num=20",14260537057252828576,https://serpapi.com/search.json?as_sdt=80000005&cites=14260537057252828576&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:oGXZ4-Wa58UJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,23",https://serpapi.com/search.json?as_sdt=0%2C23&engine=google_scholar&hl=en&num=20&q=related%3AoGXZ4-Wa58UJ%3Ascholar.google.com%2F&start=60,19.0,"https://scholar.google.com/scholar?cluster=14260537057252828576&hl=en&num=20&as_sdt=0,23",14260537057252828576,https://serpapi.com/search.json?as_sdt=0%2C23&cluster=14260537057252828576&engine=google_scholar&hl=en&num=20,"https://scholar.googleusercontent.com/scholar?q=cache:oGXZ4-Wa58UJ:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,23",,,,,Pdf,
16,Deep reinforcement learning for optimal denial-of-service attacks scheduling,8KWvS7UduFsJ,https://link.springer.com/article/10.1007/s11432-020-3027-0,"… Differing from [33], in this paper, a deep reinforcement learning (DRL) algorithm, which is independent of the initializations of attack variables, is introduced to solve the optimal DoS …","F Hou, J Sun, Q Yang, Z Pang - Science China Information Sciences, 2022 - Springer",J Sun,https://scholar.google.com/citations?user=ALVSZAYAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=ALVSZAYAAAAJ&engine=google_scholar_author&hl=en,ALVSZAYAAAAJ,Q Yang,https://scholar.google.com/citations?user=SVP3KoAAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=SVP3KoAAAAAJ&engine=google_scholar_author&hl=en,SVP3KoAAAAAJ,,,,,,,,,scichina.com,PDF,http://scis.scichina.com/en/2022/162201.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=8KWvS7UduFsJ,13.0,"https://scholar.google.com/scholar?cites=6609065117662291440&as_sdt=80000005&sciodt=0,23&hl=en&num=20",6609065117662291440,https://serpapi.com/search.json?as_sdt=80000005&cites=6609065117662291440&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:8KWvS7UduFsJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,23",https://serpapi.com/search.json?as_sdt=0%2C23&engine=google_scholar&hl=en&num=20&q=related%3A8KWvS7UduFsJ%3Ascholar.google.com%2F&start=60,3.0,"https://scholar.google.com/scholar?cluster=6609065117662291440&hl=en&num=20&as_sdt=0,23",6609065117662291440,https://serpapi.com/search.json?as_sdt=0%2C23&cluster=6609065117662291440&engine=google_scholar&hl=en&num=20,,,,,,,
17,Markov abstractions for PAC reinforcement learning in non-markov decision processes,lx83YuBXTB8J,https://arxiv.org/abs/2205.01053,"… We show that Markov abstractions can be learned during reinforcement learning. Our approach combines automata learning and classic reinforcement learning. For these two tasks, …","A Ronca, GP Licks, G De Giacomo - arXiv preprint arXiv:2205.01053, 2022 - arxiv.org",A Ronca,https://scholar.google.com/citations?user=0NwK2-IAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=0NwK2-IAAAAJ&engine=google_scholar_author&hl=en,0NwK2-IAAAAJ,GP Licks,https://scholar.google.com/citations?user=zUjLlV0AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=zUjLlV0AAAAJ&engine=google_scholar_author&hl=en,zUjLlV0AAAAJ,G De Giacomo,https://scholar.google.com/citations?user=Sfo4K0oAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=Sfo4K0oAAAAJ&engine=google_scholar_author&hl=en,Sfo4K0oAAAAJ,,,,,arxiv.org,PDF,https://arxiv.org/pdf/2205.01053,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=lx83YuBXTB8J,11.0,"https://scholar.google.com/scholar?cites=2255274134637846423&as_sdt=80000005&sciodt=0,23&hl=en&num=20",2255274134637846423,https://serpapi.com/search.json?as_sdt=80000005&cites=2255274134637846423&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:lx83YuBXTB8J:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,23",https://serpapi.com/search.json?as_sdt=0%2C23&engine=google_scholar&hl=en&num=20&q=related%3Alx83YuBXTB8J%3Ascholar.google.com%2F&start=60,9.0,"https://scholar.google.com/scholar?cluster=2255274134637846423&hl=en&num=20&as_sdt=0,23",2255274134637846423,https://serpapi.com/search.json?as_sdt=0%2C23&cluster=2255274134637846423&engine=google_scholar&hl=en&num=20,"https://scholar.googleusercontent.com/scholar?q=cache:lx83YuBXTB8J:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,23",,,,,,
18,Monte Carlo tree search algorithms for risk-aware and multi-objective reinforcement learning,T4olC0arAp4J,https://link.springer.com/article/10.1007/s10458-022-09596-0,"… 2.1 Multi-objective reinforcement learning In multi-objective reinforcement learning (MORL), we deal with decision problems with multiple objectives [44, 62], often modelled as a multi-…","CF Hayes, M Reymond, DM Roijers, E Howley… - Autonomous Agents and …, 2023 - Springer",CF Hayes,https://scholar.google.com/citations?user=p4-eiAkAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=p4-eiAkAAAAJ&engine=google_scholar_author&hl=en,p4-eiAkAAAAJ,M Reymond,https://scholar.google.com/citations?user=b9bYrHUAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=b9bYrHUAAAAJ&engine=google_scholar_author&hl=en,b9bYrHUAAAAJ,DM Roijers,https://scholar.google.com/citations?user=oi25V4EAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=oi25V4EAAAAJ&engine=google_scholar_author&hl=en,oi25V4EAAAAJ,E Howley,https://scholar.google.com/citations?user=29vbDa0AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=29vbDa0AAAAJ&engine=google_scholar_author&hl=en,29vbDa0AAAAJ,springer.com,PDF,https://link.springer.com/content/pdf/10.1007/s10458-022-09596-0.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=T4olC0arAp4J,8.0,"https://scholar.google.com/scholar?cites=11385851125269105231&as_sdt=80000005&sciodt=0,23&hl=en&num=20",11385851125269105231,https://serpapi.com/search.json?as_sdt=80000005&cites=11385851125269105231&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:T4olC0arAp4J:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,23",https://serpapi.com/search.json?as_sdt=0%2C23&engine=google_scholar&hl=en&num=20&q=related%3AT4olC0arAp4J%3Ascholar.google.com%2F&start=60,7.0,"https://scholar.google.com/scholar?cluster=11385851125269105231&hl=en&num=20&as_sdt=0,23",11385851125269105231,https://serpapi.com/search.json?as_sdt=0%2C23&cluster=11385851125269105231&engine=google_scholar&hl=en&num=20,,,,,,,
19,Provably Efficient Offline Reinforcement Learning in Regular Decision Processes,TM6S4wybLSoJ,https://proceedings.neurips.cc/paper_files/paper/2023/hash/7bf3e93543a612b75b6373178ba1faa4-Abstract-Conference.html,"… Approximate information state for approximate planning and reinforcement learning in partially observed systems. The Journal of Machine Learning Research, 23(1):483–565, 2022. …","R Cipollone, A Jonsson, A Ronca… - Advances in Neural …, 2024 - proceedings.neurips.cc",R Cipollone,https://scholar.google.com/citations?user=kR301eQAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=kR301eQAAAAJ&engine=google_scholar_author&hl=en,kR301eQAAAAJ,A Jonsson,https://scholar.google.com/citations?user=SI_uHCIAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=SI_uHCIAAAAJ&engine=google_scholar_author&hl=en,SI_uHCIAAAAJ,A Ronca,https://scholar.google.com/citations?user=0NwK2-IAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=0NwK2-IAAAAJ&engine=google_scholar_author&hl=en,0NwK2-IAAAAJ,,,,,neurips.cc,PDF,https://proceedings.neurips.cc/paper_files/paper/2023/file/7bf3e93543a612b75b6373178ba1faa4-Paper-Conference.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=TM6S4wybLSoJ,,,,,"https://scholar.google.com/scholar?q=related:TM6S4wybLSoJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,23",https://serpapi.com/search.json?as_sdt=0%2C23&engine=google_scholar&hl=en&num=20&q=related%3ATM6S4wybLSoJ%3Ascholar.google.com%2F&start=60,2.0,"https://scholar.google.com/scholar?cluster=3039255803204914764&hl=en&num=20&as_sdt=0,23",3039255803204914764,https://serpapi.com/search.json?as_sdt=0%2C23&cluster=3039255803204914764&engine=google_scholar&hl=en&num=20,"https://scholar.googleusercontent.com/scholar?q=cache:TM6S4wybLSoJ:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,23",,,,,,
0,Algorithm for Aligned Artificial General Intelligence,c-w0PoUuWo0J,,,"MK Cohen, B Vellambi, M Hutter - 2018",B Vellambi,https://scholar.google.com/citations?user=1J6LSSIAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=1J6LSSIAAAAJ&engine=google_scholar_author&hl=en,1J6LSSIAAAAJ,M Hutter,https://scholar.google.com/citations?user=7hmCntEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=7hmCntEAAAAJ&engine=google_scholar_author&hl=en,7hmCntEAAAAJ,,,,,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=c-w0PoUuWo0J,,,,,"https://scholar.google.com/scholar?q=related:c-w0PoUuWo0J:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,14",https://serpapi.com/search.json?as_sdt=0%2C14&engine=google_scholar&hl=en&num=20&q=related%3Ac-w0PoUuWo0J%3Ascholar.google.com%2F&start=80,,,,,,,,,,Citation,
1,Optimal Universal Explorative Agents,gTm5EmCFRXoJ,https://ewrl.wordpress.com/wp-content/uploads/2012/10/ewrl11_submission_26.pdf,"… We present some recent research [Ors11, OLH13] in the area of universal reinforcement learning … on Hutter’s AIXI [Hut05], which defines the optimal reinforcement learning agent, for all …","L Orseau, T Lattimore, M Hutter - ewrl.wordpress.com",L Orseau,https://scholar.google.com/citations?user=HVJjWlEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=HVJjWlEAAAAJ&engine=google_scholar_author&hl=en,HVJjWlEAAAAJ,T Lattimore,https://scholar.google.com/citations?user=fkDxJxcAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=fkDxJxcAAAAJ&engine=google_scholar_author&hl=en,fkDxJxcAAAAJ,M Hutter,https://scholar.google.com/citations?user=7hmCntEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=7hmCntEAAAAJ&engine=google_scholar_author&hl=en,7hmCntEAAAAJ,,,,,wordpress.com,PDF,https://ewrl.wordpress.com/wp-content/uploads/2012/10/ewrl11_submission_26.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=gTm5EmCFRXoJ,,,,,"https://scholar.google.com/scholar?q=related:gTm5EmCFRXoJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,14",https://serpapi.com/search.json?as_sdt=0%2C14&engine=google_scholar&hl=en&num=20&q=related%3AgTm5EmCFRXoJ%3Ascholar.google.com%2F&start=80,,,,,"https://scholar.googleusercontent.com/scholar?q=cache:gTm5EmCFRXoJ:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,14",,,,,Pdf,
2,An Introduction to Induction from an AIT View,B4v2UcE2ty8J,http://logic.pku.edu.cn/ann_attachments/aixi.pdf,"… After that we introduce Hutter’s general reinforcement learning agents—AIXI based on Solomonoff’s work. At last, we introduce some approximate approaches and applications. …",X Li - 2012 - logic.pku.edu.cn,,,,,,,,,,,,,,,,,pku.edu.cn,PDF,http://logic.pku.edu.cn/ann_attachments/aixi.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=B4v2UcE2ty8J,,,,,"https://scholar.google.com/scholar?q=related:B4v2UcE2ty8J:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,14",https://serpapi.com/search.json?as_sdt=0%2C14&engine=google_scholar&hl=en&num=20&q=related%3AB4v2UcE2ty8J%3Ascholar.google.com%2F&start=80,2.0,"https://scholar.google.com/scholar?cluster=3438277044452363015&hl=en&num=20&as_sdt=0,14",3438277044452363015,https://serpapi.com/search.json?as_sdt=0%2C14&cluster=3438277044452363015&engine=google_scholar&hl=en&num=20,"https://scholar.googleusercontent.com/scholar?q=cache:B4v2UcE2ty8J:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,14",,,,,Pdf,
3,Heterogeneous formation control of multiple rotorcrafts with unknown dynamics using reinforcement learning,kKMDHNA9LdEJ,https://ieeexplore.ieee.org/abstract/document/9030076/,"… In this paper, the newly developed reinforcement learning (RL) algorithm, as shown in [20], [21], is proposed to address the heterogeneous multi-rotorcraft formation control problem with …","H Liu, F Peng, H Modares, B Kiumarsi… - 2019 IEEE 58th …, 2019 - ieeexplore.ieee.org",H Modares,https://scholar.google.com/citations?user=xhucCdUAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=xhucCdUAAAAJ&engine=google_scholar_author&hl=en,xhucCdUAAAAJ,B Kiumarsi,https://scholar.google.com/citations?user=MnRdNZoAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=MnRdNZoAAAAJ&engine=google_scholar_author&hl=en,MnRdNZoAAAAJ,,,,,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=kKMDHNA9LdEJ,3.0,"https://scholar.google.com/scholar?cites=15072771491911410576&as_sdt=400005&sciodt=0,14&hl=en&num=20",15072771491911410576,https://serpapi.com/search.json?as_sdt=400005&cites=15072771491911410576&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:kKMDHNA9LdEJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,14",https://serpapi.com/search.json?as_sdt=0%2C14&engine=google_scholar&hl=en&num=20&q=related%3AkKMDHNA9LdEJ%3Ascholar.google.com%2F&start=80,2.0,"https://scholar.google.com/scholar?cluster=15072771491911410576&hl=en&num=20&as_sdt=0,14",15072771491911410576,https://serpapi.com/search.json?as_sdt=0%2C14&cluster=15072771491911410576&engine=google_scholar&hl=en&num=20,,,,,,,
4,Teamwork Reinforcement Learning With Concave Utilities,4PUo4NHfvroJ,https://ieeexplore.ieee.org/abstract/document/10250920/,"… Most of these works discuss a multi-agent reinforcement learning (MARL) setup… reinforcement learning model with multiple agents involved, namely the teamwork reinforcement learning …","Z Yu, J Zhang, Z Wen, A Tacchetti… - IEEE Transactions on …, 2023 - ieeexplore.ieee.org",Z Yu,https://scholar.google.com/citations?user=p5uvh2oAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=p5uvh2oAAAAJ&engine=google_scholar_author&hl=en,p5uvh2oAAAAJ,J Zhang,https://scholar.google.com/citations?user=bsN1uT0AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=bsN1uT0AAAAJ&engine=google_scholar_author&hl=en,bsN1uT0AAAAJ,Z Wen,https://scholar.google.com/citations?user=kK3qvd8AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=kK3qvd8AAAAJ&engine=google_scholar_author&hl=en,kK3qvd8AAAAJ,A Tacchetti,https://scholar.google.com/citations?user=HKybSogAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=HKybSogAAAAJ&engine=google_scholar_author&hl=en,HKybSogAAAAJ,openreview.net,PDF,https://openreview.net/pdf?id=H-MDj5kaxc,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=4PUo4NHfvroJ,,,,,"https://scholar.google.com/scholar?q=related:4PUo4NHfvroJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,14",https://serpapi.com/search.json?as_sdt=0%2C14&engine=google_scholar&hl=en&num=20&q=related%3A4PUo4NHfvroJ%3Ascholar.google.com%2F&start=80,4.0,"https://scholar.google.com/scholar?cluster=13456438829131560416&hl=en&num=20&as_sdt=0,14",13456438829131560416,https://serpapi.com/search.json?as_sdt=0%2C14&cluster=13456438829131560416&engine=google_scholar&hl=en&num=20,,,,,,,
5,Teleporting universal intelligent agents,SHmDhLrzbqIJ,https://link.springer.com/chapter/10.1007/978-3-319-09274-4_11,"… reinforcement-learning … as AIXI, it is still possible to estimate the behavior of AIXI in multi-slot environments, even though there is no direct counterpart for AIMU. We show that, since AIXI …","L Orseau - … General Intelligence: 7th International Conference, AGI …, 2014 - Springer",L Orseau,https://scholar.google.com/citations?user=HVJjWlEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=HVJjWlEAAAAJ&engine=google_scholar_author&hl=en,HVJjWlEAAAAJ,,,,,,,,,,,,,agi-conf.org,PDF,https://agi-conf.org/2014/wp-content/uploads/2014/08/orseau-teleporting-agi14.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=SHmDhLrzbqIJ,9.0,"https://scholar.google.com/scholar?cites=11704560463995173192&as_sdt=400005&sciodt=0,14&hl=en&num=20",11704560463995173192,https://serpapi.com/search.json?as_sdt=400005&cites=11704560463995173192&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:SHmDhLrzbqIJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,14",https://serpapi.com/search.json?as_sdt=0%2C14&engine=google_scholar&hl=en&num=20&q=related%3ASHmDhLrzbqIJ%3Ascholar.google.com%2F&start=80,8.0,"https://scholar.google.com/scholar?cluster=11704560463995173192&hl=en&num=20&as_sdt=0,14",11704560463995173192,https://serpapi.com/search.json?as_sdt=0%2C14&cluster=11704560463995173192&engine=google_scholar&hl=en&num=20,,,,,,,
6,Temporally extended features in model-based reinforcement learning with partial observability,MYVcOsapQ1AJ,https://www.sciencedirect.com/science/article/pii/S0925231216002563,"… In Section 4, we show how Pulse can be used to train reinforcement learning agents. Finally, we discuss our empirical evaluations in Section 5 and conclude with Section 6. …","R Lieck, M Toussaint - Neurocomputing, 2016 - Elsevier",R Lieck,https://scholar.google.com/citations?user=iGLS8hMAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=iGLS8hMAAAAJ&engine=google_scholar_author&hl=en,iGLS8hMAAAAJ,M Toussaint,https://scholar.google.com/citations?user=t2X4Mg8AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=t2X4Mg8AAAAJ&engine=google_scholar_author&hl=en,t2X4Mg8AAAAJ,,,,,,,,,robert-lieck.com,PDF,http://robert-lieck.com/literature/pdfs/DHFMUI2D/Lieck_and_Toussaint_-_2016_-_Temporally_extended_features_in_model-based_reinfo.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=MYVcOsapQ1AJ,15.0,"https://scholar.google.com/scholar?cites=5783653015321609521&as_sdt=400005&sciodt=0,14&hl=en&num=20",5783653015321609521,https://serpapi.com/search.json?as_sdt=400005&cites=5783653015321609521&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:MYVcOsapQ1AJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,14",https://serpapi.com/search.json?as_sdt=0%2C14&engine=google_scholar&hl=en&num=20&q=related%3AMYVcOsapQ1AJ%3Ascholar.google.com%2F&start=80,10.0,"https://scholar.google.com/scholar?cluster=5783653015321609521&hl=en&num=20&as_sdt=0,14",5783653015321609521,https://serpapi.com/search.json?as_sdt=0%2C14&cluster=5783653015321609521&engine=google_scholar&hl=en&num=20,,,,,,,
7,Joint routing and computation offloading based deep reinforcement learning for Flying Ad hoc Networks,HxAlDJ4U6McJ,https://www.sciencedirect.com/science/article/pii/S1389128624003463,… Development of a Computing First Routing (CFR) Protocol: We introduce a novel CFR protocol that leverages position-based information and deep reinforcement learning algorithms. …,"N Lin, J Huang, A Hawbani, L Zhao, H Tang, Y Guan… - Computer Networks, 2024 - Elsevier",A Hawbani,https://scholar.google.com/citations?user=HZRx6AkAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=HZRx6AkAAAAJ&engine=google_scholar_author&hl=en,HZRx6AkAAAAJ,L Zhao,https://scholar.google.com/citations?user=Mhud0GUAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=Mhud0GUAAAAJ&engine=google_scholar_author&hl=en,Mhud0GUAAAAJ,,,,,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=HxAlDJ4U6McJ,,,,,"https://scholar.google.com/scholar?q=related:HxAlDJ4U6McJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,14",https://serpapi.com/search.json?as_sdt=0%2C14&engine=google_scholar&hl=en&num=20&q=related%3AHxAlDJ4U6McJ%3Ascholar.google.com%2F&start=80,,,,,,,,,,,
8,Design and optimization of a thermoacoustic heat engine using reinforcement learning,U9bbZCXVpNEJ,https://academic.oup.com/ijlct/article-abstract/11/3/431/2199550,"… In this work, a radical design methodology using reinforcement learning (RL) is employed for the design and optimization of a TAHE for the first time. Reinforcement learning is a …","JA Mumith, T Karayiannis… - International Journal of …, 2016 - academic.oup.com",,,,,,,,,,,,,,,,,oup.com,PDF,https://academic.oup.com/ijlct/article-pdf/11/3/431/6767042/ctv023.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=U9bbZCXVpNEJ,12.0,"https://scholar.google.com/scholar?cites=15106433406690121299&as_sdt=400005&sciodt=0,14&hl=en&num=20",15106433406690121299,https://serpapi.com/search.json?as_sdt=400005&cites=15106433406690121299&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:U9bbZCXVpNEJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,14",https://serpapi.com/search.json?as_sdt=0%2C14&engine=google_scholar&hl=en&num=20&q=related%3AU9bbZCXVpNEJ%3Ascholar.google.com%2F&start=80,12.0,"https://scholar.google.com/scholar?cluster=15106433406690121299&hl=en&num=20&as_sdt=0,14",15106433406690121299,https://serpapi.com/search.json?as_sdt=0%2C14&cluster=15106433406690121299&engine=google_scholar&hl=en&num=20,,,,,,,
9,Optimal regret bounds for selecting the state representation in reinforcement learning,QB5iG4J1oTgJ,https://proceedings.mlr.press/v28/maillard13.html,… We consider the problem of reinforcement learning when the learner interacts sequentially with some unknown environment: first some initial observation h1 = o1 ∈ H1 = O is provided …,"OA Maillard, P Nguyen, R Ortner… - … on Machine Learning, 2013 - proceedings.mlr.press",OA Maillard,https://scholar.google.com/citations?user=7EweMdoAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=7EweMdoAAAAJ&engine=google_scholar_author&hl=en,7EweMdoAAAAJ,P Nguyen,https://scholar.google.com/citations?user=cUGY-akAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=cUGY-akAAAAJ&engine=google_scholar_author&hl=en,cUGY-akAAAAJ,R Ortner,https://scholar.google.com/citations?user=9zM7wvkAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=9zM7wvkAAAAJ&engine=google_scholar_author&hl=en,9zM7wvkAAAAJ,,,,,mlr.press,PDF,http://proceedings.mlr.press/v28/maillard13.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=QB5iG4J1oTgJ,31.0,"https://scholar.google.com/scholar?cites=4080671939039993408&as_sdt=400005&sciodt=0,14&hl=en&num=20",4080671939039993408,https://serpapi.com/search.json?as_sdt=400005&cites=4080671939039993408&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:QB5iG4J1oTgJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,14",https://serpapi.com/search.json?as_sdt=0%2C14&engine=google_scholar&hl=en&num=20&q=related%3AQB5iG4J1oTgJ%3Ascholar.google.com%2F&start=80,26.0,"https://scholar.google.com/scholar?cluster=4080671939039993408&hl=en&num=20&as_sdt=0,14",4080671939039993408,https://serpapi.com/search.json?as_sdt=0%2C14&cluster=4080671939039993408&engine=google_scholar&hl=en&num=20,"http://scholar.googleusercontent.com/scholar?q=cache:QB5iG4J1oTgJ:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,14",,,,,,
10,Robust optimal formation control of heterogeneous multi-agent system via reinforcement learning,fZINRmzvmDUJ,https://ieeexplore.ieee.org/abstract/document/9276397/,"… In this paper, a reinforcement learning algorithm is designed to learn the optimal control policy for the agent nominal system without requirement of the dynamic information (Ai,Bi). Then …","W Lin, W Zhao, H Liu - IEEE Access, 2020 - ieeexplore.ieee.org",W Zhao,https://scholar.google.com/citations?user=tzJ9BsoAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=tzJ9BsoAAAAJ&engine=google_scholar_author&hl=en,tzJ9BsoAAAAJ,,,,,,,,,,,,,ieee.org,PDF,https://ieeexplore.ieee.org/iel7/6287639/6514899/09276397.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=fZINRmzvmDUJ,12.0,"https://scholar.google.com/scholar?cites=3862099928781001341&as_sdt=400005&sciodt=0,14&hl=en&num=20",3862099928781001341,https://serpapi.com/search.json?as_sdt=400005&cites=3862099928781001341&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:fZINRmzvmDUJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,14",https://serpapi.com/search.json?as_sdt=0%2C14&engine=google_scholar&hl=en&num=20&q=related%3AfZINRmzvmDUJ%3Ascholar.google.com%2F&start=80,2.0,"https://scholar.google.com/scholar?cluster=3862099928781001341&hl=en&num=20&as_sdt=0,14",3862099928781001341,https://serpapi.com/search.json?as_sdt=0%2C14&cluster=3862099928781001341&engine=google_scholar&hl=en&num=20,,,,,,,
11,An Application of a Monte-Carlo AIXI Approximation in Ecological Fire Management,d3ugz-tHPXsJ,https://researchprofiles.canberra.edu.au/en/publications/an-application-of-a-monte-carlo-aixi-approximation-in-ecological-,… the reinforcement learning problem and the Canadian Fire Weather Index. The study illustrates that the integration of AIXI … the AIXI model and offers a practical demonstration of applied …,"R Omari, D Newth, M Böhm… - International …, 2014 - researchprofiles.canberra.edu.au",R Omari,https://scholar.google.com/citations?user=68XltcYAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=68XltcYAAAAJ&engine=google_scholar_author&hl=en,68XltcYAAAAJ,D Newth,https://scholar.google.com/citations?user=2gzK_nwAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=2gzK_nwAAAAJ&engine=google_scholar_author&hl=en,2gzK_nwAAAAJ,,,,,,,,,,,,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=d3ugz-tHPXsJ,,,,,"https://scholar.google.com/scholar?q=related:d3ugz-tHPXsJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,14",https://serpapi.com/search.json?as_sdt=0%2C14&engine=google_scholar&hl=en&num=20&q=related%3Ad3ugz-tHPXsJ%3Ascholar.google.com%2F&start=80,,,,,"https://scholar.googleusercontent.com/scholar?q=cache:d3ugz-tHPXsJ:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,14",,,,,,
12,Output resilient containment control of heterogeneous systems with active leaders using reinforcement learning under attack inputs,02NrZZpipREJ,https://ieeexplore.ieee.org/abstract/document/8869870/,… containment control problem of heterogeneous multiple-agent systems (MASs) with unknown active leaders under attack inputs by using data-based offpolicy reinforcement learning (RL…,"Q Li, L Xia, R Song - IEEE Access, 2019 - ieeexplore.ieee.org",L Xia,https://scholar.google.com/citations?user=Euxh1SoAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=Euxh1SoAAAAJ&engine=google_scholar_author&hl=en,Euxh1SoAAAAJ,R Song,https://scholar.google.com/citations?user=wGWQpLIAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=wGWQpLIAAAAJ&engine=google_scholar_author&hl=en,wGWQpLIAAAAJ,,,,,,,,,ieee.org,PDF,https://ieeexplore.ieee.org/iel7/6287639/8600701/08869870.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=02NrZZpipREJ,7.0,"https://scholar.google.com/scholar?cites=1271530885068055507&as_sdt=400005&sciodt=0,14&hl=en&num=20",1271530885068055507,https://serpapi.com/search.json?as_sdt=400005&cites=1271530885068055507&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:02NrZZpipREJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,14",https://serpapi.com/search.json?as_sdt=0%2C14&engine=google_scholar&hl=en&num=20&q=related%3A02NrZZpipREJ%3Ascholar.google.com%2F&start=80,3.0,"https://scholar.google.com/scholar?cluster=1271530885068055507&hl=en&num=20&as_sdt=0,14",1271530885068055507,https://serpapi.com/search.json?as_sdt=0%2C14&cluster=1271530885068055507&engine=google_scholar&hl=en&num=20,,,,,,,
13,Influence-based abstraction in deep reinforcement learning,TIfRd32kEpUJ,https://www.fransoliehoek.net/docs/Suau19ALA.pdf,"… Unfortunately, applying reinforcement learning algorithms to handle complex tasks be… We explore this method in the context of deep reinforcement learning, showing that by keeping …","MS de Castro, E Congeduti, RA Starre… - Proceedings of the …, 2019 - fransoliehoek.net",E Congeduti,https://scholar.google.com/citations?user=sRfhUbwAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=sRfhUbwAAAAJ&engine=google_scholar_author&hl=en,sRfhUbwAAAAJ,RA Starre,https://scholar.google.com/citations?user=DWioaoQAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=DWioaoQAAAAJ&engine=google_scholar_author&hl=en,DWioaoQAAAAJ,,,,,,,,,fransoliehoek.net,PDF,https://www.fransoliehoek.net/docs/Suau19ALA.pdf,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=TIfRd32kEpUJ,5.0,"https://scholar.google.com/scholar?cites=10741828920020141900&as_sdt=400005&sciodt=0,14&hl=en&num=20",10741828920020141900,https://serpapi.com/search.json?as_sdt=400005&cites=10741828920020141900&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:TIfRd32kEpUJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,14",https://serpapi.com/search.json?as_sdt=0%2C14&engine=google_scholar&hl=en&num=20&q=related%3ATIfRd32kEpUJ%3Ascholar.google.com%2F&start=80,6.0,"https://scholar.google.com/scholar?cluster=10741828920020141900&hl=en&num=20&as_sdt=0,14",10741828920020141900,https://serpapi.com/search.json?as_sdt=0%2C14&cluster=10741828920020141900&engine=google_scholar&hl=en&num=20,"https://scholar.googleusercontent.com/scholar?q=cache:TIfRd32kEpUJ:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,14",,,,,Pdf,
14,The ODE Method for Stochastic Approximation and Reinforcement Learning with Markovian Noise,2O_KNl9muTMJ,https://arxiv.org/abs/2401.07844,"… celebrated Borkar-Meyn theorem for stability from the Martingale difference noise setting to the Markovian noise setting, which greatly improves its applicability in reinforcement learning…","S Liu, S Chen, S Zhang - arXiv preprint arXiv:2401.07844, 2024 - arxiv.org",S Liu,https://scholar.google.com/citations?user=OVAJS8cAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=OVAJS8cAAAAJ&engine=google_scholar_author&hl=en,OVAJS8cAAAAJ,S Chen,https://scholar.google.com/citations?user=OpWNikcAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=OpWNikcAAAAJ&engine=google_scholar_author&hl=en,OpWNikcAAAAJ,S Zhang,https://scholar.google.com/citations?user=Pn7fj4IAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=Pn7fj4IAAAAJ&engine=google_scholar_author&hl=en,Pn7fj4IAAAAJ,,,,,arxiv.org,PDF,https://arxiv.org/pdf/2401.07844,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=2O_KNl9muTMJ,,,,,"https://scholar.google.com/scholar?q=related:2O_KNl9muTMJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,14",https://serpapi.com/search.json?as_sdt=0%2C14&engine=google_scholar&hl=en&num=20&q=related%3A2O_KNl9muTMJ%3Ascholar.google.com%2F&start=80,2.0,"https://scholar.google.com/scholar?cluster=3727122725752991704&hl=en&num=20&as_sdt=0,14",3727122725752991704,https://serpapi.com/search.json?as_sdt=0%2C14&cluster=3727122725752991704&engine=google_scholar&hl=en&num=20,"https://scholar.googleusercontent.com/scholar?q=cache:2O_KNl9muTMJ:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,14",,,,,,
15,Data-Driven Solutions to Mixed Control: A Hamilton-Inequality-Driven Reinforcement Learning Approach,Ka2Kj_qG_hkJ,https://ieeexplore.ieee.org/abstract/document/9206320/,… This paper presents a data-based reinforcement learning algorithm for control of nonlinear … which can be written in a compact form as a least squares (LS) equation AiXi = Bi with …,"Y Yang, M Mazouchi, H Modares - 2020 IEEE Conference on …, 2020 - ieeexplore.ieee.org",Y Yang,https://scholar.google.com/citations?user=smwQtUwAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=smwQtUwAAAAJ&engine=google_scholar_author&hl=en,smwQtUwAAAAJ,M Mazouchi,https://scholar.google.com/citations?user=vpqGNf8AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=vpqGNf8AAAAJ&engine=google_scholar_author&hl=en,vpqGNf8AAAAJ,H Modares,https://scholar.google.com/citations?user=xhucCdUAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=xhucCdUAAAAJ&engine=google_scholar_author&hl=en,xhucCdUAAAAJ,,,,,google.com,PDF,https://drive.google.com/file/d/1uRiP4Ps01KWlAEC6-QTJ3BiBc36X-FYp/view,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=Ka2Kj_qG_hkJ,,,,,"https://scholar.google.com/scholar?q=related:Ka2Kj_qG_hkJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,14",https://serpapi.com/search.json?as_sdt=0%2C14&engine=google_scholar&hl=en&num=20&q=related%3AKa2Kj_qG_hkJ%3Ascholar.google.com%2F&start=80,3.0,"https://scholar.google.com/scholar?cluster=1873082905740881193&hl=en&num=20&as_sdt=0,14",1873082905740881193,https://serpapi.com/search.json?as_sdt=0%2C14&cluster=1873082905740881193&engine=google_scholar&hl=en&num=20,,,,,,,
16,Feature Reinforcement Learning: Part II. Structured MDPs,qD4jktBNl04J,https://intapi.sciendo.com/pdf/10.2478/jagi-2021-0003,"… Nevertheless the reduction of the complex (ill-defined) reinforcement learning problem to an … , but see Hutter (2005); Legg and Hutter (2007) for such a discussion for the AIXI model. …","M Hutter - Journal of Artificial General Intelligence, 2021 - intapi.sciendo.com",M Hutter,https://scholar.google.com/citations?user=7hmCntEAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=7hmCntEAAAAJ&engine=google_scholar_author&hl=en,7hmCntEAAAAJ,,,,,,,,,,,,,sciendo.com,PDF,https://intapi.sciendo.com/pdf/10.2478/jagi-2021-0003,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=qD4jktBNl04J,1.0,"https://scholar.google.com/scholar?cites=5663080614642007720&as_sdt=400005&sciodt=0,14&hl=en&num=20",5663080614642007720,https://serpapi.com/search.json?as_sdt=400005&cites=5663080614642007720&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:qD4jktBNl04J:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,14",https://serpapi.com/search.json?as_sdt=0%2C14&engine=google_scholar&hl=en&num=20&q=related%3AqD4jktBNl04J%3Ascholar.google.com%2F&start=80,4.0,"https://scholar.google.com/scholar?cluster=5663080614642007720&hl=en&num=20&as_sdt=0,14",5663080614642007720,https://serpapi.com/search.json?as_sdt=0%2C14&cluster=5663080614642007720&engine=google_scholar&hl=en&num=20,"https://scholar.googleusercontent.com/scholar?q=cache:qD4jktBNl04J:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,14",,,,,Pdf,
17,Toward negotiable reinforcement learning: shifting priorities in Pareto optimal sequential decision-making,LBbHg2HdxpkJ,https://arxiv.org/abs/1701.01302,"… Existing multi-objective reinforcement learning (MORL) algorithms do not account for objectives that arise from players with differing beliefs. Concretely, consider two players with …","A Critch - arXiv preprint arXiv:1701.01302, 2017 - arxiv.org",A Critch,https://scholar.google.com/citations?user=F3_yOXUAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=F3_yOXUAAAAJ&engine=google_scholar_author&hl=en,F3_yOXUAAAAJ,,,,,,,,,,,,,arxiv.org,PDF,https://arxiv.org/pdf/1701.01302,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=LBbHg2HdxpkJ,15.0,"https://scholar.google.com/scholar?cites=11080787344084112940&as_sdt=400005&sciodt=0,14&hl=en&num=20",11080787344084112940,https://serpapi.com/search.json?as_sdt=400005&cites=11080787344084112940&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:LBbHg2HdxpkJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,14",https://serpapi.com/search.json?as_sdt=0%2C14&engine=google_scholar&hl=en&num=20&q=related%3ALBbHg2HdxpkJ%3Ascholar.google.com%2F&start=80,6.0,"https://scholar.google.com/scholar?cluster=11080787344084112940&hl=en&num=20&as_sdt=0,14",11080787344084112940,https://serpapi.com/search.json?as_sdt=0%2C14&cluster=11080787344084112940&engine=google_scholar&hl=en&num=20,"https://scholar.googleusercontent.com/scholar?q=cache:LBbHg2HdxpkJ:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,14",,,,,,
18,Theoretical foundations for programmatic reinforcement learning,_5yTGkDkGngJ,https://arxiv.org/abs/2402.11650,The field of Reinforcement Learning (RL) is concerned with algorithms for learning optimal policies in unknown stochastic environments. Programmatic RL studies representations of …,"G Shabadi, N Fijalkow, T Matricon - arXiv preprint arXiv:2402.11650, 2024 - arxiv.org",N Fijalkow,https://scholar.google.com/citations?user=3p7lwgMAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=3p7lwgMAAAAJ&engine=google_scholar_author&hl=en,3p7lwgMAAAAJ,T Matricon,https://scholar.google.com/citations?user=wzFtELYAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=wzFtELYAAAAJ&engine=google_scholar_author&hl=en,wzFtELYAAAAJ,,,,,,,,,arxiv.org,PDF,https://arxiv.org/pdf/2402.11650,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=_5yTGkDkGngJ,,,,,"https://scholar.google.com/scholar?q=related:_5yTGkDkGngJ:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,14",https://serpapi.com/search.json?as_sdt=0%2C14&engine=google_scholar&hl=en&num=20&q=related%3A_5yTGkDkGngJ%3Ascholar.google.com%2F&start=80,2.0,"https://scholar.google.com/scholar?cluster=8654480597920750847&hl=en&num=20&as_sdt=0,14",8654480597920750847,https://serpapi.com/search.json?as_sdt=0%2C14&cluster=8654480597920750847&engine=google_scholar&hl=en&num=20,"https://scholar.googleusercontent.com/scholar?q=cache:_5yTGkDkGngJ:scholar.google.com/+%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,14",,,,,,
19,Asynchronous distributed reinforcement learning for lqr control via zeroth-order block coordinate descent,OVlXZdi17A0J,https://ieeexplore.ieee.org/abstract/document/10494371/,"… Abstract—Recently introduced distributed zeroth-order optimization (ZOO) algorithms have shown their utility in distributed reinforcement learning (RL). Unfortunately, in the gradient …","G Jing, H Bai, J George, A Chakrabortty… - IEEE Transactions on …, 2024 - ieeexplore.ieee.org",G Jing,https://scholar.google.com/citations?user=Czk87NYAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=Czk87NYAAAAJ&engine=google_scholar_author&hl=en,Czk87NYAAAAJ,H Bai,https://scholar.google.com/citations?user=z7N_-U4AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=z7N_-U4AAAAJ&engine=google_scholar_author&hl=en,z7N_-U4AAAAJ,J George,https://scholar.google.com/citations?user=caxjdt4AAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=caxjdt4AAAAJ&engine=google_scholar_author&hl=en,caxjdt4AAAAJ,A Chakrabortty,https://scholar.google.com/citations?user=cyRBMkkAAAAJ&hl=en&num=20&oi=sra,https://serpapi.com/search.json?author_id=cyRBMkkAAAAJ&engine=google_scholar_author&hl=en,cyRBMkkAAAAJ,arxiv.org,PDF,https://arxiv.org/pdf/2107.12416,https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=OVlXZdi17A0J,4.0,"https://scholar.google.com/scholar?cites=1003376758014564665&as_sdt=400005&sciodt=0,14&hl=en&num=20",1003376758014564665,https://serpapi.com/search.json?as_sdt=400005&cites=1003376758014564665&engine=google_scholar&hl=en&num=20,"https://scholar.google.com/scholar?q=related:OVlXZdi17A0J:scholar.google.com/&scioq=%22reinforcement+learning%22+AND+%22aixi%22&hl=en&num=20&as_sdt=0,14",https://serpapi.com/search.json?as_sdt=0%2C14&engine=google_scholar&hl=en&num=20&q=related%3AOVlXZdi17A0J%3Ascholar.google.com%2F&start=80,4.0,"https://scholar.google.com/scholar?cluster=1003376758014564665&hl=en&num=20&as_sdt=0,14",1003376758014564665,https://serpapi.com/search.json?as_sdt=0%2C14&cluster=1003376758014564665&engine=google_scholar&hl=en&num=20,,,,,,,
